{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/artaalikhani/fault_fcwt_CNN/blob/master/fcwt_cnn_SRM_fault_detection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.nn import Parameter\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import math\n",
        "import pickle\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics import accuracy_score\n",
        "from fcwt import *\n",
        "#import fcwt\n",
        "import pywt\n",
        "import time\n",
        "\n",
        "import os\n",
        "#os.chdir('C:/Users/artaa/Documents/ITSC_sim')\n",
        "\n",
        "from Mchcnn2d import *\n",
        "\n",
        "\n",
        "\n",
        "def normalise(spectra):\n",
        "    if type(spectra) is np.ndarray:\n",
        "        max_I = np.max(spectra)\n",
        "        min_I = np.min(spectra)\n",
        "    elif type(spectra) is torch.Tensor:\n",
        "        max_I = max(spectra)\n",
        "        min_I = min(spectra)\n",
        "    spectra_normed = (spectra - min_I) / (max_I - min_I)\n",
        "    return spectra_normed\n",
        "\n",
        "def random_data_split(spectra, labels, settings):\n",
        "    thresh1 = round(settings[0]*settings[5])\n",
        "    thresh2 = round((settings[0] - thresh1)/2 + thresh1)\n",
        "    l = list(range(settings[0]))\n",
        "    if os.path.isfile('data_test_ind.csv'):\n",
        "      lr = np.loadtxt('data_test_ind.csv', delimiter=\",\").astype(int)\n",
        "    else:\n",
        "      lr = random.sample(l, settings[0])\n",
        "      np.savetxt('data_test_ind.csv',lr, delimiter=\",\")\n",
        "\n",
        "    data_train = np.array([spectra[idx] for idx in lr[:thresh1]])\n",
        "    data_val = np.array([spectra[idx] for idx in lr[thresh1:thresh2]])\n",
        "    data_test = np.array([spectra[idx] for idx in lr[thresh2:]])\n",
        "    labels_train = np.array([labels[idx] for idx in lr[:thresh1]])\n",
        "    labels_val = np.array([labels[idx] for idx in lr[thresh1:thresh2]])\n",
        "    labels_test = np.array([labels[idx] for idx in lr[thresh2:]])\n",
        "\n",
        "    return (data_train, data_val, data_test), (labels_train, labels_val, labels_test)\n",
        "\n",
        "\n",
        "class AugmentedDataset(Dataset):\n",
        "    def __init__(self, tensors, settings, settings_aug):\n",
        "        self.tensors = tensors\n",
        "        self.settings = settings\n",
        "        self.settings_aug = settings_aug\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        x = torch.from_numpy(np.asarray(self.tensors[0][index][:])).to(self.settings[4])\n",
        "        y = torch.tensor(\n",
        "            self.tensors[1][index].astype(np.float32)\n",
        "        ).to(self.settings[4])\n",
        "        return x, y\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.tensors[0])\n",
        "\n",
        "def AugmentedDataloader(spectra, labels, settings, settings_aug):\n",
        "    tensors = (spectra, labels)\n",
        "    #print('spectrashape',spectra.shape)\n",
        "    ds = AugmentedDataset(\n",
        "        tensors,\n",
        "        settings,\n",
        "        settings_aug,\n",
        "    )\n",
        "    loader = DataLoader(\n",
        "        ds,\n",
        "        batch_size=settings[6],\n",
        "        shuffle=True,\n",
        "        drop_last=True,\n",
        "    )\n",
        "    return loader\n",
        "\n",
        "def dataloader_preparation(split_ratio=0.7, batch_size=50):\n",
        "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    # load dataset\n",
        "    #spectra = normalise(xrd_datasets[0][:, np.newaxis, :])\n",
        "    #labels = xrd_datasets[1]\n",
        "    #loaded_arr = np.loadtxt(\"SFT_Data_sim_SVM_BLAC_Simple_STFT3.csv\",delimiter=\",\")\n",
        "    #loaded_arr = np.loadtxt(\"SFT_Data_simple.csv\",delimiter=\",\")\n",
        "    #Data = loaded_arr.reshape(loaded_arr.shape[0], loaded_arr.shape[1] //51, 51)\n",
        "\n",
        "    xrd_datasets1 = np.loadtxt('Vib_data2.csv',delimiter=\",\")\n",
        "    xrd_datasets = xrd_datasets1[:,:]\n",
        "    morl1 = Morlet(0.5)\n",
        "    #morl2 = Morlet(4.0)\n",
        "    #morl3 = Morlet(16.0)\n",
        "\n",
        "    fn = 40\n",
        "    f0 = 1\n",
        "    f1 = 1500\n",
        "    fs = 12000\n",
        "\n",
        "    Data1=np.zeros([len(xrd_datasets[:,2400]),fn,2400])\n",
        "    #Data2=np.zeros([len(xrd_datasets[:,2400]),100,2400])\n",
        "    #Data3=np.zeros([len(xrd_datasets[:,2400]),100,2400])\n",
        "    spectra = []\n",
        "    Class_label=np.zeros(len(xrd_datasets[:,2400]))\n",
        "    t0 = time.time()\n",
        "    for i in range(len(xrd_datasets[:,2400])):#\n",
        "      fs=12000\n",
        "      N_s=2400\n",
        "      N_f=2400\n",
        "      x=xrd_datasets[i,0:N_s].astype('single')#+np.random.normal(0,.5,N_s).astype('single')\n",
        "      class_n=xrd_datasets[i,2400]\n",
        "      '''\n",
        "\n",
        "      #############################tuned settings fCWT##########################\n",
        "\n",
        "\n",
        "      for j in range(fn):\n",
        "        BW = 0.2*(j+1)\n",
        "        morl1 = Morlet(BW)\n",
        "        out = np.zeros((2,len(x)), dtype='csingle')\n",
        "        freqs = np.zeros((2), dtype='single')\n",
        "        scales = Scales(morl1,FCWT_LOGSCALES,fs,(j+1)*30,(j+2)*30,2)\n",
        "        scales.getFrequencies(freqs)\n",
        "        fcwt = FCWT(morl1, 7, False, False)\n",
        "\n",
        "        fcwt.cwt(x, scales, out)\n",
        "        Data1[i,j,:] = abs(out[0,:])\n",
        "\n",
        "      '''\n",
        "      '''\n",
        "      ####################CONVENTIONAL tuned settings fCWT######################\n",
        "\n",
        "\n",
        "      BW = 4\n",
        "      morl1 = Morlet(BW)\n",
        "      out = np.zeros((fn,len(x)), dtype='csingle')\n",
        "      freqs = np.zeros((fn), dtype='single')\n",
        "      scales = Scales(morl1,FCWT_LOGSCALES,fs,f0,f1,fn)\n",
        "      scales.getFrequencies(freqs)\n",
        "      fcwt = FCWT(morl1, 8, False, False)\n",
        "\n",
        "      fcwt.cwt(x, scales, out)\n",
        "      Data1[i,:,:] = abs(out)\n",
        "\n",
        "      '''\n",
        "      '''\n",
        "      out = np.zeros((fn,len(x)), dtype='csingle')\n",
        "      freqs = np.zeros((fn), dtype='single')\n",
        "      scales = Scales(morl2,FCWT_LOGSCALES,fs,f0,f1,fn)\n",
        "      scales.getFrequencies(freqs)\n",
        "      fcwt = FCWT(morl2, 8, False, False)\n",
        "\n",
        "      fcwt.cwt(x, scales, out)\n",
        "      Data2[i,:,:] = abs(out)\n",
        "\n",
        "      out = np.zeros((fn,len(x)), dtype='csingle')\n",
        "      freqs = np.zeros((fn), dtype='single')\n",
        "      scales = Scales(morl3,FCWT_LOGSCALES,fs,f0,f1,fn)\n",
        "      scales.getFrequencies(freqs)\n",
        "      fcwt = FCWT(morl3, 8, False, False)\n",
        "\n",
        "      fcwt.cwt(x, scales, out)\n",
        "      Data3[i,:,:] = abs(out)\n",
        "      '''\n",
        "      '''\n",
        "      for j in range(19):\n",
        "        idx = np.arange(j*30+28,j*30+32)\n",
        "        Data[i,j*5:j*5+4,:]=np.abs(out[idx,:])\n",
        "      '''\n",
        "      '''\n",
        "      ##################################fCWT####################################\n",
        "      signal_CWRU=xrd_datasets[i,0:N_s]\n",
        "      class_n=xrd_datasets[i,2400]\n",
        "      freqs, coef=fcwt.cwt(signal_CWRU,12000,1,500,20)\n",
        "      Data[i,:,:]=coef\n",
        "      '''\n",
        "      '''\n",
        "      #############################Conventional CWT#############################\n",
        "      signal_CWRU=xrd_datasets[i,0:N_s]\n",
        "      class_n=xrd_datasets[i,2400]\n",
        "\n",
        "      coef, freqs=pywt.cwt(signal_CWRU,np.arange(1,41),'cmor4-1')\n",
        "      Data1[i,:,:]=coef\n",
        "      '''\n",
        "      Class_label[i]=class_n\n",
        "    print('Signal processing time is:'+str(time.time()-t0))\n",
        "    spectra = Data1[:,np.newaxis,:,:]\n",
        "    #spectra = np.append(spectra,Data2[:,np.newaxis,:,:],axis=1)\n",
        "    #spectra = np.append(spectra,Data3[:,np.newaxis,:,:],axis=1)\n",
        "    '''\n",
        "    ############################################################################\n",
        "    f0 = 1\n",
        "    f1 = 1000\n",
        "\n",
        "    Data=np.zeros([len(xrd_datasets[:,2400]),fn,2400])\n",
        "    Class_label=np.zeros(len(xrd_datasets[:,2400]))\n",
        "    for i in range(len(xrd_datasets[:,2400])):#\n",
        "      fs=12000\n",
        "      N_s=2400\n",
        "      N_f=2400\n",
        "\n",
        "      #############################tuned settings fCWT##########################\n",
        "      x=xrd_datasets[i,0:N_s].astype('single')#+np.random.normal(0,.5,N_s).astype('single')\n",
        "      class_n=xrd_datasets[i,2400]\n",
        "\n",
        "      out = np.zeros((fn,len(x)), dtype='csingle')\n",
        "      freqs = np.zeros((fn), dtype='single')\n",
        "      scales = Scales(morl,FCWT_LOGSCALES,fs,f0,f1,fn)\n",
        "      scales.getFrequencies(freqs)\n",
        "      fcwt = FCWT(morl, 8, True, False)\n",
        "\n",
        "      fcwt.cwt(x, scales, out)\n",
        "\n",
        "      Data[i,:,:]=abs(out)\n",
        "      Class_label[i]=class_n\n",
        "    spectra = np.append(spectra,Data[:,np.newaxis,:,:],axis=1)\n",
        "\n",
        "    ############################################################################\n",
        "    f0 = 1\n",
        "    f1 = 5000\n",
        "\n",
        "    Data=np.zeros([len(xrd_datasets[:,2400]),fn,2400])\n",
        "    Class_label=np.zeros(len(xrd_datasets[:,2400]))\n",
        "    for i in range(len(xrd_datasets[:,2400])):#\n",
        "      fs=12000\n",
        "      N_s=2400\n",
        "      N_f=2400\n",
        "\n",
        "      #############################tuned settings fCWT##########################\n",
        "      x=xrd_datasets[i,0:N_s].astype('single')#+np.random.normal(0,.5,N_s).astype('single')\n",
        "      class_n=xrd_datasets[i,2400]\n",
        "\n",
        "      out = np.zeros((fn,len(x)), dtype='csingle')\n",
        "      freqs = np.zeros((fn), dtype='single')\n",
        "      scales = Scales(morl,FCWT_LOGSCALES,fs,f0,f1,fn)\n",
        "      scales.getFrequencies(freqs)\n",
        "      fcwt = FCWT(morl, 8, True, False)\n",
        "\n",
        "      fcwt.cwt(x, scales, out)\n",
        "\n",
        "      Data[i,:,:]=abs(out)\n",
        "      Class_label[i]=class_n\n",
        "    spectra = np.append(spectra,Data[:,np.newaxis,:,:],axis=1)\n",
        "    ############################################################################\n",
        "    '''\n",
        "    #labels = np.loadtxt(\"Class_label_SVM_BLAC_test.csv\",delimiter=',')\n",
        "    #labels = np.loadtxt(\"Class_label_specto_simple.csv\",delimiter=',')\n",
        "    labels = Class_label\n",
        "    # measure the numbers of dataset shape\n",
        "    n_samples, n_channel, n_length,_ = spectra.shape\n",
        "    n_class = len(np.unique(labels))\n",
        "    settings = (n_samples, n_channel, n_length, n_class, device, split_ratio, batch_size)\n",
        "    settings_aug = (100, 120, 0.2, 0.2, 0.5)\n",
        "    # (window size, max peak shift size, probability of peak elimination,\n",
        "    #  probability of peak scailing, probability of peak shift)\n",
        "\n",
        "    # dataloaders\n",
        "    spectra_split, labels_split = random_data_split(spectra, labels, settings)\n",
        "    dataloader_train = AugmentedDataloader(\n",
        "        spectra_split[0],\n",
        "        labels_split[0],\n",
        "        settings,\n",
        "        settings_aug,\n",
        "    )\n",
        "\n",
        "    dataloader_val = DataLoader(\n",
        "        MyDataset(spectra_split[1],labels_split[1]),\n",
        "        batch_size=settings[6],\n",
        "        #drop_last=True,\n",
        "    )\n",
        "\n",
        "    dataloader_test = DataLoader(\n",
        "        MyDataset(spectra_split[2],labels_split[2]),\n",
        "        batch_size=settings[6],\n",
        "        #drop_last=True,\n",
        "    )\n",
        "\n",
        "    # compile dataloaders and settings\n",
        "    dataloaders = (dataloader_train, dataloader_val, dataloader_test)\n",
        "    return dataloaders, settings\n",
        "\n",
        "def load_model(settings):\n",
        "\n",
        "    #model = MSResNet(input_channel=1,layers=[4,2,5], num_classes=4)\n",
        "\n",
        "    model = Net(in_channels=1,\n",
        "                   n_class=12,##################################################################\n",
        "                  )\n",
        "\n",
        "    model.to(settings[4])\n",
        "    model=model.double()\n",
        "    return model\n",
        "\n",
        "def top_k(pred, label, k:int = 1):\n",
        "    labels_dim = 1\n",
        "    k_labels = torch.topk(input=pred, k=k, dim=1, largest=True, sorted=True)[1]\n",
        "    a = ~torch.prod(\n",
        "        input = torch.abs(label.unsqueeze(labels_dim) - k_labels),\n",
        "        dim=labels_dim,\n",
        "    ).to(torch.bool)\n",
        "    a = a.to(torch.int8)\n",
        "    y_pred = a * label + (1-a) * k_labels[:,0]\n",
        "    acc = accuracy_score(y_pred.cpu(), label.cpu())*100\n",
        "    return acc\n",
        "\n",
        "def record_learning_curve(lc_name, epoch, results, loss_train, acc_train, loss_val, acc_val):\n",
        "    results[epoch, :] = np.array([loss_train, acc_train, loss_val, acc_val])\n",
        "    df = pd.DataFrame(results, columns=['loss_train', 'acc_train', 'loss_val', 'acc_val'])\n",
        "    df.to_csv(lc_name)\n",
        "\n",
        "def save_model(best_acc, epoch, model):\n",
        "    print('--------> The best model has been replaced.')\n",
        "    print('epoch: '+str(epoch)+' | best_acc: '+str(best_acc))\n",
        "    model_path = './wdcnn2d_epoch_CWRU'+str(epoch)+'.pt'\n",
        "    torch.save(model.state_dict(), model_path)\n",
        "    print('The best model has been saved in '+model_path)\n",
        "\n",
        "def train(dataloaders, settings, model, criterion, optimizer):\n",
        "    running_loss = 0.0\n",
        "    running_corrects = 0.0\n",
        "    model.train()\n",
        "    model.zero_grad()\n",
        "\n",
        "    for batch_idx, batch in enumerate(dataloaders[0]):\n",
        "        # train\n",
        "        input, label = tuple(t.to(settings[4]) for t in batch)\n",
        "        label = label.long()\n",
        "        pred = model(input)\n",
        "        loss = criterion(pred, label)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # evaluate\n",
        "        running_corrects += top_k(pred, label, k=1) * len(label)\n",
        "        running_loss += loss.item()\n",
        "        print('[Train] batch: '+str(batch_idx+1)+' | loss: '+str(loss.item()))\n",
        "\n",
        "    # summarise\n",
        "    n_train = round(settings[0] * settings[5])\n",
        "    epoch_loss = running_loss / n_train\n",
        "    epoch_acc = running_corrects / n_train\n",
        "    print('[Train total] loss: '+str(epoch_loss)+' | acc: '+str(epoch_acc))\n",
        "    return epoch_loss, epoch_acc\n",
        "\n",
        "def val(dataloader, settings, model, criterion):\n",
        "    running_loss = 0.0\n",
        "    running_corrects = 0.0\n",
        "    model.eval()\n",
        "    model.zero_grad()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, batch in enumerate(dataloader):\n",
        "            # test\n",
        "            input, label = tuple(t.to(settings[4]) for t in batch)\n",
        "            label = label.long()\n",
        "            model=model.to(settings[4])\n",
        "            pred = model(input.double())\n",
        "            loss = criterion(pred, label)\n",
        "\n",
        "            # evaluate\n",
        "            running_corrects += top_k(pred, label, k=1) * len(label)\n",
        "            running_loss += loss.item()\n",
        "            print('[Val] batch: '+str(batch_idx+1)+' | loss: '+str(loss.item()))\n",
        "\n",
        "    # summarise\n",
        "    n_test = round(settings[0] * (1 - settings[5])/2)\n",
        "    epoch_loss = running_loss / n_test\n",
        "    epoch_acc = running_corrects / n_test\n",
        "    print('[Val total] loss: '+str(epoch_loss)+' | acc: '+str(epoch_acc))\n",
        "    return epoch_loss, epoch_acc\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    lc_name = 'learning_curve_Mch_lstm_CNN2D_CWRU_fcwt.csv'\n",
        "    n_epoch = 50\n",
        "    batch = 14\n",
        "\n",
        "    dataloaders, settings = dataloader_preparation(batch_size=batch)\n",
        "    model = load_model(settings)\n",
        "    criterion = nn.CrossEntropyLoss().to(settings[4])\n",
        "    optimizer = optim.SGD(model.parameters(), lr=0.001)\n",
        "    '''\n",
        "    from torchsummary import summary\n",
        "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "    summary(model.to(device=device, dtype=torch.float),(1,10,51))\n",
        "    '''\n",
        "    best_acc = 0.0\n",
        "    results = np.zeros([n_epoch, 4])\n",
        "    for epoch in range(n_epoch):\n",
        "        print('>>>>>> epoch '+str(epoch)+' starts')\n",
        "        model=model.double()\n",
        "        model.eval()\n",
        "        loss_train, acc_train = train(dataloaders, settings, model, criterion, optimizer)\n",
        "        loss_val, acc_val = val(dataloaders[1], settings, model, criterion)\n",
        "        record_learning_curve(lc_name,epoch,results,loss_train,acc_train,loss_val,acc_val)\n",
        "\n",
        "        # save better model\n",
        "        if best_acc <= acc_val:\n",
        "            best_acc = acc_val\n",
        "            loss_test, acc_test = val(dataloaders[2], settings, model, criterion)\n",
        "\n",
        "            #if epoch>10:\n",
        "            #save_model(best_acc, epoch, model)\n"
      ],
      "metadata": {
        "id": "DgHBzKXk1Z9z",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 785
        },
        "outputId": "946efa67-4ac1-4d39-dd64-a4bbae41817d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "C:\\Users\\artaa\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Signal processing time is:59.6892991065979\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "C:\\Users\\artaa\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.\n",
            "  warnings.warn('Lazy modules are a new feature under heavy development '\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ">>>>>> epoch 0 starts\n",
            "[Train] batch: 1 | loss: 2.674809371723716\n",
            "[Train] batch: 2 | loss: 2.3136332013587095\n",
            "[Train] batch: 3 | loss: 2.358745107805398\n",
            "[Train] batch: 4 | loss: 2.2013469600994364\n",
            "[Train] batch: 5 | loss: 2.4026490767560063\n",
            "[Train] batch: 6 | loss: 2.397490513560814\n",
            "[Train] batch: 7 | loss: 2.3207586522327874\n",
            "[Train] batch: 8 | loss: 2.0300888369143886\n",
            "[Train] batch: 9 | loss: 2.0321716155890686\n",
            "[Train] batch: 10 | loss: 2.1113410341197225\n",
            "[Train] batch: 11 | loss: 1.9117288101499013\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn [1], line 401\u001b[0m\n\u001b[0;32m    399\u001b[0m model\u001b[38;5;241m=\u001b[39mmodel\u001b[38;5;241m.\u001b[39mdouble()\n\u001b[0;32m    400\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m--> 401\u001b[0m loss_train, acc_train \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msettings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    402\u001b[0m loss_val, acc_val \u001b[38;5;241m=\u001b[39m val(dataloaders[\u001b[38;5;241m1\u001b[39m], settings, model, criterion)\n\u001b[0;32m    403\u001b[0m record_learning_curve(lc_name,epoch,results,loss_train,acc_train,loss_val,acc_val)\n",
            "Cell \u001b[1;32mIn [1], line 343\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(dataloaders, settings, model, criterion, optimizer)\u001b[0m\n\u001b[0;32m    340\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m    342\u001b[0m \u001b[38;5;66;03m# evaluate\u001b[39;00m\n\u001b[1;32m--> 343\u001b[0m running_corrects \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mtop_k\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mlen\u001b[39m(label)\n\u001b[0;32m    344\u001b[0m running_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m    345\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m[Train] batch: \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;28mstr\u001b[39m(batch_idx\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m | loss: \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;28mstr\u001b[39m(loss\u001b[38;5;241m.\u001b[39mitem()))\n",
            "Cell \u001b[1;32mIn [1], line 311\u001b[0m, in \u001b[0;36mtop_k\u001b[1;34m(pred, label, k)\u001b[0m\n\u001b[0;32m    309\u001b[0m a \u001b[38;5;241m=\u001b[39m a\u001b[38;5;241m.\u001b[39mto(torch\u001b[38;5;241m.\u001b[39mint8)\n\u001b[0;32m    310\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m a \u001b[38;5;241m*\u001b[39m label \u001b[38;5;241m+\u001b[39m (\u001b[38;5;241m1\u001b[39m\u001b[38;5;241m-\u001b[39ma) \u001b[38;5;241m*\u001b[39m k_labels[:,\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m--> 311\u001b[0m acc \u001b[38;5;241m=\u001b[39m accuracy_score(\u001b[43my_pred\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m, label\u001b[38;5;241m.\u001b[39mcpu())\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m100\u001b[39m\n\u001b[0;32m    312\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m acc\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "save_model(best_acc, epoch, model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o4EpOKRFZkwA",
        "outputId": "71d9433e-9b90-4793-9c09-c76f0996556f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------> The best model has been replaced.\n",
            "epoch: 49 | best_acc: 100.0\n",
            "The best model has been saved in ./wdcnn2d_epoch_CWRU49.pt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from Mchcnn2d import *\n",
        "model = load_model(settings)\n",
        "criterion = nn.CrossEntropyLoss().to(settings[4])\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.001)\n",
        "best_acc = 0.0\n",
        "results = np.zeros([n_epoch, 4])\n",
        "for epoch in range(n_epoch):\n",
        "        print('>>>>>> epoch '+str(epoch)+' starts')\n",
        "        model=model.double()\n",
        "        model.eval()\n",
        "        loss_train, acc_train = train(dataloaders, settings, model, criterion, optimizer)\n",
        "        loss_val, acc_val = val(dataloaders[1], settings, model, criterion)\n",
        "        record_learning_curve(lc_name,epoch,results,loss_train,acc_train,loss_val,acc_val)\n",
        "\n",
        "        # save better model\n",
        "        if best_acc <= acc_val:\n",
        "            best_acc = acc_val\n",
        "            loss_test, acc_test = val(dataloaders[2], settings, model, criterion)"
      ],
      "metadata": {
        "id": "AAz5gWswfKrW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchsummary import summary\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "summary(model.to(device=device, dtype=torch.float),(1,40,2400))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NdrWE28ZMA6x",
        "outputId": "9100a314-0996-45eb-ff0d-c659c89f207d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "       BatchNorm2d-1          [-1, 1, 40, 2400]               2\n",
            "            Conv2d-2           [-1, 4, 10, 150]             340\n",
            "       BatchNorm2d-3           [-1, 4, 10, 150]               8\n",
            "        BasicBlock-4           [-1, 4, 10, 150]               0\n",
            "            Conv2d-5           [-1, 4, 10, 150]             144\n",
            "       BatchNorm2d-6           [-1, 4, 10, 150]               8\n",
            "        BasicBlock-7           [-1, 4, 10, 150]               0\n",
            "            Conv2d-8           [-1, 4, 10, 150]             144\n",
            "       BatchNorm2d-9           [-1, 4, 10, 150]               8\n",
            "       BasicBlock-10           [-1, 4, 10, 150]               0\n",
            "           Conv2d-11           [-1, 4, 10, 150]             144\n",
            "      BatchNorm2d-12           [-1, 4, 10, 150]               8\n",
            "       BasicBlock-13           [-1, 4, 10, 150]               0\n",
            "     CascadeBlock-14          [-1, 16, 10, 150]               0\n",
            "           Conv2d-15           [-1, 16, 4, 144]          20,736\n",
            "      BatchNorm2d-16           [-1, 16, 4, 144]              32\n",
            "       BasicBlock-17           [-1, 16, 4, 144]               0\n",
            "          Flatten-18                 [-1, 9216]               0\n",
            "           Linear-19                   [-1, 12]         110,604\n",
            "================================================================\n",
            "Total params: 132,178\n",
            "Trainable params: 132,178\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.37\n",
            "Forward/backward pass size (MB): 1.75\n",
            "Params size (MB): 0.50\n",
            "Estimated Total Size (MB): 2.62\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# FCWT"
      ],
      "metadata": {
        "id": "Hx9WKunds26a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from fcwt import *\n",
        "#import fcwt\n",
        "import pywt\n",
        "import time\n",
        "\n",
        "\n",
        "xrd_datasets1 = np.loadtxt('Vib_data2.csv',delimiter=\",\")\n",
        "xrd_datasets = xrd_datasets1[:,:]\n",
        "\n",
        "fn = 50\n",
        "f0 = 1\n",
        "f1 = 1500\n",
        "fs = 12000\n",
        "\n",
        "Data1=np.zeros([len(xrd_datasets[:,2400]),fn,2400])\n",
        "spectra = []\n",
        "Class_label=np.zeros(len(xrd_datasets[:,2400]))\n",
        "t0 = time.time()\n",
        "for i in range(740):#len(xrd_datasets[:,2400])\n",
        "      fs=12000\n",
        "      N_s=2400\n",
        "      N_f=2400\n",
        "      x=xrd_datasets[i,0:N_s].astype('single')#+np.random.normal(0,.5,N_s).astype('single')\n",
        "      class_n=xrd_datasets[i,2400]\n",
        "      #'''\n",
        "\n",
        "      #############################tuned settings fCWT##########################\n",
        "\n",
        "\n",
        "      for j in range(fn):\n",
        "        BW = 0.3*(j+1)\n",
        "        morl1 = Morlet(BW)\n",
        "        out = np.zeros((2,len(x)), dtype='csingle')\n",
        "        freqs = np.zeros((2), dtype='single')\n",
        "        scales = Scales(morl1,FCWT_LOGSCALES,fs,(j+1)*30,(j+2)*30,2)\n",
        "        scales.getFrequencies(freqs)\n",
        "        fcwt = FCWT(morl1, 8, False, False)\n",
        "\n",
        "        fcwt.cwt(x, scales, out)\n",
        "        Data1[i,j,:] = abs(out[0,:])\n",
        "\n",
        "\n",
        "delta_t = time.time()-t0\n",
        "print(delta_t)"
      ],
      "metadata": {
        "id": "og_VsKpHn9GP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from fcwt import *\n",
        "#import fcwt\n",
        "import pywt\n",
        "import time\n",
        "\n",
        "xrd_datasets = np.loadtxt('Vib_data2.csv',delimiter=\",\")\n",
        "\n",
        "morl = Morlet(4)\n",
        "\n",
        "fn = 50\n",
        "f0 = 1\n",
        "f1 = 1500\n",
        "fs = 12000\n",
        "\n",
        "Data3=np.zeros([len(xrd_datasets[:,2400]),fn,2400])\n",
        "Class_label=np.zeros(len(xrd_datasets[:,2400]))\n",
        "t0 = time.time()\n",
        "for i in range(740):#len(xrd_datasets[:,2400])\n",
        "      fs=12000\n",
        "      N_s=2400\n",
        "      N_f=2400\n",
        "      #'''\n",
        "      #############################tuned settings fCWT##########################\n",
        "      signal_CWRU=xrd_datasets[i,0:N_s]\n",
        "      class_n=xrd_datasets[i,2400]\n",
        "\n",
        "      coef, freqs=pywt.cwt(signal_CWRU,np.arange(1,51),'gaus1')\n",
        "      Data3[i,:,:]=coef\n",
        "      '''\n",
        "      for j in range(19):\n",
        "        idx = np.arange(j*25+23,j*25+27)\n",
        "        Data[i,j*5:j*5+4,:]=np.abs(out[idx,:])\n",
        "\n",
        "      '''\n",
        "\n",
        "delta_t = time.time()-t0\n",
        "print(delta_t)idx = 280\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "fig.set_size_inches(28.5, 10.5)\n",
        "ax.matshow(Data1[idx,:,:])\n",
        "ax.set_axis_off()\n",
        "ax.set_title('Inner race fault 0.028\"-VBWfCWT')\n",
        "print(Class_label[idx])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        },
        "id": "sFyuYHMay5fE",
        "outputId": "001d3a19-a29e-4f83-b418-f312c8c559c1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 2850x1050 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fig, ax = plt.subplots()\n",
        "fig.set_size_inches(28.5, 10.5)\n",
        "ax.matshow(Data2[idx,:,:])\n",
        "ax.set_axis_off()\n",
        "ax.set_title('Inner race fault 0.028\"-fCWT')\n",
        "print(Class_label[idx])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        },
        "id": "Cw7v-nYvbry2",
        "outputId": "7025b04c-6b2c-422e-bb77-57ee3b52c878"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 2850x1050 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Data[1,:,:]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rnzr3t-E_y5n",
        "outputId": "c0f5c369-eeef-477d-cbad-e63e15541a1e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[-2.37412286e+00, -2.87331581e+00, -3.17662501e+00, ...,\n",
              "         2.35136509e+00,  5.91339111e+00,  9.06126499e+00],\n",
              "       [-2.98677540e+01, -3.13147697e+01, -3.16530914e+01, ...,\n",
              "        -3.16987991e+00, -7.99067116e+00, -1.25119534e+01],\n",
              "       [-2.57952332e+00, -1.47910953e+00, -3.48862648e-01, ...,\n",
              "         1.11665211e+01,  1.19044638e+01,  1.24203091e+01],\n",
              "       ...,\n",
              "       [-9.12370735e-11, -9.11569917e-11, -9.10715531e-11, ...,\n",
              "        -8.60810728e-11, -8.63230112e-11, -8.65598912e-11],\n",
              "       [-9.12370735e-11, -9.11569917e-11, -9.10715531e-11, ...,\n",
              "        -8.60810728e-11, -8.63230112e-11, -8.65598912e-11],\n",
              "       [-9.12370735e-11, -9.11569917e-11, -9.10715531e-11, ...,\n",
              "        -8.60810728e-11, -8.63230112e-11, -8.65598912e-11]])"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def save_model(best_acc, epoch, model):\n",
        "    print('--------> The best model has been replaced.')\n",
        "    print('epoch: '+str(epoch)+' | best_acc: '+str(best_acc))\n",
        "    model_path = './wdcnn2d_epoch_CWRU_fcwt_conventional'+str(epoch)+'.pt'\n",
        "    torch.save(model.state_dict(), model_path)\n",
        "    print('The best model has been saved in '+model_path)\n",
        "save_model(best_acc, epoch, model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "itTDAPeiWZ_F",
        "outputId": "84480264-d3b4-4e0e-a738-2450a5d09fbd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------> The best model has been replaced.\n",
            "epoch: 49 | best_acc: 100.0\n",
            "The best model has been saved in ./wdcnn2d_epoch_CWRU_final_final49.pt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#load model"
      ],
      "metadata": {
        "id": "U2oHikw3LhG9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, confusion_matrix, f1_score, recall_score, precision_score\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.nn import Parameter\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import math\n",
        "import pickle\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics import accuracy_score\n",
        "import math\n",
        "import scipy.io\n",
        "from scipy import signal\n",
        "from fcwt import *\n",
        "#import fcwt\n",
        "import pywt\n",
        "\n",
        "\n",
        "import os\n",
        "os.chdir('C:/Users/artaa/Documents/ITSC_sim')\n",
        "\n",
        "from Mchcnn2d_new import *\n",
        "#from wdcnn import *\n",
        "\n",
        "def random_data_split(spectra, labels, settings):\n",
        "    thresh1 = round(settings[0]*settings[5])\n",
        "    thresh2 = round((settings[0] - thresh1)/2 + thresh1)\n",
        "    l = list(range(settings[0]))\n",
        "    lr = random.sample(l, settings[0])\n",
        "    ind = np.loadtxt('data_test_ind_main.csv', delimiter=\",\")\n",
        "    ind = np.loadtxt('data_test_ind.csv', delimiter=\",\")\n",
        "    data_train = np.array([spectra[idx] for idx in lr[:thresh1]])\n",
        "    data_val = np.array([spectra[idx] for idx in lr[thresh1:thresh2]])\n",
        "    data_test = np.array([spectra[idx] for idx in ind[thresh2:].astype(int)])\n",
        "    labels_train = np.array([labels[idx] for idx in lr[:thresh1]])\n",
        "    labels_val = np.array([labels[idx] for idx in lr[thresh1:thresh2]])\n",
        "    labels_test = np.array([labels[idx] for idx in ind[thresh2:].astype(int)])\n",
        "\n",
        "    return (data_train, data_val, data_test), (labels_train, labels_val, labels_test)\n",
        "class AugmentedDataset(Dataset):\n",
        "    def __init__(self, tensors, settings, settings_aug):\n",
        "        self.tensors = tensors\n",
        "        self.settings = settings\n",
        "        self.settings_aug = settings_aug\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        x = torch.from_numpy(np.asarray(self.tensors[0][index][:])).to(self.settings[4])\n",
        "        y = torch.tensor(\n",
        "            self.tensors[1][index].astype(np.float32)\n",
        "        ).to(self.settings[4])\n",
        "        return x, y\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.tensors[0])\n",
        "def AugmentedDataloader(spectra, labels, settings, settings_aug):\n",
        "    tensors = (spectra, labels)\n",
        "    #print('spectrashape',spectra.shape)\n",
        "    ds = AugmentedDataset(\n",
        "        tensors,\n",
        "        settings,\n",
        "        settings_aug,\n",
        "    )\n",
        "    loader = DataLoader(\n",
        "        ds,\n",
        "        batch_size=settings[6],\n",
        "        shuffle=True,\n",
        "        drop_last=True,\n",
        "    )\n",
        "    return loader\n",
        "def dataloader_preparation(split_ratio=0.7, batch_size=50, noise_std=0):\n",
        "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    xrd_datasets1 = np.loadtxt('Vib_data2.csv',delimiter=\",\")\n",
        "    ind = np.loadtxt('data_test_ind.csv', delimiter=\",\")\n",
        "    xrd_datasets = xrd_datasets1[:,:]\n",
        "    morl = Morlet(4.0)\n",
        "\n",
        "    fn = 50\n",
        "    f0 = 1\n",
        "    f1 = 1200\n",
        "    fs = 12000\n",
        "\n",
        "    Data1=np.zeros([len(xrd_datasets[:,2400]),fn,2400])\n",
        "    #Data2=np.zeros([len(xrd_datasets[:,2400]),100,2400])\n",
        "    #Data3=np.zeros([len(xrd_datasets[:,2400]),100,2400])\n",
        "    spectra = []\n",
        "    Class_label=np.zeros(len(xrd_datasets[:,2400]))\n",
        "    for i in range(len(xrd_datasets[:,2400])):#\n",
        "      fs=12000\n",
        "      N_s=2400\n",
        "      N_f=2400\n",
        "      #'''\n",
        "      #############################tuned settings fCWT##########################\n",
        "      x=xrd_datasets[i,0:N_s].astype('single')+np.random.normal(0,noise_std,N_s).astype('single')#+np.sin(np.linspace(0,2400,2400)*np.pi/180*20/12000).astype('single')\n",
        "      class_n=xrd_datasets[i,2400]\n",
        "\n",
        "      for j in range(fn):\n",
        "        BW = 0.3*(j+1)\n",
        "        morl1 = Morlet(BW)\n",
        "        out = np.zeros((2,len(x)), dtype='csingle')\n",
        "        freqs = np.zeros((2), dtype='single')\n",
        "        scales = Scales(morl1,FCWT_LOGSCALES,fs,(j+1)*30,(j+2)*30,2)\n",
        "        scales.getFrequencies(freqs)\n",
        "        fcwt = FCWT(morl1, 8, False, False)\n",
        "\n",
        "        fcwt.cwt(x, scales, out)\n",
        "        Data1[i,j,:] = abs(out[0,:])\n",
        "      Class_label[i]=class_n\n",
        "\n",
        "    spectra=Data1[:,np.newaxis,:,:]\n",
        "    labels = Class_label\n",
        "\n",
        "    # measure the numbers of dataset shape\n",
        "    n_samples, n_channel, n_length,_ = spectra.shape\n",
        "    n_class = len(np.unique(labels))\n",
        "    settings = (n_samples, n_channel, n_length, n_class, device, split_ratio, batch_size)\n",
        "    settings_aug = (100, 120, 0.2, 0.2, 0.5)\n",
        "    # (window size, max peak shift size, probability of peak elimination,\n",
        "    #  probability of peak scailing, probability of peak shift)\n",
        "\n",
        "\n",
        "    # dataloaders\n",
        "    spectra_split, labels_split = random_data_split(spectra, labels, settings)\n",
        "    dataloader_train = AugmentedDataloader(\n",
        "        spectra_split[0],\n",
        "        labels_split[0],\n",
        "        settings,\n",
        "        settings_aug,\n",
        "    )\n",
        "\n",
        "    dataloader_val = DataLoader(\n",
        "        MyDataset(spectra_split[1],labels_split[1]),\n",
        "        batch_size=settings[6],\n",
        "        #drop_last=True,\n",
        "    )\n",
        "\n",
        "    dataloader_test = DataLoader(\n",
        "        MyDataset(spectra_split[2],labels_split[2]),\n",
        "        batch_size=settings[6],\n",
        "        #drop_last=True,\n",
        "    )\n",
        "\n",
        "    # compile dataloaders and settings\n",
        "    dataloaders = (dataloader_train, dataloader_val, dataloader_test)\n",
        "    return dataloaders, settings\n",
        "def load_model(settings):\n",
        "\n",
        "    #model = MSResNet(input_channel=1,layers=[4,2,5], num_classes=4)\n",
        "\n",
        "    model = Net(in_channels=1,\n",
        "                   n_class=12,\n",
        "                  )\n",
        "\n",
        "    model.to(settings[4])\n",
        "    model=model.double()\n",
        "    return model\n",
        "\n",
        "def top_k(pred, label, k:int = 1):\n",
        "    labels_dim = 1\n",
        "    k_labels = torch.topk(input=pred, k=k, dim=1, largest=True, sorted=True)[1]\n",
        "    a = ~torch.prod(\n",
        "        input = torch.abs(label.unsqueeze(labels_dim) - k_labels),\n",
        "        dim=labels_dim,\n",
        "    ).to(torch.bool)\n",
        "    a = a.to(torch.int8)\n",
        "    y_pred = a * label + (1-a) * k_labels[:,0]\n",
        "    acc = accuracy_score(y_pred.cpu(), label.cpu())*100\n",
        "    print(confusion_matrix(label.cpu(), y_pred.cpu()))\n",
        "    return acc\n",
        "def val(dataloader, settings, model, criterion):\n",
        "    running_loss = 0.0\n",
        "    running_corrects = 0.0\n",
        "    model.eval()\n",
        "    model.zero_grad()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, batch in enumerate(dataloader):\n",
        "            # test\n",
        "            input, label = tuple(t.to(settings[4]) for t in batch)\n",
        "            label = label.long()\n",
        "            model=model.to(settings[4])\n",
        "            pred = model(input.double())\n",
        "            loss = criterion(pred, label)\n",
        "\n",
        "            # evaluate\n",
        "            running_corrects += top_k(pred, label, k=1) * len(label)\n",
        "            running_loss += loss.item()\n",
        "            print('[Val] batch: '+str(batch_idx+1)+' | loss: '+str(loss.item()))\n",
        "    # summarise\n",
        "    n_test = 555#round(settings[0] * (1 - settings[5])/2)\n",
        "    epoch_loss = running_loss / n_test\n",
        "    epoch_acc = running_corrects / n_test\n",
        "    print('[Val total] loss: '+str(epoch_loss)+' | acc: '+str(epoch_acc))\n",
        "    return epoch_loss, epoch_acc"
      ],
      "metadata": {
        "id": "wJj7RrlvLiXO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1aa04123-d07c-4bd0-fdcc-f233aae3e855"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "C:\\Users\\artaa\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch = 200\n",
        "\n",
        "dataloaders, settings = dataloader_preparation(batch_size=batch,noise_std=0)\n",
        "\n",
        "model = load_model(settings)\n",
        "model=model.double()\n",
        "#model.load_state_dict(torch.load('Results_wdcnn/wdcnn1d_epoch261_wdcnn_mod.pt'))\n",
        "#model.load_state_dict(torch.load('Results_mlwdcnn/wdcnn1d_epoch199.pt'))\n",
        "criterion = nn.CrossEntropyLoss().to(settings[4])\n",
        "model.load_state_dict(torch.load('wdcnn2d_epoch_CWRU_final_final49.pt'))\n",
        "loss_val, acc_val = val(dataloaders[2], settings, model, criterion)"
      ],
      "metadata": {
        "id": "ehGhR7pbOl0H",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "210a6329-7199-4459-aa5b-4fc9b095cedc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "C:\\Users\\artaa\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.\n",
            "  warnings.warn('Lazy modules are a new feature under heavy development '\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[36  0  0  0  0  0  0  0  0  0  0  0]\n",
            " [ 0  7  0  0  0  0  0  0  0  0  0  0]\n",
            " [ 0  0 14  0  0  0  0  0  0  1  0  0]\n",
            " [ 0  0  0 11  0  0  0  0  0  0  0  0]\n",
            " [ 0  0  0  0  9  0  0  0  0  0  0  0]\n",
            " [ 0  0  1  0  0  8  0  0  0  1  0  0]\n",
            " [ 0  0  0  0  0  0 10  0  0  0  0  0]\n",
            " [ 0  0  0  0  0  1  0 12  0  0  1  0]\n",
            " [ 0  0  0  0  0  0  0  0 12  0  0  0]\n",
            " [ 0  0  0  0  0  0  0  0  0 35  0  0]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  6  0]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  0 35]]\n",
            "[Val] batch: 1 | loss: 0.09338184886902268\n",
            "[[43  0  0  0  0  0  0  0  0  0  0  0]\n",
            " [ 0 14  0  0  0  0  0  0  0  0  0  0]\n",
            " [ 0  0  7  0  0  0  0  0  0  0  0  0]\n",
            " [ 0  0  0  9  0  0  0  0  0  0  0  0]\n",
            " [ 0  0  0  0 16  0  0  0  0  0  0  0]\n",
            " [ 0  0  0  0  0  6  0  0  0  1  0  0]\n",
            " [ 0  0  0  0  0  0  8  0  0  0  0  0]\n",
            " [ 0  0  1  0  0  0  0 11  0  0  0  0]\n",
            " [ 0  0  0  0  0  0  0  0 10  0  0  0]\n",
            " [ 0  0  0  0  0  0  0  0  0 39  0  0]\n",
            " [ 0  0  1  0  0  0  0  0  0  0 10  0]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  0 24]]\n",
            "[Val] batch: 2 | loss: 0.07130763043834086\n",
            "[[28  0  0  0  0  0  0  0  0  0  0  0]\n",
            " [ 0  8  0  0  0  0  0  0  0  0  0  0]\n",
            " [ 0  0  8  0  0  0  0  0  0  0  0  0]\n",
            " [ 0  0  0  5  0  0  0  0  0  0  0  0]\n",
            " [ 0  0  0  0  5  0  0  0  0  0  0  0]\n",
            " [ 0  0  0  0  0  5  0  0  0  0  0  0]\n",
            " [ 0  0  0  0  0  0 12  0  0  0  0  0]\n",
            " [ 0  0  0  0  0  0  0  5  0  0  0  0]\n",
            " [ 0  0  0  0  0  0  0  0 13  0  0  0]\n",
            " [ 0  0  0  0  0  0  0  0  0 30  0  0]\n",
            " [ 0  0  0  0  0  0  0  0  0  0 10  0]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  0 26]]\n",
            "[Val] batch: 3 | loss: 0.05509247458853374\n",
            "[Val total] loss: 0.00039600352053314824 | acc: 98.55855855855856\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#plot data"
      ],
      "metadata": {
        "id": "K2mFM-ABLf-G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mat = scipy.io.loadmat('itsc_fault_data_MC_test_SVM_BLAC_test8.mat')\n",
        "Iq_csv = mat['Iq_csv']\n",
        "xrd_datasets1 = Iq_csv\n",
        "spectra_plt=xrd_datasets1[:,0:500]"
      ],
      "metadata": {
        "id": "1Mkrtkq8Ev9m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.plot(spectra_plt[2100,:]+np.random.normal(0,2.2,500),'r')\n",
        "plt.plot(spectra_plt[2100,:],'b')\n",
        "\n",
        "#plt.xlim([250,280])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 447
        },
        "id": "wDWH-IHVE1ut",
        "outputId": "63111fe9-e9c7-45cd-fc0f-69ef20304b66"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x1b5cf1e0610>]"
            ]
          },
          "metadata": {},
          "execution_count": 47
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Psig = np.var(spectra_plt[2100,:])\n",
        "noise = np.random.laplace(0,6,500)\n",
        "Pnos=np.var(noise)\n",
        "\n",
        "SNR=10*np.log10(Psig/Pnos)\n",
        "print(\"SNR = \"+str(SNR))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7MqyYEDTEyKf",
        "outputId": "df852809-4d69-4556-a1b3-f68854925a9b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SNR = -12.471718491274263\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "save_model(best_acc, epoch, model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RJwgPeg5_LB9",
        "outputId": "2146f0eb-8a8c-4bc8-96ee-ac5dbb4fbe5d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------> The best model has been replaced.\n",
            "epoch: 262 | best_acc: 99.39393939393939\n",
            "The best model has been saved in ./wdcnn1d_epoch262.pt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oYMctm11F3ZH"
      },
      "outputs": [],
      "source": [
        "pip install torchmetrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "58IcRMlVGf2c"
      },
      "outputs": [],
      "source": [
        "pip install pytorch_lightning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3FMgKnkhDjVJ"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import os\n",
        "os.chdir('/content/gdrive/My Drive/ResNet')\n",
        "import numpy as np\n",
        "xrd_datasets = np.loadtxt('Vib_data2.csv',delimiter=\",\")\n",
        "\n",
        "#spectra=xrd_datasets[:,0:2400]\n",
        "labels = xrd_datasets[:,12000]\n",
        "#print(spectra.shape)\n",
        "#plt.plot(spectra[10,:])\n",
        "A=xrd_datasets[:,0:2400]\n",
        "for i in range(4):\n",
        "    A=np.append(A,xrd_datasets[:,2400*(i+1):2400*(i+2)],axis=0)\n",
        "\n",
        "B_prim=xrd_datasets[:,12000]\n",
        "B=[]\n",
        "for i in range(5):\n",
        "    B=np.append(B,B_prim,axis=0)\n",
        "print(B.shape)\n",
        "B = np.reshape(B, (-1, 1))\n",
        "A=np.append(A,B,axis=1)\n",
        "print(A.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "InBC3DA0y6KB"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "os.chdir('/content/drive/My Drive/ITSC fault')\n",
        "df = pd.read_csv('learning_curve_PRO.csv', header= 'infer', sep=',')\n",
        "acc_loss=df.to_numpy()\n",
        "df2 = pd.read_csv('learning_curve_RESNET.csv', header= 'infer', sep=',')\n",
        "acc_loss2=df2.to_numpy()\n",
        "#acc_loss = np.loadtxt('learning_curve_PRO.csv',delimiter=\",\")\n",
        "plt.rcParams['font.serif'] = \"Times New Roman\"\n",
        "\n",
        "font = {'family' : 'serif',\n",
        "        'weight' : 'normal',\n",
        "        'size'   : 13}\n",
        "plt.rcdefaults()\n",
        "plt.rc('font', **font)\n",
        "\n",
        "plt.plot(acc_loss[0:21,1],label='Proposed')\n",
        "plt.plot(acc_loss2[0:21,1],'r--',label='1D-ResNet')\n",
        "plt.xlim([0, 20])\n",
        "#plt.ylim([0, 0.02])\n",
        "plt.xticks(np.arange(0, 19, step=2),np.arange(0, 20, step=2))\n",
        "plt.legend(loc=\"upper right\")\n",
        "plt.xlabel(\"epoch\")\n",
        "plt.ylabel(\"Training Loss\")\n",
        "plt.grid()\n",
        "plt.figure(2)\n",
        "plt.plot(acc_loss[0:21,2],label='Proposed')\n",
        "plt.plot(acc_loss2[0:21,2],'r--',label='1D-ResNet')\n",
        "plt.xlim([0, 20])\n",
        "#plt.ylim([70, 105])\n",
        "plt.xticks(np.arange(0, 19, step=2),np.arange(0, 20, step=2))\n",
        "plt.legend(loc=\"center right\")\n",
        "plt.xlabel(\"epoch\")\n",
        "plt.ylabel(\"Training Accuracy (%)\")\n",
        "plt.grid()\n",
        "plt.figure(3)\n",
        "plt.plot(acc_loss[0:21,4],label='Proposed')\n",
        "plt.plot(acc_loss2[0:21,4],'r--',label='1D-ResNet')\n",
        "plt.xlim([0, 20])\n",
        "#plt.ylim([70, 105])\n",
        "plt.xticks(np.arange(0, 19, step=2),np.arange(0, 20, step=2))\n",
        "plt.legend(loc=\"center right\")\n",
        "plt.xlabel(\"epoch\")\n",
        "plt.ylabel(\"Validation Accuracy (%)\")\n",
        "plt.grid()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#STFT training"
      ],
      "metadata": {
        "id": "OpxTPvhPs-5V"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RSJbMRlubsma",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "9b219c8a-b8ae-4ca4-a467-32f3755c83c8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "C:\\Users\\artaa\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "STFT time is:40.491140842437744\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "C:\\Users\\artaa\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.\n",
            "  warnings.warn('Lazy modules are a new feature under heavy development '\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ">>>>>> epoch 0 starts\n",
            "[Train] batch: 1 | loss: 2.500507261503309\n",
            "[Train] batch: 2 | loss: 2.290400509258085\n",
            "[Train] batch: 3 | loss: 2.773392924436436\n",
            "[Train] batch: 4 | loss: 2.5789247466801415\n",
            "[Train] batch: 5 | loss: 2.2644917555914774\n",
            "[Train] batch: 6 | loss: 2.485762031791303\n",
            "[Train] batch: 7 | loss: 2.322272843950113\n",
            "[Train] batch: 8 | loss: 2.329521916144054\n",
            "[Train] batch: 9 | loss: 2.074600156337288\n",
            "[Train] batch: 10 | loss: 2.1275930274677672\n",
            "[Train] batch: 11 | loss: 2.5219191961337364\n",
            "[Train] batch: 12 | loss: 2.2511613112367734\n",
            "[Train] batch: 13 | loss: 2.1015969473590554\n",
            "[Train] batch: 14 | loss: 2.0680209210510108\n",
            "[Train] batch: 15 | loss: 2.428964940156596\n",
            "[Train] batch: 16 | loss: 2.3419134161263027\n",
            "[Train] batch: 17 | loss: 2.0108405712745205\n",
            "[Train] batch: 18 | loss: 2.0888472439759163\n",
            "[Train] batch: 19 | loss: 1.9208683181510113\n",
            "[Train] batch: 20 | loss: 2.1438551444988683\n",
            "[Train] batch: 21 | loss: 1.952664997868622\n",
            "[Train] batch: 22 | loss: 1.8268587084636163\n",
            "[Train] batch: 23 | loss: 1.6604022402847742\n",
            "[Train] batch: 24 | loss: 2.0646621087132835\n",
            "[Train] batch: 25 | loss: 1.8873626745807588\n",
            "[Train] batch: 26 | loss: 2.049014370342539\n",
            "[Train] batch: 27 | loss: 1.7339885763002783\n",
            "[Train] batch: 28 | loss: 1.885135043708429\n",
            "[Train] batch: 29 | loss: 1.7592058133343007\n",
            "[Train] batch: 30 | loss: 1.8279041370884814\n",
            "[Train] batch: 31 | loss: 2.2201481262771057\n",
            "[Train] batch: 32 | loss: 1.5711051855971274\n",
            "[Train] batch: 33 | loss: 1.7960861054917499\n",
            "[Train] batch: 34 | loss: 1.8353357150171636\n",
            "[Train] batch: 35 | loss: 1.7156888165892847\n",
            "[Train] batch: 36 | loss: 1.8338741210683476\n",
            "[Train] batch: 37 | loss: 1.9865600971389359\n",
            "[Train] batch: 38 | loss: 1.7952204784892547\n",
            "[Train] batch: 39 | loss: 1.7764955961712512\n",
            "[Train] batch: 40 | loss: 1.6828531486858271\n",
            "[Train] batch: 41 | loss: 1.6005257502336296\n",
            "[Train] batch: 42 | loss: 1.7665142351473855\n",
            "[Train] batch: 43 | loss: 1.544343320021559\n",
            "[Train] batch: 44 | loss: 2.009583273690215\n",
            "[Train] batch: 45 | loss: 1.673194429927799\n",
            "[Train] batch: 46 | loss: 1.8966392617437915\n",
            "[Train] batch: 47 | loss: 1.751972233874517\n",
            "[Train] batch: 48 | loss: 1.3708349184638202\n",
            "[Train] batch: 49 | loss: 1.8046868976823711\n",
            "[Train] batch: 50 | loss: 1.8920378129535944\n",
            "[Train] batch: 51 | loss: 1.6620440168053288\n",
            "[Train] batch: 52 | loss: 1.5986116125330823\n",
            "[Train] batch: 53 | loss: 1.8695692130884918\n",
            "[Train] batch: 54 | loss: 1.7003685318464794\n",
            "[Train] batch: 55 | loss: 1.294092673438122\n",
            "[Train] batch: 56 | loss: 1.4815157687839322\n",
            "[Train] batch: 57 | loss: 1.4453299752763447\n",
            "[Train] batch: 58 | loss: 1.2366466797389326\n",
            "[Train] batch: 59 | loss: 1.882212678799848\n",
            "[Train] batch: 60 | loss: 1.2204332619101688\n",
            "[Train] batch: 61 | loss: 1.29657504626578\n",
            "[Train] batch: 62 | loss: 1.7684730922374388\n",
            "[Train] batch: 63 | loss: 1.925529900710746\n",
            "[Train] batch: 64 | loss: 1.6740251013939027\n",
            "[Train] batch: 65 | loss: 1.7650006081941654\n",
            "[Train] batch: 66 | loss: 1.2997421899030408\n",
            "[Train] batch: 67 | loss: 1.6613165257857148\n",
            "[Train] batch: 68 | loss: 1.6710868390822988\n",
            "[Train] batch: 69 | loss: 2.070771095206305\n",
            "[Train] batch: 70 | loss: 1.7182890301083098\n",
            "[Train] batch: 71 | loss: 1.610988859025561\n",
            "[Train] batch: 72 | loss: 1.6081779183043854\n",
            "[Train] batch: 73 | loss: 1.1970557502171602\n",
            "[Train] batch: 74 | loss: 1.457923873210422\n",
            "[Train] batch: 75 | loss: 1.8184372789496692\n",
            "[Train] batch: 76 | loss: 1.3738104536335363\n",
            "[Train] batch: 77 | loss: 1.3514386047997626\n",
            "[Train] batch: 78 | loss: 1.4725985235096017\n",
            "[Train] batch: 79 | loss: 1.5607027786013408\n",
            "[Train] batch: 80 | loss: 1.4860802648329645\n",
            "[Train] batch: 81 | loss: 1.5279180735984355\n",
            "[Train] batch: 82 | loss: 1.5926870821098198\n",
            "[Train] batch: 83 | loss: 1.5300440972496396\n",
            "[Train] batch: 84 | loss: 1.2751859271821853\n",
            "[Train] batch: 85 | loss: 1.2911161391293255\n",
            "[Train] batch: 86 | loss: 1.1724999582464908\n",
            "[Train] batch: 87 | loss: 1.0769237489937271\n",
            "[Train] batch: 88 | loss: 1.2430957677597851\n",
            "[Train] batch: 89 | loss: 1.139262741645082\n",
            "[Train] batch: 90 | loss: 1.0151902966062512\n",
            "[Train] batch: 91 | loss: 1.2885730197624972\n",
            "[Train] batch: 92 | loss: 1.34849381582699\n",
            "[Train] batch: 93 | loss: 1.3704735046516594\n",
            "[Train] batch: 94 | loss: 1.2229738198181617\n",
            "[Train] batch: 95 | loss: 1.769291437445412\n",
            "[Train] batch: 96 | loss: 1.3477494493509847\n",
            "[Train] batch: 97 | loss: 1.4355284000399122\n",
            "[Train] batch: 98 | loss: 1.0260986689618352\n",
            "[Train] batch: 99 | loss: 1.966059823669836\n",
            "[Train] batch: 100 | loss: 1.4874904555176651\n",
            "[Train] batch: 101 | loss: 1.3886380398365616\n",
            "[Train] batch: 102 | loss: 1.3804774581622519\n",
            "[Train] batch: 103 | loss: 1.1559734919024167\n",
            "[Train] batch: 104 | loss: 1.1853904375708708\n",
            "[Train] batch: 105 | loss: 1.7167195615924165\n",
            "[Train] batch: 106 | loss: 1.2740722116453982\n",
            "[Train] batch: 107 | loss: 1.1745003145118036\n",
            "[Train] batch: 108 | loss: 1.408579005689353\n",
            "[Train] batch: 109 | loss: 1.2625553428979004\n",
            "[Train] batch: 110 | loss: 1.1843416555928776\n",
            "[Train] batch: 111 | loss: 1.3791574149441548\n",
            "[Train] batch: 112 | loss: 1.5586781622442272\n",
            "[Train] batch: 113 | loss: 1.6316521638374153\n",
            "[Train] batch: 114 | loss: 1.1552391451227144\n",
            "[Train] batch: 115 | loss: 1.2538652334149647\n",
            "[Train] batch: 116 | loss: 1.1320463321011567\n",
            "[Train] batch: 117 | loss: 1.596460228015348\n",
            "[Train] batch: 118 | loss: 1.1293625964476184\n",
            "[Train] batch: 119 | loss: 1.3128837994860127\n",
            "[Train] batch: 120 | loss: 1.2995266961813705\n",
            "[Train] batch: 121 | loss: 0.9681998977210162\n",
            "[Train] batch: 122 | loss: 1.258249340753542\n",
            "[Train] batch: 123 | loss: 1.0843720204192295\n",
            "[Train] batch: 124 | loss: 0.7729309900449858\n",
            "[Train] batch: 125 | loss: 1.222277647836336\n",
            "[Train] batch: 126 | loss: 1.0743159622079712\n",
            "[Train] batch: 127 | loss: 1.5210004082507684\n",
            "[Train] batch: 128 | loss: 1.0549807684667039\n",
            "[Train] batch: 129 | loss: 1.4113477317520133\n",
            "[Train] batch: 130 | loss: 1.4062206529376213\n",
            "[Train] batch: 131 | loss: 1.2388317218957876\n",
            "[Train] batch: 132 | loss: 1.0702748274301312\n",
            "[Train] batch: 133 | loss: 1.0881303648469165\n",
            "[Train] batch: 134 | loss: 1.4177703524568999\n",
            "[Train] batch: 135 | loss: 1.1290741663527324\n",
            "[Train] batch: 136 | loss: 1.1059443741612278\n",
            "[Train] batch: 137 | loss: 0.9978740627715963\n",
            "[Train] batch: 138 | loss: 1.3846946154532325\n",
            "[Train] batch: 139 | loss: 1.4365613680279847\n",
            "[Train] batch: 140 | loss: 0.9810005058153005\n",
            "[Train] batch: 141 | loss: 1.7561031694182636\n",
            "[Train] batch: 142 | loss: 1.2349012905586034\n",
            "[Train] batch: 143 | loss: 1.3322543183970517\n",
            "[Train] batch: 144 | loss: 1.202333438415089\n",
            "[Train] batch: 145 | loss: 0.9182285120413276\n",
            "[Train] batch: 146 | loss: 1.5658819697998647\n",
            "[Train] batch: 147 | loss: 1.427503278082068\n",
            "[Train] batch: 148 | loss: 1.2798720284938077\n",
            "[Train] batch: 149 | loss: 1.2252147939279117\n",
            "[Train] batch: 150 | loss: 0.8175254612801035\n",
            "[Train] batch: 151 | loss: 1.084392834191337\n",
            "[Train] batch: 152 | loss: 1.1524719357208686\n",
            "[Train] batch: 153 | loss: 1.3545342589959264\n",
            "[Train] batch: 154 | loss: 1.180896119303324\n",
            "[Train] batch: 155 | loss: 1.2233452302503012\n",
            "[Train] batch: 156 | loss: 1.2427900403667655\n",
            "[Train] batch: 157 | loss: 1.4636395666872064\n",
            "[Train] batch: 158 | loss: 0.7543644860808367\n",
            "[Train] batch: 159 | loss: 1.3858180265330287\n",
            "[Train] batch: 160 | loss: 0.7743623132681597\n",
            "[Train] batch: 161 | loss: 1.0762963308103426\n",
            "[Train] batch: 162 | loss: 1.4155869426265375\n",
            "[Train] batch: 163 | loss: 1.5507003586070631\n",
            "[Train] batch: 164 | loss: 1.11723137243152\n",
            "[Train] batch: 165 | loss: 1.6067207628509383\n",
            "[Train] batch: 166 | loss: 1.1319115900765107\n",
            "[Train] batch: 167 | loss: 1.2675918053294042\n",
            "[Train] batch: 168 | loss: 1.1951953991337259\n",
            "[Train] batch: 169 | loss: 1.0448369172590706\n",
            "[Train] batch: 170 | loss: 1.1920722913423805\n",
            "[Train] batch: 171 | loss: 1.0891873068260784\n",
            "[Train] batch: 172 | loss: 0.8540711380174997\n",
            "[Train] batch: 173 | loss: 1.230595432836445\n",
            "[Train] batch: 174 | loss: 1.0742134237434815\n",
            "[Train] batch: 175 | loss: 0.9461949083513407\n",
            "[Train] batch: 176 | loss: 0.9072763087508726\n",
            "[Train] batch: 177 | loss: 1.060375358752908\n",
            "[Train] batch: 178 | loss: 1.186327957995843\n",
            "[Train] batch: 179 | loss: 0.7982038824552881\n",
            "[Train] batch: 180 | loss: 1.2194983068253002\n",
            "[Train] batch: 181 | loss: 1.4044405913220213\n",
            "[Train] batch: 182 | loss: 1.1421572908056858\n",
            "[Train] batch: 183 | loss: 1.42493134011422\n",
            "[Train] batch: 184 | loss: 1.2046157183472492\n",
            "[Train] batch: 185 | loss: 1.116578127776276\n",
            "[Train total] loss: 0.10731246052425901 | acc: 54.4015444015444\n",
            "[Val] batch: 1 | loss: 1.4962769730572578\n",
            "[Val] batch: 2 | loss: 0.8772874751616536\n",
            "[Val] batch: 3 | loss: 1.075809360157685\n",
            "[Val] batch: 4 | loss: 0.7627309759703716\n",
            "[Val] batch: 5 | loss: 1.062393009245432\n",
            "[Val] batch: 6 | loss: 1.3781719426268328\n",
            "[Val] batch: 7 | loss: 0.9604176232735122\n",
            "[Val] batch: 8 | loss: 0.9640437564820472\n",
            "[Val] batch: 9 | loss: 0.9146121602004139\n",
            "[Val] batch: 10 | loss: 1.2454575100957008\n",
            "[Val] batch: 11 | loss: 1.3622461705877467\n",
            "[Val] batch: 12 | loss: 1.0948319968492883\n",
            "[Val] batch: 13 | loss: 1.4183501419423703\n",
            "[Val] batch: 14 | loss: 0.7032537471310386\n",
            "[Val] batch: 15 | loss: 0.687855958877748\n",
            "[Val] batch: 16 | loss: 1.0138648049875143\n",
            "[Val] batch: 17 | loss: 0.7059736611386075\n",
            "[Val] batch: 18 | loss: 0.8717138420575516\n",
            "[Val] batch: 19 | loss: 0.6852884144356273\n",
            "[Val] batch: 20 | loss: 1.4461821258812635\n",
            "[Val] batch: 21 | loss: 1.332743698000079\n",
            "[Val] batch: 22 | loss: 0.7306370996171008\n",
            "[Val] batch: 23 | loss: 1.3204358474722506\n",
            "[Val] batch: 24 | loss: 1.1853409339039334\n",
            "[Val] batch: 25 | loss: 0.8462054883343353\n",
            "[Val] batch: 26 | loss: 0.9049391930040176\n",
            "[Val] batch: 27 | loss: 0.7075080711993128\n",
            "[Val] batch: 28 | loss: 0.7449983146292171\n",
            "[Val] batch: 29 | loss: 1.1977324413074886\n",
            "[Val] batch: 30 | loss: 1.0519893156039124\n",
            "[Val] batch: 31 | loss: 1.3208791346800681\n",
            "[Val] batch: 32 | loss: 1.511390811359021\n",
            "[Val] batch: 33 | loss: 0.920989994625155\n",
            "[Val] batch: 34 | loss: 1.081850664813566\n",
            "[Val] batch: 35 | loss: 0.936251740800609\n",
            "[Val] batch: 36 | loss: 0.765836091437878\n",
            "[Val] batch: 37 | loss: 0.9964575027479798\n",
            "[Val] batch: 38 | loss: 0.7331681590794297\n",
            "[Val] batch: 39 | loss: 0.8772805652896775\n",
            "[Val] batch: 40 | loss: 1.1806252048583206\n",
            "[Val total] loss: 0.0740072467079694 | acc: 72.97297297297297\n",
            "[Val] batch: 1 | loss: 0.837480029358048\n",
            "[Val] batch: 2 | loss: 1.3638705396510848\n",
            "[Val] batch: 3 | loss: 1.1374298748663532\n",
            "[Val] batch: 4 | loss: 1.2811398577556727\n",
            "[Val] batch: 5 | loss: 1.3500359928608587\n",
            "[Val] batch: 6 | loss: 1.233942926103905\n",
            "[Val] batch: 7 | loss: 0.5480407653580243\n",
            "[Val] batch: 8 | loss: 0.9360020946888604\n",
            "[Val] batch: 9 | loss: 0.7851274876718276\n",
            "[Val] batch: 10 | loss: 1.2514114143895976\n",
            "[Val] batch: 11 | loss: 1.3549014610553851\n",
            "[Val] batch: 12 | loss: 0.9116762735098938\n",
            "[Val] batch: 13 | loss: 1.2391096936078774\n",
            "[Val] batch: 14 | loss: 0.9816086681602648\n",
            "[Val] batch: 15 | loss: 0.8221330315815522\n",
            "[Val] batch: 16 | loss: 0.8030851823679511\n",
            "[Val] batch: 17 | loss: 0.9948451036635539\n",
            "[Val] batch: 18 | loss: 1.1205886124241304\n",
            "[Val] batch: 19 | loss: 0.9315520403809946\n",
            "[Val] batch: 20 | loss: 0.9482292522183134\n",
            "[Val] batch: 21 | loss: 1.1184336144711398\n",
            "[Val] batch: 22 | loss: 1.459581789872116\n",
            "[Val] batch: 23 | loss: 1.251073320778604\n",
            "[Val] batch: 24 | loss: 0.9088601729321371\n",
            "[Val] batch: 25 | loss: 0.6600540733752756\n",
            "[Val] batch: 26 | loss: 1.1147262213245677\n",
            "[Val] batch: 27 | loss: 1.136715839804398\n",
            "[Val] batch: 28 | loss: 0.9241488521588349\n",
            "[Val] batch: 29 | loss: 0.9545504377120839\n",
            "[Val] batch: 30 | loss: 0.6261886584014208\n",
            "[Val] batch: 31 | loss: 1.095483163930928\n",
            "[Val] batch: 32 | loss: 1.0193986197921034\n",
            "[Val] batch: 33 | loss: 1.1655104662453348\n",
            "[Val] batch: 34 | loss: 1.2566257905686125\n",
            "[Val] batch: 35 | loss: 1.1718782743144072\n",
            "[Val] batch: 36 | loss: 1.3261272719699106\n",
            "[Val] batch: 37 | loss: 1.0343316251277554\n",
            "[Val] batch: 38 | loss: 1.1249072461091607\n",
            "[Val] batch: 39 | loss: 0.99477521606826\n",
            "[Val] batch: 40 | loss: 0.8295093558771308\n",
            "[Val total] loss: 0.07568484740992493 | acc: 71.89189189189189\n",
            ">>>>>> epoch 1 starts\n",
            "[Train] batch: 1 | loss: 1.0045160335296244\n",
            "[Train] batch: 2 | loss: 0.676029523277785\n",
            "[Train] batch: 3 | loss: 0.8019659748638664\n",
            "[Train] batch: 4 | loss: 0.8808217798098475\n",
            "[Train] batch: 5 | loss: 1.0636741578709379\n",
            "[Train] batch: 6 | loss: 1.0960727384752178\n",
            "[Train] batch: 7 | loss: 1.3770927200798326\n",
            "[Train] batch: 8 | loss: 1.220793070661513\n",
            "[Train] batch: 9 | loss: 1.2642364374521493\n",
            "[Train] batch: 10 | loss: 1.1487173615026338\n",
            "[Train] batch: 11 | loss: 1.442637233943195\n",
            "[Train] batch: 12 | loss: 1.062373033710417\n",
            "[Train] batch: 13 | loss: 1.2732382195354581\n",
            "[Train] batch: 14 | loss: 0.870318496326316\n",
            "[Train] batch: 15 | loss: 0.9892222158390032\n",
            "[Train] batch: 16 | loss: 1.0670062019122428\n",
            "[Train] batch: 17 | loss: 1.1700821814369742\n",
            "[Train] batch: 18 | loss: 1.1442402298532937\n",
            "[Train] batch: 19 | loss: 0.9675322662579119\n",
            "[Train] batch: 20 | loss: 1.0803265927742225\n",
            "[Train] batch: 21 | loss: 1.061407577211259\n",
            "[Train] batch: 22 | loss: 0.7682732179775628\n",
            "[Train] batch: 23 | loss: 0.8590281372898433\n",
            "[Train] batch: 24 | loss: 0.7008046830482753\n",
            "[Train] batch: 25 | loss: 1.0835572219268126\n",
            "[Train] batch: 26 | loss: 0.8252143418042966\n",
            "[Train] batch: 27 | loss: 0.8500580939584298\n",
            "[Train] batch: 28 | loss: 0.8622718107760271\n",
            "[Train] batch: 29 | loss: 0.641134309849815\n",
            "[Train] batch: 30 | loss: 1.0408494370044425\n",
            "[Train] batch: 31 | loss: 0.47583215130370576\n",
            "[Train] batch: 32 | loss: 0.9998483751324126\n",
            "[Train] batch: 33 | loss: 1.0756038975091708\n",
            "[Train] batch: 34 | loss: 1.0910039004457448\n",
            "[Train] batch: 35 | loss: 0.884400337229124\n",
            "[Train] batch: 36 | loss: 0.76951955350815\n",
            "[Train] batch: 37 | loss: 0.9299055895166056\n",
            "[Train] batch: 38 | loss: 0.7164868473026594\n",
            "[Train] batch: 39 | loss: 0.7990631782661434\n",
            "[Train] batch: 40 | loss: 1.045698952367734\n",
            "[Train] batch: 41 | loss: 0.8034265223690097\n",
            "[Train] batch: 42 | loss: 1.3603811523972096\n",
            "[Train] batch: 43 | loss: 0.7341144138685479\n",
            "[Train] batch: 44 | loss: 1.5620113231096444\n",
            "[Train] batch: 45 | loss: 0.846143665507381\n",
            "[Train] batch: 46 | loss: 1.2880963529721423\n",
            "[Train] batch: 47 | loss: 1.1966535084203591\n",
            "[Train] batch: 48 | loss: 0.8688412357083315\n",
            "[Train] batch: 49 | loss: 1.1012595126961207\n",
            "[Train] batch: 50 | loss: 1.0527758702647654\n",
            "[Train] batch: 51 | loss: 0.8271698296056895\n",
            "[Train] batch: 52 | loss: 0.8523977791023236\n",
            "[Train] batch: 53 | loss: 0.9118066677025075\n",
            "[Train] batch: 54 | loss: 0.9754951542948266\n",
            "[Train] batch: 55 | loss: 0.8415409504334738\n",
            "[Train] batch: 56 | loss: 0.7856442905084214\n",
            "[Train] batch: 57 | loss: 0.8861162246642492\n",
            "[Train] batch: 58 | loss: 0.8110851194667072\n",
            "[Train] batch: 59 | loss: 0.8884016080984126\n",
            "[Train] batch: 60 | loss: 1.0028906653548642\n",
            "[Train] batch: 61 | loss: 1.1019781220482108\n",
            "[Train] batch: 62 | loss: 0.8632050671070429\n",
            "[Train] batch: 63 | loss: 1.2311023258252551\n",
            "[Train] batch: 64 | loss: 0.8145655727144181\n",
            "[Train] batch: 65 | loss: 1.4148216593030203\n",
            "[Train] batch: 66 | loss: 1.2241181531716738\n",
            "[Train] batch: 67 | loss: 1.0981031771137055\n",
            "[Train] batch: 68 | loss: 0.9651942176926328\n",
            "[Train] batch: 69 | loss: 0.5020606311710847\n",
            "[Train] batch: 70 | loss: 1.0275004101514345\n",
            "[Train] batch: 71 | loss: 0.7568403087910758\n",
            "[Train] batch: 72 | loss: 0.936429744429388\n",
            "[Train] batch: 73 | loss: 0.6953194989153125\n",
            "[Train] batch: 74 | loss: 1.0650496130133789\n",
            "[Train] batch: 75 | loss: 1.0917487986596386\n",
            "[Train] batch: 76 | loss: 1.0413572682486267\n",
            "[Train] batch: 77 | loss: 0.8125924249202142\n",
            "[Train] batch: 78 | loss: 0.7540196527490577\n",
            "[Train] batch: 79 | loss: 1.418062459958547\n",
            "[Train] batch: 80 | loss: 0.5557370886797269\n",
            "[Train] batch: 81 | loss: 0.7150210704255319\n",
            "[Train] batch: 82 | loss: 0.9438618462237904\n",
            "[Train] batch: 83 | loss: 0.9986891563203465\n",
            "[Train] batch: 84 | loss: 0.920239131849199\n",
            "[Train] batch: 85 | loss: 0.7902172984340305\n",
            "[Train] batch: 86 | loss: 0.6988943640175745\n",
            "[Train] batch: 87 | loss: 0.5240590406170045\n",
            "[Train] batch: 88 | loss: 0.7372546788692821\n",
            "[Train] batch: 89 | loss: 0.9887676939032873\n",
            "[Train] batch: 90 | loss: 0.8680483594148601\n",
            "[Train] batch: 91 | loss: 0.6648187643938852\n",
            "[Train] batch: 92 | loss: 0.7626968584883728\n",
            "[Train] batch: 93 | loss: 0.8771822414989067\n",
            "[Train] batch: 94 | loss: 0.6422929027732954\n",
            "[Train] batch: 95 | loss: 0.8124622928429196\n",
            "[Train] batch: 96 | loss: 0.7628588844763264\n",
            "[Train] batch: 97 | loss: 0.5540086469501792\n",
            "[Train] batch: 98 | loss: 0.7400542240639361\n",
            "[Train] batch: 99 | loss: 0.7255184161700712\n",
            "[Train] batch: 100 | loss: 1.0341731957388036\n",
            "[Train] batch: 101 | loss: 0.7568578261785408\n",
            "[Train] batch: 102 | loss: 0.5561570145254822\n",
            "[Train] batch: 103 | loss: 0.8396145505932345\n",
            "[Train] batch: 104 | loss: 0.953697775915335\n",
            "[Train] batch: 105 | loss: 1.0598067285202626\n",
            "[Train] batch: 106 | loss: 0.9577778044922894\n",
            "[Train] batch: 107 | loss: 0.7722196089932928\n",
            "[Train] batch: 108 | loss: 0.649012542353319\n",
            "[Train] batch: 109 | loss: 0.9363977533951908\n",
            "[Train] batch: 110 | loss: 1.1784365743381369\n",
            "[Train] batch: 111 | loss: 0.8586220966485063\n",
            "[Train] batch: 112 | loss: 0.8043832876934472\n",
            "[Train] batch: 113 | loss: 0.8015889022322138\n",
            "[Train] batch: 114 | loss: 0.8833307367563993\n",
            "[Train] batch: 115 | loss: 1.00580801457912\n",
            "[Train] batch: 116 | loss: 0.6741704843385464\n",
            "[Train] batch: 117 | loss: 0.66370157372319\n",
            "[Train] batch: 118 | loss: 0.9867610299501897\n",
            "[Train] batch: 119 | loss: 0.817831801804965\n",
            "[Train] batch: 120 | loss: 0.7839149844182822\n",
            "[Train] batch: 121 | loss: 0.5836217593293431\n",
            "[Train] batch: 122 | loss: 0.6466478788301313\n",
            "[Train] batch: 123 | loss: 0.6991248639558011\n",
            "[Train] batch: 124 | loss: 1.078002971154691\n",
            "[Train] batch: 125 | loss: 1.1475970325158111\n",
            "[Train] batch: 126 | loss: 0.8285668247287594\n",
            "[Train] batch: 127 | loss: 0.600381584287813\n",
            "[Train] batch: 128 | loss: 0.6220512951662417\n",
            "[Train] batch: 129 | loss: 0.5963101307037554\n",
            "[Train] batch: 130 | loss: 0.7941585648770351\n",
            "[Train] batch: 131 | loss: 0.8873365731858653\n",
            "[Train] batch: 132 | loss: 0.5606342994008174\n",
            "[Train] batch: 133 | loss: 0.8646107278233286\n",
            "[Train] batch: 134 | loss: 0.6163748287744619\n",
            "[Train] batch: 135 | loss: 0.7103712507700272\n",
            "[Train] batch: 136 | loss: 0.8656284034251527\n",
            "[Train] batch: 137 | loss: 0.9288747989708046\n",
            "[Train] batch: 138 | loss: 0.513554211903054\n",
            "[Train] batch: 139 | loss: 0.6923937704826779\n",
            "[Train] batch: 140 | loss: 1.3247664436082298\n",
            "[Train] batch: 141 | loss: 0.8588568797392321\n",
            "[Train] batch: 142 | loss: 0.5796166521891862\n",
            "[Train] batch: 143 | loss: 1.3687078932821302\n",
            "[Train] batch: 144 | loss: 0.8058796175925663\n",
            "[Train] batch: 145 | loss: 0.7220227970270191\n",
            "[Train] batch: 146 | loss: 0.47864256413087763\n",
            "[Train] batch: 147 | loss: 0.7903396280514812\n",
            "[Train] batch: 148 | loss: 0.7369915642321324\n",
            "[Train] batch: 149 | loss: 0.6547206796926217\n",
            "[Train] batch: 150 | loss: 0.8303218723687477\n",
            "[Train] batch: 151 | loss: 1.3058370163360098\n",
            "[Train] batch: 152 | loss: 0.923608977688198\n",
            "[Train] batch: 153 | loss: 0.7570130370856468\n",
            "[Train] batch: 154 | loss: 0.5473389507464405\n",
            "[Train] batch: 155 | loss: 0.8094558020739362\n",
            "[Train] batch: 156 | loss: 0.9237075990896937\n",
            "[Train] batch: 157 | loss: 0.7719202545551854\n",
            "[Train] batch: 158 | loss: 1.0186962111892908\n",
            "[Train] batch: 159 | loss: 1.188679170159182\n",
            "[Train] batch: 160 | loss: 0.714457463636822\n",
            "[Train] batch: 161 | loss: 0.6672347758620772\n",
            "[Train] batch: 162 | loss: 0.8175006004276141\n",
            "[Train] batch: 163 | loss: 0.8362998759725789\n",
            "[Train] batch: 164 | loss: 0.8257313759943214\n",
            "[Train] batch: 165 | loss: 0.6460947391263331\n",
            "[Train] batch: 166 | loss: 0.6752455397044909\n",
            "[Train] batch: 167 | loss: 0.3967244061816134\n",
            "[Train] batch: 168 | loss: 0.7133169215777991\n",
            "[Train] batch: 169 | loss: 0.9443442707120216\n",
            "[Train] batch: 170 | loss: 0.7173073885605625\n",
            "[Train] batch: 171 | loss: 0.7234380334190567\n",
            "[Train] batch: 172 | loss: 0.9289147105464831\n",
            "[Train] batch: 173 | loss: 0.3925999942588243\n",
            "[Train] batch: 174 | loss: 0.8682705119404928\n",
            "[Train] batch: 175 | loss: 0.27930343482261716\n",
            "[Train] batch: 176 | loss: 0.7977699158567864\n",
            "[Train] batch: 177 | loss: 0.7963585485137884\n",
            "[Train] batch: 178 | loss: 0.730576805836244\n",
            "[Train] batch: 179 | loss: 0.7312434262291988\n",
            "[Train] batch: 180 | loss: 0.7777470866710857\n",
            "[Train] batch: 181 | loss: 0.6325639870077335\n",
            "[Train] batch: 182 | loss: 0.8686136026951222\n",
            "[Train] batch: 183 | loss: 0.7255211779708134\n",
            "[Train] batch: 184 | loss: 0.820304359316386\n",
            "[Train] batch: 185 | loss: 1.0529038565033624\n",
            "[Train total] loss: 0.062340493618972266 | acc: 72.97297297297297\n",
            "[Val] batch: 1 | loss: 1.0297307365480877\n",
            "[Val] batch: 2 | loss: 0.6708983535184353\n",
            "[Val] batch: 3 | loss: 0.7921275077128092\n",
            "[Val] batch: 4 | loss: 0.4522664288093834\n",
            "[Val] batch: 5 | loss: 0.6236313219812212\n",
            "[Val] batch: 6 | loss: 0.9546961121028278\n",
            "[Val] batch: 7 | loss: 0.7698258037959024\n",
            "[Val] batch: 8 | loss: 0.6952746463922809\n",
            "[Val] batch: 9 | loss: 0.6483450399179767\n",
            "[Val] batch: 10 | loss: 0.8889972317426612\n",
            "[Val] batch: 11 | loss: 1.0011741685174063\n",
            "[Val] batch: 12 | loss: 0.780050415230457\n",
            "[Val] batch: 13 | loss: 1.047133978712311\n",
            "[Val] batch: 14 | loss: 0.3455217867442751\n",
            "[Val] batch: 15 | loss: 0.44195040783611345\n",
            "[Val] batch: 16 | loss: 0.6288473911641617\n",
            "[Val] batch: 17 | loss: 0.3587629024498236\n",
            "[Val] batch: 18 | loss: 0.5293449211075734\n",
            "[Val] batch: 19 | loss: 0.4554227919770585\n",
            "[Val] batch: 20 | loss: 1.0059769855841425\n",
            "[Val] batch: 21 | loss: 1.0922417112626441\n",
            "[Val] batch: 22 | loss: 0.452577458176419\n",
            "[Val] batch: 23 | loss: 0.9306462177588565\n",
            "[Val] batch: 24 | loss: 0.888896289438602\n",
            "[Val] batch: 25 | loss: 0.5527576940214779\n",
            "[Val] batch: 26 | loss: 0.5893459369239907\n",
            "[Val] batch: 27 | loss: 0.4925830616716796\n",
            "[Val] batch: 28 | loss: 0.4313723979387526\n",
            "[Val] batch: 29 | loss: 0.8864401015696511\n",
            "[Val] batch: 30 | loss: 0.7669219269173937\n",
            "[Val] batch: 31 | loss: 0.9741750156622802\n",
            "[Val] batch: 32 | loss: 1.2057348342765213\n",
            "[Val] batch: 33 | loss: 0.6483956480736061\n",
            "[Val] batch: 34 | loss: 0.7015696009541507\n",
            "[Val] batch: 35 | loss: 0.6738133448139874\n",
            "[Val] batch: 36 | loss: 0.5022688037476747\n",
            "[Val] batch: 37 | loss: 0.6738377644142595\n",
            "[Val] batch: 38 | loss: 0.5227881763581289\n",
            "[Val] batch: 39 | loss: 0.5245360594265903\n",
            "[Val] batch: 40 | loss: 0.7538128134309363\n",
            "[Val total] loss: 0.05114359241204056 | acc: 78.1981981981982\n",
            "[Val] batch: 1 | loss: 0.487034618661332\n",
            "[Val] batch: 2 | loss: 0.9478951729316684\n",
            "[Val] batch: 3 | loss: 0.8730972072980047\n",
            "[Val] batch: 4 | loss: 0.9243880498722213\n",
            "[Val] batch: 5 | loss: 1.042990655060472\n",
            "[Val] batch: 6 | loss: 0.8205456478751464\n",
            "[Val] batch: 7 | loss: 0.3356416334162476\n",
            "[Val] batch: 8 | loss: 0.6150395986864144\n",
            "[Val] batch: 9 | loss: 0.4910779093014742\n",
            "[Val] batch: 10 | loss: 0.9682808356185061\n",
            "[Val] batch: 11 | loss: 0.9142116730991265\n",
            "[Val] batch: 12 | loss: 0.678492937523891\n",
            "[Val] batch: 13 | loss: 0.8351963692653204\n",
            "[Val] batch: 14 | loss: 0.7030246109335936\n",
            "[Val] batch: 15 | loss: 0.5756520195978985\n",
            "[Val] batch: 16 | loss: 0.5300031235189331\n",
            "[Val] batch: 17 | loss: 0.6184293226376482\n",
            "[Val] batch: 18 | loss: 0.8437416188252115\n",
            "[Val] batch: 19 | loss: 0.5861554020356454\n",
            "[Val] batch: 20 | loss: 0.7243821782930306\n",
            "[Val] batch: 21 | loss: 0.7343356892868295\n",
            "[Val] batch: 22 | loss: 1.1008592752712403\n",
            "[Val] batch: 23 | loss: 0.8354921174569612\n",
            "[Val] batch: 24 | loss: 0.6925960743498731\n",
            "[Val] batch: 25 | loss: 0.30599940917608665\n",
            "[Val] batch: 26 | loss: 0.7784839065739524\n",
            "[Val] batch: 27 | loss: 0.8407014744555397\n",
            "[Val] batch: 28 | loss: 0.7028171712068624\n",
            "[Val] batch: 29 | loss: 0.614908160693379\n",
            "[Val] batch: 30 | loss: 0.3957918405203113\n",
            "[Val] batch: 31 | loss: 0.7301361443598525\n",
            "[Val] batch: 32 | loss: 0.7639737623353952\n",
            "[Val] batch: 33 | loss: 0.6975785157867846\n",
            "[Val] batch: 34 | loss: 0.8602212305327542\n",
            "[Val] batch: 35 | loss: 0.7570845609592597\n",
            "[Val] batch: 36 | loss: 1.0092100331709035\n",
            "[Val] batch: 37 | loss: 0.6825125596776739\n",
            "[Val] batch: 38 | loss: 0.8415672019133724\n",
            "[Val] batch: 39 | loss: 0.7868944324763044\n",
            "[Val] batch: 40 | loss: 0.4497148400443375\n",
            "[Val total] loss: 0.05242551168414317 | acc: 78.01801801801801\n",
            ">>>>>> epoch 2 starts\n",
            "[Train] batch: 1 | loss: 0.6945720731966901\n",
            "[Train] batch: 2 | loss: 0.5387105413870412\n",
            "[Train] batch: 3 | loss: 0.5947239154628262\n",
            "[Train] batch: 4 | loss: 0.8003475836941497\n",
            "[Train] batch: 5 | loss: 1.015128127452754\n",
            "[Train] batch: 6 | loss: 0.9953226163450598\n",
            "[Train] batch: 7 | loss: 0.7655632682086286\n",
            "[Train] batch: 8 | loss: 0.5336428382540671\n",
            "[Train] batch: 9 | loss: 0.6174820986130822\n",
            "[Train] batch: 10 | loss: 0.7753016807659484\n",
            "[Train] batch: 11 | loss: 0.4101875149724091\n",
            "[Train] batch: 12 | loss: 0.6241091853724902\n",
            "[Train] batch: 13 | loss: 0.8900581599066373\n",
            "[Train] batch: 14 | loss: 0.3850170826340765\n",
            "[Train] batch: 15 | loss: 0.6776778277387462\n",
            "[Train] batch: 16 | loss: 0.406267675818266\n",
            "[Train] batch: 17 | loss: 0.7562456823967232\n",
            "[Train] batch: 18 | loss: 0.8282621625492407\n",
            "[Train] batch: 19 | loss: 0.8072049026658734\n",
            "[Train] batch: 20 | loss: 0.6013644362216647\n",
            "[Train] batch: 21 | loss: 0.696623626796999\n",
            "[Train] batch: 22 | loss: 0.6780269210469123\n",
            "[Train] batch: 23 | loss: 0.578252209003078\n",
            "[Train] batch: 24 | loss: 0.9206110979730153\n",
            "[Train] batch: 25 | loss: 0.5416689477545635\n",
            "[Train] batch: 26 | loss: 0.5025070389673693\n",
            "[Train] batch: 27 | loss: 0.7452932991803636\n",
            "[Train] batch: 28 | loss: 0.8885054625127242\n",
            "[Train] batch: 29 | loss: 1.1046360947237652\n",
            "[Train] batch: 30 | loss: 0.8591942302520825\n",
            "[Train] batch: 31 | loss: 0.9852503000966858\n",
            "[Train] batch: 32 | loss: 0.7515033792460739\n",
            "[Train] batch: 33 | loss: 0.6906548676538139\n",
            "[Train] batch: 34 | loss: 0.5392596263716353\n",
            "[Train] batch: 35 | loss: 0.6904700718130129\n",
            "[Train] batch: 36 | loss: 0.27576380740473405\n",
            "[Train] batch: 37 | loss: 0.863852927542124\n",
            "[Train] batch: 38 | loss: 0.5239759917945145\n",
            "[Train] batch: 39 | loss: 0.465055781362984\n",
            "[Train] batch: 40 | loss: 0.7806955077701102\n",
            "[Train] batch: 41 | loss: 0.5518930171113992\n",
            "[Train] batch: 42 | loss: 0.6764997445754737\n",
            "[Train] batch: 43 | loss: 0.7810265153526911\n",
            "[Train] batch: 44 | loss: 0.8086430891299451\n",
            "[Train] batch: 45 | loss: 0.9883787112978029\n",
            "[Train] batch: 46 | loss: 0.7656534706932305\n",
            "[Train] batch: 47 | loss: 0.6579547856547493\n",
            "[Train] batch: 48 | loss: 0.3115497838398045\n",
            "[Train] batch: 49 | loss: 0.5863630466025789\n",
            "[Train] batch: 50 | loss: 0.7041969013564164\n",
            "[Train] batch: 51 | loss: 0.8939749586575626\n",
            "[Train] batch: 52 | loss: 0.6037479772960124\n",
            "[Train] batch: 53 | loss: 0.7492158643894484\n",
            "[Train] batch: 54 | loss: 0.9303200792425599\n",
            "[Train] batch: 55 | loss: 0.8927593677858408\n",
            "[Train] batch: 56 | loss: 0.4687081236060858\n",
            "[Train] batch: 57 | loss: 0.46987106490285263\n",
            "[Train] batch: 58 | loss: 0.530114481238834\n",
            "[Train] batch: 59 | loss: 0.8276046322776098\n",
            "[Train] batch: 60 | loss: 0.7470264051104742\n",
            "[Train] batch: 61 | loss: 0.9580436239704425\n",
            "[Train] batch: 62 | loss: 0.4657006129522589\n",
            "[Train] batch: 63 | loss: 0.6112461048627615\n",
            "[Train] batch: 64 | loss: 0.7887299343292613\n",
            "[Train] batch: 65 | loss: 0.6844230153847478\n",
            "[Train] batch: 66 | loss: 0.5940081495897415\n",
            "[Train] batch: 67 | loss: 0.7323385081940543\n",
            "[Train] batch: 68 | loss: 0.758181659037116\n",
            "[Train] batch: 69 | loss: 0.6103283136346149\n",
            "[Train] batch: 70 | loss: 0.8993819727278167\n",
            "[Train] batch: 71 | loss: 0.8243181918675017\n",
            "[Train] batch: 72 | loss: 0.7271237788253196\n",
            "[Train] batch: 73 | loss: 0.7311777227678081\n",
            "[Train] batch: 74 | loss: 1.096539226203607\n",
            "[Train] batch: 75 | loss: 0.8443338579046069\n",
            "[Train] batch: 76 | loss: 0.5866624915360062\n",
            "[Train] batch: 77 | loss: 0.6752570715113048\n",
            "[Train] batch: 78 | loss: 0.5600804577054609\n",
            "[Train] batch: 79 | loss: 0.6451857460505998\n",
            "[Train] batch: 80 | loss: 0.30209182694250736\n",
            "[Train] batch: 81 | loss: 0.7287727963649894\n",
            "[Train] batch: 82 | loss: 0.3639739473508434\n",
            "[Train] batch: 83 | loss: 0.478505640723555\n",
            "[Train] batch: 84 | loss: 0.5187944747699599\n",
            "[Train] batch: 85 | loss: 0.5941033778981012\n",
            "[Train] batch: 86 | loss: 0.4871209607760224\n",
            "[Train] batch: 87 | loss: 0.6208235793459801\n",
            "[Train] batch: 88 | loss: 0.701099143909819\n",
            "[Train] batch: 89 | loss: 0.6586722591973311\n",
            "[Train] batch: 90 | loss: 0.7501386619752755\n",
            "[Train] batch: 91 | loss: 0.5130101628266682\n",
            "[Train] batch: 92 | loss: 0.5862897195240533\n",
            "[Train] batch: 93 | loss: 0.687753912249585\n",
            "[Train] batch: 94 | loss: 0.7656876414269317\n",
            "[Train] batch: 95 | loss: 0.7336854514379576\n",
            "[Train] batch: 96 | loss: 0.6718188885928897\n",
            "[Train] batch: 97 | loss: 0.5250118090997964\n",
            "[Train] batch: 98 | loss: 0.744304619972902\n",
            "[Train] batch: 99 | loss: 0.4527704377295258\n",
            "[Train] batch: 100 | loss: 0.646928578884558\n",
            "[Train] batch: 101 | loss: 0.3782435126791415\n",
            "[Train] batch: 102 | loss: 0.48841590773998966\n",
            "[Train] batch: 103 | loss: 0.5488311391844589\n",
            "[Train] batch: 104 | loss: 0.45936382299906603\n",
            "[Train] batch: 105 | loss: 0.6190295825400699\n",
            "[Train] batch: 106 | loss: 0.5477581864706892\n",
            "[Train] batch: 107 | loss: 0.875805751766799\n",
            "[Train] batch: 108 | loss: 0.8467027233622915\n",
            "[Train] batch: 109 | loss: 0.6673080815350366\n",
            "[Train] batch: 110 | loss: 0.5711142351818419\n",
            "[Train] batch: 111 | loss: 0.5130336976933676\n",
            "[Train] batch: 112 | loss: 0.5715740972104241\n",
            "[Train] batch: 113 | loss: 0.5129645734628979\n",
            "[Train] batch: 114 | loss: 0.5101905256347558\n",
            "[Train] batch: 115 | loss: 0.47665329488824304\n",
            "[Train] batch: 116 | loss: 0.8013434090522837\n",
            "[Train] batch: 117 | loss: 0.44509801820557743\n",
            "[Train] batch: 118 | loss: 0.6749920247224592\n",
            "[Train] batch: 119 | loss: 0.78562125407007\n",
            "[Train] batch: 120 | loss: 0.8685625292610072\n",
            "[Train] batch: 121 | loss: 0.5093379970874354\n",
            "[Train] batch: 122 | loss: 0.72276411609401\n",
            "[Train] batch: 123 | loss: 0.5737076764563664\n",
            "[Train] batch: 124 | loss: 0.9662839582968543\n",
            "[Train] batch: 125 | loss: 0.7404158887299139\n",
            "[Train] batch: 126 | loss: 0.6925728833093341\n",
            "[Train] batch: 127 | loss: 0.4731288705163932\n",
            "[Train] batch: 128 | loss: 0.7315807729306155\n",
            "[Train] batch: 129 | loss: 0.26327382968143603\n",
            "[Train] batch: 130 | loss: 0.2615703176663093\n",
            "[Train] batch: 131 | loss: 0.2698978604382099\n",
            "[Train] batch: 132 | loss: 0.6210961860041342\n",
            "[Train] batch: 133 | loss: 0.4840884791642285\n",
            "[Train] batch: 134 | loss: 0.6062980994042741\n",
            "[Train] batch: 135 | loss: 0.6195451072329211\n",
            "[Train] batch: 136 | loss: 0.711344079451424\n",
            "[Train] batch: 137 | loss: 0.6335481159401998\n",
            "[Train] batch: 138 | loss: 0.9263040440977289\n",
            "[Train] batch: 139 | loss: 0.9170810064069908\n",
            "[Train] batch: 140 | loss: 0.7304698683050719\n",
            "[Train] batch: 141 | loss: 0.8159722322726554\n",
            "[Train] batch: 142 | loss: 0.720380104449844\n",
            "[Train] batch: 143 | loss: 0.5067614999420235\n",
            "[Train] batch: 144 | loss: 0.617864075723706\n",
            "[Train] batch: 145 | loss: 0.7843516698545513\n",
            "[Train] batch: 146 | loss: 0.4052417133621439\n",
            "[Train] batch: 147 | loss: 0.5810389217553488\n",
            "[Train] batch: 148 | loss: 0.4326124021555161\n",
            "[Train] batch: 149 | loss: 0.3980531965174392\n",
            "[Train] batch: 150 | loss: 0.6707248850298868\n",
            "[Train] batch: 151 | loss: 0.6863576560016602\n",
            "[Train] batch: 152 | loss: 0.5329789606744529\n",
            "[Train] batch: 153 | loss: 0.4633412025522646\n",
            "[Train] batch: 154 | loss: 0.5738191087185428\n",
            "[Train] batch: 155 | loss: 0.6487123361072343\n",
            "[Train] batch: 156 | loss: 0.7613692973233136\n",
            "[Train] batch: 157 | loss: 0.38797191790084046\n",
            "[Train] batch: 158 | loss: 0.6980467071566243\n",
            "[Train] batch: 159 | loss: 0.24952643500744426\n",
            "[Train] batch: 160 | loss: 0.6207792910943596\n",
            "[Train] batch: 161 | loss: 0.6811471004574031\n",
            "[Train] batch: 162 | loss: 0.6628288758152181\n",
            "[Train] batch: 163 | loss: 0.5305971445493\n",
            "[Train] batch: 164 | loss: 0.49379740954288553\n",
            "[Train] batch: 165 | loss: 0.8520220713199448\n",
            "[Train] batch: 166 | loss: 0.7747526523279394\n",
            "[Train] batch: 167 | loss: 0.4262506031604138\n",
            "[Train] batch: 168 | loss: 0.6460018305293136\n",
            "[Train] batch: 169 | loss: 0.4397681561959037\n",
            "[Train] batch: 170 | loss: 0.7913481253775159\n",
            "[Train] batch: 171 | loss: 0.4410618282092732\n",
            "[Train] batch: 172 | loss: 0.7203139537687194\n",
            "[Train] batch: 173 | loss: 0.8121476105525919\n",
            "[Train] batch: 174 | loss: 0.49452213319487365\n",
            "[Train] batch: 175 | loss: 0.5886757884744469\n",
            "[Train] batch: 176 | loss: 0.5955991956890333\n",
            "[Train] batch: 177 | loss: 0.2774822975967331\n",
            "[Train] batch: 178 | loss: 0.5713555377250655\n",
            "[Train] batch: 179 | loss: 0.48246951491048506\n",
            "[Train] batch: 180 | loss: 0.2230125775010645\n",
            "[Train] batch: 181 | loss: 0.8117715170425281\n",
            "[Train] batch: 182 | loss: 0.5701189875147351\n",
            "[Train] batch: 183 | loss: 0.5482748872732192\n",
            "[Train] batch: 184 | loss: 0.6235701115381026\n",
            "[Train] batch: 185 | loss: 0.9136689569122407\n",
            "[Train total] loss: 0.04602868298021305 | acc: 81.04247104247104\n",
            "[Val] batch: 1 | loss: 0.770190085135045\n",
            "[Val] batch: 2 | loss: 0.5966458823599842\n",
            "[Val] batch: 3 | loss: 0.6434719960841171\n",
            "[Val] batch: 4 | loss: 0.3286709666541266\n",
            "[Val] batch: 5 | loss: 0.45970203479313543\n",
            "[Val] batch: 6 | loss: 0.77706915214028\n",
            "[Val] batch: 7 | loss: 0.6945703253113856\n",
            "[Val] batch: 8 | loss: 0.585109435505389\n",
            "[Val] batch: 9 | loss: 0.5731514160904193\n",
            "[Val] batch: 10 | loss: 0.7352718604084811\n",
            "[Val] batch: 11 | loss: 0.8243897012157431\n",
            "[Val] batch: 12 | loss: 0.6605416503301902\n",
            "[Val] batch: 13 | loss: 0.9079940235179865\n",
            "[Val] batch: 14 | loss: 0.22006889856382297\n",
            "[Val] batch: 15 | loss: 0.31251074904766585\n",
            "[Val] batch: 16 | loss: 0.4689653475240811\n",
            "[Val] batch: 17 | loss: 0.23692357795458943\n",
            "[Val] batch: 18 | loss: 0.33536371147405114\n",
            "[Val] batch: 19 | loss: 0.35451274403754535\n",
            "[Val] batch: 20 | loss: 0.7655584065742549\n",
            "[Val] batch: 21 | loss: 0.9408299372628107\n",
            "[Val] batch: 22 | loss: 0.2990762878657843\n",
            "[Val] batch: 23 | loss: 0.7282841734451442\n",
            "[Val] batch: 24 | loss: 0.7342477694377388\n",
            "[Val] batch: 25 | loss: 0.395518293273641\n",
            "[Val] batch: 26 | loss: 0.44714505786572206\n",
            "[Val] batch: 27 | loss: 0.4128847137393693\n",
            "[Val] batch: 28 | loss: 0.3189703244408214\n",
            "[Val] batch: 29 | loss: 0.6952965512360689\n",
            "[Val] batch: 30 | loss: 0.6186639714884267\n",
            "[Val] batch: 31 | loss: 0.8510283351541486\n",
            "[Val] batch: 32 | loss: 1.0959454365485441\n",
            "[Val] batch: 33 | loss: 0.5464964107486795\n",
            "[Val] batch: 34 | loss: 0.5587351240025881\n",
            "[Val] batch: 35 | loss: 0.5154722485891774\n",
            "[Val] batch: 36 | loss: 0.3881262620221828\n",
            "[Val] batch: 37 | loss: 0.5346123069480025\n",
            "[Val] batch: 38 | loss: 0.4225394352630822\n",
            "[Val] batch: 39 | loss: 0.3752493057629886\n",
            "[Val] batch: 40 | loss: 0.49523175707223605\n",
            "[Val total] loss: 0.040765830030431445 | acc: 83.42342342342343\n",
            "[Val] batch: 1 | loss: 0.32559294026641744\n",
            "[Val] batch: 2 | loss: 0.7687998003845501\n",
            "[Val] batch: 3 | loss: 0.7490657667188719\n",
            "[Val] batch: 4 | loss: 0.7707540141169866\n",
            "[Val] batch: 5 | loss: 0.8893803485488323\n",
            "[Val] batch: 6 | loss: 0.6150573616389059\n",
            "[Val] batch: 7 | loss: 0.2666983253558466\n",
            "[Val] batch: 8 | loss: 0.4432051066613466\n",
            "[Val] batch: 9 | loss: 0.35840933593002655\n",
            "[Val] batch: 10 | loss: 0.785603258082756\n",
            "[Val] batch: 11 | loss: 0.7025454591812742\n",
            "[Val] batch: 12 | loss: 0.5666657450437385\n",
            "[Val] batch: 13 | loss: 0.6197665971586835\n",
            "[Val] batch: 14 | loss: 0.6364090948710286\n",
            "[Val] batch: 15 | loss: 0.4808292600901107\n",
            "[Val] batch: 16 | loss: 0.43995169565712827\n",
            "[Val] batch: 17 | loss: 0.44463516466937836\n",
            "[Val] batch: 18 | loss: 0.6925158162456544\n",
            "[Val] batch: 19 | loss: 0.4260836943589718\n",
            "[Val] batch: 20 | loss: 0.6056138378304102\n",
            "[Val] batch: 21 | loss: 0.5297397018960902\n",
            "[Val] batch: 22 | loss: 0.9540126858871328\n",
            "[Val] batch: 23 | loss: 0.6162406940276942\n",
            "[Val] batch: 24 | loss: 0.6249864017184652\n",
            "[Val] batch: 25 | loss: 0.15485218156592237\n",
            "[Val] batch: 26 | loss: 0.6781307334839262\n",
            "[Val] batch: 27 | loss: 0.6931182755075724\n",
            "[Val] batch: 28 | loss: 0.5528618888062711\n",
            "[Val] batch: 29 | loss: 0.46197298568331935\n",
            "[Val] batch: 30 | loss: 0.31922867824452644\n",
            "[Val] batch: 31 | loss: 0.5687941684120126\n",
            "[Val] batch: 32 | loss: 0.7023639342178204\n",
            "[Val] batch: 33 | loss: 0.5360229419569144\n",
            "[Val] batch: 34 | loss: 0.6692230345138892\n",
            "[Val] batch: 35 | loss: 0.574744657415691\n",
            "[Val] batch: 36 | loss: 0.853211869065311\n",
            "[Val] batch: 37 | loss: 0.49564123151817696\n",
            "[Val] batch: 38 | loss: 0.6776532246454652\n",
            "[Val] batch: 39 | loss: 0.6534081341815219\n",
            "[Val] batch: 40 | loss: 0.26690823066089453\n",
            "[Val total] loss: 0.04174900590309828 | acc: 81.8018018018018\n",
            ">>>>>> epoch 3 starts\n",
            "[Train] batch: 1 | loss: 0.2528816845621379\n",
            "[Train] batch: 2 | loss: 0.7672863884272426\n",
            "[Train] batch: 3 | loss: 0.708586235868452\n",
            "[Train] batch: 4 | loss: 0.5506373134250794\n",
            "[Train] batch: 5 | loss: 0.3110410710815879\n",
            "[Train] batch: 6 | loss: 0.2061748144143396\n",
            "[Train] batch: 7 | loss: 0.5003817532002192\n",
            "[Train] batch: 8 | loss: 0.43167703250690154\n",
            "[Train] batch: 9 | loss: 0.8887016657940263\n",
            "[Train] batch: 10 | loss: 0.7824742239752039\n",
            "[Train] batch: 11 | loss: 0.5007352456949062\n",
            "[Train] batch: 12 | loss: 0.573108602099767\n",
            "[Train] batch: 13 | loss: 0.6162908856613345\n",
            "[Train] batch: 14 | loss: 0.49868401187559847\n",
            "[Train] batch: 15 | loss: 0.5823900686358804\n",
            "[Train] batch: 16 | loss: 0.39155787230291267\n",
            "[Train] batch: 17 | loss: 0.7097238317318174\n",
            "[Train] batch: 18 | loss: 0.5032185793382317\n",
            "[Train] batch: 19 | loss: 0.3928077850511881\n",
            "[Train] batch: 20 | loss: 0.27707214777636896\n",
            "[Train] batch: 21 | loss: 0.6720101471840303\n",
            "[Train] batch: 22 | loss: 0.6951246588508273\n",
            "[Train] batch: 23 | loss: 0.4836874443588238\n",
            "[Train] batch: 24 | loss: 0.4802333210879297\n",
            "[Train] batch: 25 | loss: 0.5684412950771208\n",
            "[Train] batch: 26 | loss: 0.8097597836870282\n",
            "[Train] batch: 27 | loss: 0.74537085563008\n",
            "[Train] batch: 28 | loss: 0.31914414013782516\n",
            "[Train] batch: 29 | loss: 0.676780301284546\n",
            "[Train] batch: 30 | loss: 0.6220677781259802\n",
            "[Train] batch: 31 | loss: 0.5551769889149633\n",
            "[Train] batch: 32 | loss: 0.8765057091546964\n",
            "[Train] batch: 33 | loss: 0.7294895732361356\n",
            "[Train] batch: 34 | loss: 0.8279385505921611\n",
            "[Train] batch: 35 | loss: 0.45781007046626454\n",
            "[Train] batch: 36 | loss: 0.35698167020786853\n",
            "[Train] batch: 37 | loss: 0.620532670883145\n",
            "[Train] batch: 38 | loss: 0.46321970238186777\n",
            "[Train] batch: 39 | loss: 0.5404569990363963\n",
            "[Train] batch: 40 | loss: 0.3197179293435391\n",
            "[Train] batch: 41 | loss: 0.6132947805575476\n",
            "[Train] batch: 42 | loss: 0.7599914603584467\n",
            "[Train] batch: 43 | loss: 0.7035840608835927\n",
            "[Train] batch: 44 | loss: 0.4839929837054041\n",
            "[Train] batch: 45 | loss: 0.4721035390094653\n",
            "[Train] batch: 46 | loss: 0.5911104272699618\n",
            "[Train] batch: 47 | loss: 0.65722621313998\n",
            "[Train] batch: 48 | loss: 0.4803005262361009\n",
            "[Train] batch: 49 | loss: 0.2242326774280011\n",
            "[Train] batch: 50 | loss: 0.3102333261227214\n",
            "[Train] batch: 51 | loss: 0.3153420733997489\n",
            "[Train] batch: 52 | loss: 0.7282699645178043\n",
            "[Train] batch: 53 | loss: 0.5393165247504552\n",
            "[Train] batch: 54 | loss: 0.7063252546569567\n",
            "[Train] batch: 55 | loss: 0.372235797690661\n",
            "[Train] batch: 56 | loss: 0.4810949128215964\n",
            "[Train] batch: 57 | loss: 0.5182994974564559\n",
            "[Train] batch: 58 | loss: 0.3865204064345052\n",
            "[Train] batch: 59 | loss: 0.40179942644818023\n",
            "[Train] batch: 60 | loss: 0.5174443283056874\n",
            "[Train] batch: 61 | loss: 0.7467563791065868\n",
            "[Train] batch: 62 | loss: 0.7090029455065595\n",
            "[Train] batch: 63 | loss: 0.3753348548600789\n",
            "[Train] batch: 64 | loss: 0.5222478718219731\n",
            "[Train] batch: 65 | loss: 0.6032612441221649\n",
            "[Train] batch: 66 | loss: 0.5692360009264165\n",
            "[Train] batch: 67 | loss: 0.7100553242907013\n",
            "[Train] batch: 68 | loss: 0.49339560178053715\n",
            "[Train] batch: 69 | loss: 0.48182405385515387\n",
            "[Train] batch: 70 | loss: 0.8920092848784489\n",
            "[Train] batch: 71 | loss: 0.5760554479407732\n",
            "[Train] batch: 72 | loss: 0.47118923219392206\n",
            "[Train] batch: 73 | loss: 0.5947922534557082\n",
            "[Train] batch: 74 | loss: 0.5200545732684511\n",
            "[Train] batch: 75 | loss: 0.7847697806301474\n",
            "[Train] batch: 76 | loss: 0.5311341861931248\n",
            "[Train] batch: 77 | loss: 0.6547868232946948\n",
            "[Train] batch: 78 | loss: 0.38126711968363275\n",
            "[Train] batch: 79 | loss: 0.43932991341525135\n",
            "[Train] batch: 80 | loss: 0.45933478275130124\n",
            "[Train] batch: 81 | loss: 0.5658030955512761\n",
            "[Train] batch: 82 | loss: 0.9028646463077779\n",
            "[Train] batch: 83 | loss: 0.5291295024336183\n",
            "[Train] batch: 84 | loss: 0.5169953845378963\n",
            "[Train] batch: 85 | loss: 0.6887221186923204\n",
            "[Train] batch: 86 | loss: 0.2988314165731338\n",
            "[Train] batch: 87 | loss: 1.0158444062387069\n",
            "[Train] batch: 88 | loss: 0.37606900150319295\n",
            "[Train] batch: 89 | loss: 0.5700922824981404\n",
            "[Train] batch: 90 | loss: 0.6893010600842142\n",
            "[Train] batch: 91 | loss: 0.38523233029479736\n",
            "[Train] batch: 92 | loss: 0.5524829738479243\n",
            "[Train] batch: 93 | loss: 0.6269197099961911\n",
            "[Train] batch: 94 | loss: 0.6541675323090265\n",
            "[Train] batch: 95 | loss: 0.5225356706451292\n",
            "[Train] batch: 96 | loss: 0.4399721550477757\n",
            "[Train] batch: 97 | loss: 0.628703611963776\n",
            "[Train] batch: 98 | loss: 0.4676850424210688\n",
            "[Train] batch: 99 | loss: 0.5905142550267125\n",
            "[Train] batch: 100 | loss: 0.3681308148337329\n",
            "[Train] batch: 101 | loss: 0.38033527578029885\n",
            "[Train] batch: 102 | loss: 0.2416977089863186\n",
            "[Train] batch: 103 | loss: 0.6852524505284191\n",
            "[Train] batch: 104 | loss: 0.7135336632221668\n",
            "[Train] batch: 105 | loss: 0.40966224976491455\n",
            "[Train] batch: 106 | loss: 0.44591932390165\n",
            "[Train] batch: 107 | loss: 0.4636496495360121\n",
            "[Train] batch: 108 | loss: 0.658192416600589\n",
            "[Train] batch: 109 | loss: 0.3379016559430295\n",
            "[Train] batch: 110 | loss: 0.441778234513031\n",
            "[Train] batch: 111 | loss: 0.678699640796102\n",
            "[Train] batch: 112 | loss: 0.6590457333690695\n",
            "[Train] batch: 113 | loss: 0.3610166026372812\n",
            "[Train] batch: 114 | loss: 0.4532363674790423\n",
            "[Train] batch: 115 | loss: 0.7576923394312882\n",
            "[Train] batch: 116 | loss: 0.34073499577494015\n",
            "[Train] batch: 117 | loss: 0.46038613307983944\n",
            "[Train] batch: 118 | loss: 0.40621917482441194\n",
            "[Train] batch: 119 | loss: 0.569381704616766\n",
            "[Train] batch: 120 | loss: 0.5849548112762876\n",
            "[Train] batch: 121 | loss: 0.5337731778100793\n",
            "[Train] batch: 122 | loss: 0.7096486151564108\n",
            "[Train] batch: 123 | loss: 0.5102024249020081\n",
            "[Train] batch: 124 | loss: 0.946831692978974\n",
            "[Train] batch: 125 | loss: 0.6665924502084183\n",
            "[Train] batch: 126 | loss: 0.45138757937725427\n",
            "[Train] batch: 127 | loss: 0.5480485476106554\n",
            "[Train] batch: 128 | loss: 0.49802688798748207\n",
            "[Train] batch: 129 | loss: 0.6459842859819446\n",
            "[Train] batch: 130 | loss: 0.3996189636861022\n",
            "[Train] batch: 131 | loss: 0.4636223294873116\n",
            "[Train] batch: 132 | loss: 0.2903879505107184\n",
            "[Train] batch: 133 | loss: 0.5692602379785894\n",
            "[Train] batch: 134 | loss: 0.6282550273127907\n",
            "[Train] batch: 135 | loss: 0.4231534247751673\n",
            "[Train] batch: 136 | loss: 0.3017132875624201\n",
            "[Train] batch: 137 | loss: 0.25559756348523505\n",
            "[Train] batch: 138 | loss: 0.3281800694445671\n",
            "[Train] batch: 139 | loss: 0.60558819402858\n",
            "[Train] batch: 140 | loss: 0.32163348838836797\n",
            "[Train] batch: 141 | loss: 0.46003595540902237\n",
            "[Train] batch: 142 | loss: 0.5191088464349681\n",
            "[Train] batch: 143 | loss: 0.35546145116341704\n",
            "[Train] batch: 144 | loss: 0.551273425841114\n",
            "[Train] batch: 145 | loss: 0.506081259023804\n",
            "[Train] batch: 146 | loss: 0.3314480711677296\n",
            "[Train] batch: 147 | loss: 0.3684241034742151\n",
            "[Train] batch: 148 | loss: 0.410426719011259\n",
            "[Train] batch: 149 | loss: 0.7691001024267786\n",
            "[Train] batch: 150 | loss: 0.5274973151698477\n",
            "[Train] batch: 151 | loss: 0.685268261918747\n",
            "[Train] batch: 152 | loss: 0.5419242206685981\n",
            "[Train] batch: 153 | loss: 0.5022702881395961\n",
            "[Train] batch: 154 | loss: 0.523915564161256\n",
            "[Train] batch: 155 | loss: 0.4613775082688128\n",
            "[Train] batch: 156 | loss: 0.38914636820894316\n",
            "[Train] batch: 157 | loss: 0.2076429619158833\n",
            "[Train] batch: 158 | loss: 0.5866852155701354\n",
            "[Train] batch: 159 | loss: 0.3595208916596187\n",
            "[Train] batch: 160 | loss: 0.6478270584155785\n",
            "[Train] batch: 161 | loss: 0.46634846267452756\n",
            "[Train] batch: 162 | loss: 0.20736492069214024\n",
            "[Train] batch: 163 | loss: 0.4752123278573503\n",
            "[Train] batch: 164 | loss: 0.5737982726942897\n",
            "[Train] batch: 165 | loss: 0.3386062872189063\n",
            "[Train] batch: 166 | loss: 0.4338351540930026\n",
            "[Train] batch: 167 | loss: 0.182877250944405\n",
            "[Train] batch: 168 | loss: 0.4182101114598115\n",
            "[Train] batch: 169 | loss: 0.5342087822769956\n",
            "[Train] batch: 170 | loss: 0.4451855304839105\n",
            "[Train] batch: 171 | loss: 0.34366706514097667\n",
            "[Train] batch: 172 | loss: 0.7725792703824977\n",
            "[Train] batch: 173 | loss: 0.45028778966644384\n",
            "[Train] batch: 174 | loss: 0.5510119233639788\n",
            "[Train] batch: 175 | loss: 0.47050162384736016\n",
            "[Train] batch: 176 | loss: 0.4321238491602678\n",
            "[Train] batch: 177 | loss: 0.35770603423220054\n",
            "[Train] batch: 178 | loss: 0.44301925925657215\n",
            "[Train] batch: 179 | loss: 0.5775125894962917\n",
            "[Train] batch: 180 | loss: 0.301269616797028\n",
            "[Train] batch: 181 | loss: 0.11980479673334023\n",
            "[Train] batch: 182 | loss: 0.3816880371258358\n",
            "[Train] batch: 183 | loss: 0.29466459758508445\n",
            "[Train] batch: 184 | loss: 0.42185719285470125\n",
            "[Train] batch: 185 | loss: 0.711356234163259\n",
            "[Train total] loss: 0.0370619160494304 | acc: 86.21621621621621\n",
            "[Val] batch: 1 | loss: 0.6790901859783366\n",
            "[Val] batch: 2 | loss: 0.5134099558647862\n",
            "[Val] batch: 3 | loss: 0.5656484687963089\n",
            "[Val] batch: 4 | loss: 0.2957567884875651\n",
            "[Val] batch: 5 | loss: 0.4078418430239391\n",
            "[Val] batch: 6 | loss: 0.7309685697492301\n",
            "[Val] batch: 7 | loss: 0.6551712695578866\n",
            "[Val] batch: 8 | loss: 0.5431635241673353\n",
            "[Val] batch: 9 | loss: 0.5306197335477345\n",
            "[Val] batch: 10 | loss: 0.665062809854196\n",
            "[Val] batch: 11 | loss: 0.7248869600063811\n",
            "[Val] batch: 12 | loss: 0.6035116324088887\n",
            "[Val] batch: 13 | loss: 0.8453514775859157\n",
            "[Val] batch: 14 | loss: 0.18959191905590028\n",
            "[Val] batch: 15 | loss: 0.2624974820130369\n",
            "[Val] batch: 16 | loss: 0.3409727112215149\n",
            "[Val] batch: 17 | loss: 0.21675740226765935\n",
            "[Val] batch: 18 | loss: 0.26847256014660675\n",
            "[Val] batch: 19 | loss: 0.28860016581575504\n",
            "[Val] batch: 20 | loss: 0.6836038710058228\n",
            "[Val] batch: 21 | loss: 0.8378576298059434\n",
            "[Val] batch: 22 | loss: 0.23396772948020694\n",
            "[Val] batch: 23 | loss: 0.6432286879556086\n",
            "[Val] batch: 24 | loss: 0.6461542560428221\n",
            "[Val] batch: 25 | loss: 0.33687936344054503\n",
            "[Val] batch: 26 | loss: 0.3954464734901714\n",
            "[Val] batch: 27 | loss: 0.3879693161627616\n",
            "[Val] batch: 28 | loss: 0.31943059513209515\n",
            "[Val] batch: 29 | loss: 0.5683643817353516\n",
            "[Val] batch: 30 | loss: 0.5422789127793114\n",
            "[Val] batch: 31 | loss: 0.7851425042316732\n",
            "[Val] batch: 32 | loss: 1.0373237780806752\n",
            "[Val] batch: 33 | loss: 0.4647939928288852\n",
            "[Val] batch: 34 | loss: 0.4910768826218164\n",
            "[Val] batch: 35 | loss: 0.44220354647779564\n",
            "[Val] batch: 36 | loss: 0.34243836523340965\n",
            "[Val] batch: 37 | loss: 0.44111702613653386\n",
            "[Val] batch: 38 | loss: 0.37292087212077624\n",
            "[Val] batch: 39 | loss: 0.34164777259180423\n",
            "[Val] batch: 40 | loss: 0.4383498775203465\n",
            "[Val total] loss: 0.0361794077376997 | acc: 85.04504504504504\n",
            "[Val] batch: 1 | loss: 0.27979016618657016\n",
            "[Val] batch: 2 | loss: 0.7109084232496274\n",
            "[Val] batch: 3 | loss: 0.6985712076984701\n",
            "[Val] batch: 4 | loss: 0.6916719044410115\n",
            "[Val] batch: 5 | loss: 0.7798020623973969\n",
            "[Val] batch: 6 | loss: 0.5210629345823014\n",
            "[Val] batch: 7 | loss: 0.24017416944521847\n",
            "[Val] batch: 8 | loss: 0.34408107584275344\n",
            "[Val] batch: 9 | loss: 0.29056146966823043\n",
            "[Val] batch: 10 | loss: 0.6728583214646113\n",
            "[Val] batch: 11 | loss: 0.6360133106934328\n",
            "[Val] batch: 12 | loss: 0.4907561343232609\n",
            "[Val] batch: 13 | loss: 0.5342035596769661\n",
            "[Val] batch: 14 | loss: 0.6160638977903403\n",
            "[Val] batch: 15 | loss: 0.41986125070245933\n",
            "[Val] batch: 16 | loss: 0.3777280834909522\n",
            "[Val] batch: 17 | loss: 0.36435139056780647\n",
            "[Val] batch: 18 | loss: 0.5820125637551009\n",
            "[Val] batch: 19 | loss: 0.40619700091983607\n",
            "[Val] batch: 20 | loss: 0.5188616263242426\n",
            "[Val] batch: 21 | loss: 0.41112577288756\n",
            "[Val] batch: 22 | loss: 0.8708357877293836\n",
            "[Val] batch: 23 | loss: 0.5369079378171396\n",
            "[Val] batch: 24 | loss: 0.6224708731678189\n",
            "[Val] batch: 25 | loss: 0.12276301835664946\n",
            "[Val] batch: 26 | loss: 0.6514987842335206\n",
            "[Val] batch: 27 | loss: 0.6335987875923663\n",
            "[Val] batch: 28 | loss: 0.41034066286922216\n",
            "[Val] batch: 29 | loss: 0.38810604721241443\n",
            "[Val] batch: 30 | loss: 0.2754534639630943\n",
            "[Val] batch: 31 | loss: 0.5456098336025976\n",
            "[Val] batch: 32 | loss: 0.690310040226546\n",
            "[Val] batch: 33 | loss: 0.49433185598616725\n",
            "[Val] batch: 34 | loss: 0.6254778408830681\n",
            "[Val] batch: 35 | loss: 0.5250061390681645\n",
            "[Val] batch: 36 | loss: 0.75381937733972\n",
            "[Val] batch: 37 | loss: 0.39859492387845036\n",
            "[Val] batch: 38 | loss: 0.5556625227295522\n",
            "[Val] batch: 39 | loss: 0.5528217543950237\n",
            "[Val] batch: 40 | loss: 0.23376759023169724\n",
            "[Val total] loss: 0.03689015057187522 | acc: 84.14414414414415\n",
            ">>>>>> epoch 4 starts\n",
            "[Train] batch: 1 | loss: 0.3564971039164115\n",
            "[Train] batch: 2 | loss: 0.3940079957934256\n",
            "[Train] batch: 3 | loss: 0.6497869261879351\n",
            "[Train] batch: 4 | loss: 0.19439394848300948\n",
            "[Train] batch: 5 | loss: 0.3697306388735041\n",
            "[Train] batch: 6 | loss: 0.2858053747659544\n",
            "[Train] batch: 7 | loss: 0.5170791537055467\n",
            "[Train] batch: 8 | loss: 0.22731875519903658\n",
            "[Train] batch: 9 | loss: 0.48444995146459885\n",
            "[Train] batch: 10 | loss: 0.7335152280143552\n",
            "[Train] batch: 11 | loss: 0.5359110385006395\n",
            "[Train] batch: 12 | loss: 0.2274958600628346\n",
            "[Train] batch: 13 | loss: 0.42293339505872823\n",
            "[Train] batch: 14 | loss: 0.4109819447780642\n",
            "[Train] batch: 15 | loss: 0.6294971602094066\n",
            "[Train] batch: 16 | loss: 0.4837282093985139\n",
            "[Train] batch: 17 | loss: 0.3315290060506978\n",
            "[Train] batch: 18 | loss: 0.2893715335974502\n",
            "[Train] batch: 19 | loss: 0.29211502307070514\n",
            "[Train] batch: 20 | loss: 0.26287568106484305\n",
            "[Train] batch: 21 | loss: 0.37487810625592877\n",
            "[Train] batch: 22 | loss: 0.3223037299229886\n",
            "[Train] batch: 23 | loss: 0.4083219034215994\n",
            "[Train] batch: 24 | loss: 0.18747749855602716\n",
            "[Train] batch: 25 | loss: 0.3486351957399088\n",
            "[Train] batch: 26 | loss: 0.39112051869386927\n",
            "[Train] batch: 27 | loss: 0.977093859473248\n",
            "[Train] batch: 28 | loss: 0.32728069169777635\n",
            "[Train] batch: 29 | loss: 0.9264469903108639\n",
            "[Train] batch: 30 | loss: 0.343867037781998\n",
            "[Train] batch: 31 | loss: 0.3405281907199881\n",
            "[Train] batch: 32 | loss: 0.4494929903827571\n",
            "[Train] batch: 33 | loss: 0.4705783714771127\n",
            "[Train] batch: 34 | loss: 0.5434948243706639\n",
            "[Train] batch: 35 | loss: 0.3241523581328799\n",
            "[Train] batch: 36 | loss: 0.3575446299420582\n",
            "[Train] batch: 37 | loss: 0.35254915612584153\n",
            "[Train] batch: 38 | loss: 0.44694392028770136\n",
            "[Train] batch: 39 | loss: 0.5426948499334889\n",
            "[Train] batch: 40 | loss: 0.5863620160272098\n",
            "[Train] batch: 41 | loss: 0.4337356758993541\n",
            "[Train] batch: 42 | loss: 0.2331235424132904\n",
            "[Train] batch: 43 | loss: 0.552080190684407\n",
            "[Train] batch: 44 | loss: 0.378521737274571\n",
            "[Train] batch: 45 | loss: 0.5336701792076678\n",
            "[Train] batch: 46 | loss: 0.12569364472052127\n",
            "[Train] batch: 47 | loss: 0.12117300345102348\n",
            "[Train] batch: 48 | loss: 0.33933594974622494\n",
            "[Train] batch: 49 | loss: 0.30499491700385983\n",
            "[Train] batch: 50 | loss: 0.7139138756500933\n",
            "[Train] batch: 51 | loss: 0.6793786278094027\n",
            "[Train] batch: 52 | loss: 0.7179469859253577\n",
            "[Train] batch: 53 | loss: 0.49559375705337294\n",
            "[Train] batch: 54 | loss: 0.42104410004046955\n",
            "[Train] batch: 55 | loss: 0.46985669757489423\n",
            "[Train] batch: 56 | loss: 0.6461509592447291\n",
            "[Train] batch: 57 | loss: 0.4921119971746066\n",
            "[Train] batch: 58 | loss: 0.5776053385396974\n",
            "[Train] batch: 59 | loss: 0.8253383681996079\n",
            "[Train] batch: 60 | loss: 0.7876829225899639\n",
            "[Train] batch: 61 | loss: 0.6657957034438338\n",
            "[Train] batch: 62 | loss: 0.507904691363154\n",
            "[Train] batch: 63 | loss: 0.3006395846228592\n",
            "[Train] batch: 64 | loss: 0.2238229751831724\n",
            "[Train] batch: 65 | loss: 0.17281540861057318\n",
            "[Train] batch: 66 | loss: 0.4755597464245144\n",
            "[Train] batch: 67 | loss: 0.42138050550346656\n",
            "[Train] batch: 68 | loss: 0.5138269090641414\n",
            "[Train] batch: 69 | loss: 0.3092309584188681\n",
            "[Train] batch: 70 | loss: 0.29036736438385347\n",
            "[Train] batch: 71 | loss: 0.5307771270457051\n",
            "[Train] batch: 72 | loss: 0.3947013575407703\n",
            "[Train] batch: 73 | loss: 0.578940406838946\n",
            "[Train] batch: 74 | loss: 0.4834192277539536\n",
            "[Train] batch: 75 | loss: 0.2689299457740175\n",
            "[Train] batch: 76 | loss: 0.19003161874225608\n",
            "[Train] batch: 77 | loss: 0.29909708972330457\n",
            "[Train] batch: 78 | loss: 0.37438049990616384\n",
            "[Train] batch: 79 | loss: 0.4064725174320082\n",
            "[Train] batch: 80 | loss: 0.5888215644295213\n",
            "[Train] batch: 81 | loss: 0.40121970039996685\n",
            "[Train] batch: 82 | loss: 0.7345484193742221\n",
            "[Train] batch: 83 | loss: 0.05740803548886736\n",
            "[Train] batch: 84 | loss: 0.4432159168610094\n",
            "[Train] batch: 85 | loss: 0.5660601198279253\n",
            "[Train] batch: 86 | loss: 0.6933845420369513\n",
            "[Train] batch: 87 | loss: 0.6452323009224411\n",
            "[Train] batch: 88 | loss: 0.33565487840073416\n",
            "[Train] batch: 89 | loss: 0.36821114568714924\n",
            "[Train] batch: 90 | loss: 0.4263630380371498\n",
            "[Train] batch: 91 | loss: 0.2162064459752088\n",
            "[Train] batch: 92 | loss: 0.28510932970897473\n",
            "[Train] batch: 93 | loss: 0.3648559373351251\n",
            "[Train] batch: 94 | loss: 0.4145329714732509\n",
            "[Train] batch: 95 | loss: 0.5084059379973773\n",
            "[Train] batch: 96 | loss: 0.6428266510317043\n",
            "[Train] batch: 97 | loss: 0.48067424254500135\n",
            "[Train] batch: 98 | loss: 0.3997019467068287\n",
            "[Train] batch: 99 | loss: 0.21252958164308536\n",
            "[Train] batch: 100 | loss: 0.43522485618125684\n",
            "[Train] batch: 101 | loss: 0.3818610823180051\n",
            "[Train] batch: 102 | loss: 0.3178284169471889\n",
            "[Train] batch: 103 | loss: 0.39291064926729347\n",
            "[Train] batch: 104 | loss: 0.523659374036914\n",
            "[Train] batch: 105 | loss: 0.334819929807809\n",
            "[Train] batch: 106 | loss: 0.6131955038164153\n",
            "[Train] batch: 107 | loss: 0.33624422278805255\n",
            "[Train] batch: 108 | loss: 0.3396932807842375\n",
            "[Train] batch: 109 | loss: 0.36016831156066337\n",
            "[Train] batch: 110 | loss: 0.37767362433068874\n",
            "[Train] batch: 111 | loss: 0.47229650597100303\n",
            "[Train] batch: 112 | loss: 0.358149594291945\n",
            "[Train] batch: 113 | loss: 0.5279098889922259\n",
            "[Train] batch: 114 | loss: 0.2848927470622878\n",
            "[Train] batch: 115 | loss: 0.3305414055227773\n",
            "[Train] batch: 116 | loss: 0.3211917672712558\n",
            "[Train] batch: 117 | loss: 0.6048712964325577\n",
            "[Train] batch: 118 | loss: 0.34551254188866887\n",
            "[Train] batch: 119 | loss: 0.4574227497344614\n",
            "[Train] batch: 120 | loss: 0.35278861340525225\n",
            "[Train] batch: 121 | loss: 0.42830668764611746\n",
            "[Train] batch: 122 | loss: 0.4542770949122348\n",
            "[Train] batch: 123 | loss: 0.29448547014242266\n",
            "[Train] batch: 124 | loss: 0.4414259177452398\n",
            "[Train] batch: 125 | loss: 0.4872300819146327\n",
            "[Train] batch: 126 | loss: 0.3274836102205056\n",
            "[Train] batch: 127 | loss: 0.471600781345164\n",
            "[Train] batch: 128 | loss: 0.4154139201765216\n",
            "[Train] batch: 129 | loss: 0.3928524666724842\n",
            "[Train] batch: 130 | loss: 0.43308658639496145\n",
            "[Train] batch: 131 | loss: 0.479763816998702\n",
            "[Train] batch: 132 | loss: 0.5296449857637029\n",
            "[Train] batch: 133 | loss: 0.38550210441191546\n",
            "[Train] batch: 134 | loss: 0.49702460216343025\n",
            "[Train] batch: 135 | loss: 0.360536985657157\n",
            "[Train] batch: 136 | loss: 0.27259159824786844\n",
            "[Train] batch: 137 | loss: 0.19080502525036774\n",
            "[Train] batch: 138 | loss: 0.5850808756151443\n",
            "[Train] batch: 139 | loss: 0.5371372310946783\n",
            "[Train] batch: 140 | loss: 0.3497569276978433\n",
            "[Train] batch: 141 | loss: 0.44480159082831155\n",
            "[Train] batch: 142 | loss: 0.5136908667053622\n",
            "[Train] batch: 143 | loss: 0.4106909198205379\n",
            "[Train] batch: 144 | loss: 0.5837403581205904\n",
            "[Train] batch: 145 | loss: 0.3683903228221871\n",
            "[Train] batch: 146 | loss: 0.2982408591411704\n",
            "[Train] batch: 147 | loss: 0.37984318431219677\n",
            "[Train] batch: 148 | loss: 0.4755009741228968\n",
            "[Train] batch: 149 | loss: 0.580718556950117\n",
            "[Train] batch: 150 | loss: 0.4250119190893276\n",
            "[Train] batch: 151 | loss: 0.3510812350799694\n",
            "[Train] batch: 152 | loss: 0.4456916735232364\n",
            "[Train] batch: 153 | loss: 0.30990725004106257\n",
            "[Train] batch: 154 | loss: 0.23468124792999043\n",
            "[Train] batch: 155 | loss: 0.2901855399092604\n",
            "[Train] batch: 156 | loss: 0.396229865298637\n",
            "[Train] batch: 157 | loss: 0.3113722577491815\n",
            "[Train] batch: 158 | loss: 0.5781465346312504\n",
            "[Train] batch: 159 | loss: 0.5594632478245913\n",
            "[Train] batch: 160 | loss: 0.3773770051261917\n",
            "[Train] batch: 161 | loss: 0.1968276154293233\n",
            "[Train] batch: 162 | loss: 0.3817195285682047\n",
            "[Train] batch: 163 | loss: 0.4005761306795126\n",
            "[Train] batch: 164 | loss: 0.4659837350841852\n",
            "[Train] batch: 165 | loss: 0.37393349922047153\n",
            "[Train] batch: 166 | loss: 0.2950323928101958\n",
            "[Train] batch: 167 | loss: 0.5640311915300072\n",
            "[Train] batch: 168 | loss: 0.3146670140522768\n",
            "[Train] batch: 169 | loss: 0.5090258951928438\n",
            "[Train] batch: 170 | loss: 0.5120965997499629\n",
            "[Train] batch: 171 | loss: 0.26972919931465505\n",
            "[Train] batch: 172 | loss: 0.5987254621706827\n",
            "[Train] batch: 173 | loss: 0.3265009489915864\n",
            "[Train] batch: 174 | loss: 0.3518953240108182\n",
            "[Train] batch: 175 | loss: 0.24046067890820758\n",
            "[Train] batch: 176 | loss: 0.5988496255421865\n",
            "[Train] batch: 177 | loss: 0.17003872209124152\n",
            "[Train] batch: 178 | loss: 0.2897109286620977\n",
            "[Train] batch: 179 | loss: 0.5055056117974155\n",
            "[Train] batch: 180 | loss: 0.5955700792216341\n",
            "[Train] batch: 181 | loss: 0.4206026149364175\n",
            "[Train] batch: 182 | loss: 0.18445657684202485\n",
            "[Train] batch: 183 | loss: 0.5080708810897505\n",
            "[Train] batch: 184 | loss: 0.48458589043814065\n",
            "[Train] batch: 185 | loss: 0.3816002281443322\n",
            "[Train total] loss: 0.030208735327141713 | acc: 89.61389961389962\n",
            "[Val] batch: 1 | loss: 0.56471998127333\n",
            "[Val] batch: 2 | loss: 0.44529066114059834\n",
            "[Val] batch: 3 | loss: 0.5026041035367969\n",
            "[Val] batch: 4 | loss: 0.26233283714217165\n",
            "[Val] batch: 5 | loss: 0.34853414742589794\n",
            "[Val] batch: 6 | loss: 0.6441738425582421\n",
            "[Val] batch: 7 | loss: 0.5864612892506206\n",
            "[Val] batch: 8 | loss: 0.4544541242662035\n",
            "[Val] batch: 9 | loss: 0.47586586388945457\n",
            "[Val] batch: 10 | loss: 0.6174555221650033\n",
            "[Val] batch: 11 | loss: 0.5334096524656174\n",
            "[Val] batch: 12 | loss: 0.5388278312026106\n",
            "[Val] batch: 13 | loss: 0.7241215774030234\n",
            "[Val] batch: 14 | loss: 0.17037614612326402\n",
            "[Val] batch: 15 | loss: 0.19553804424381896\n",
            "[Val] batch: 16 | loss: 0.35417134122702115\n",
            "[Val] batch: 17 | loss: 0.18492232858568375\n",
            "[Val] batch: 18 | loss: 0.19706033201892997\n",
            "[Val] batch: 19 | loss: 0.27443176508047307\n",
            "[Val] batch: 20 | loss: 0.5892812737219776\n",
            "[Val] batch: 21 | loss: 0.7493601256868047\n",
            "[Val] batch: 22 | loss: 0.17845971320350587\n",
            "[Val] batch: 23 | loss: 0.5991215713134715\n",
            "[Val] batch: 24 | loss: 0.5836236001695567\n",
            "[Val] batch: 25 | loss: 0.29562096975369584\n",
            "[Val] batch: 26 | loss: 0.3353680093985735\n",
            "[Val] batch: 27 | loss: 0.35994689250198547\n",
            "[Val] batch: 28 | loss: 0.2765579928254827\n",
            "[Val] batch: 29 | loss: 0.4924103448117002\n",
            "[Val] batch: 30 | loss: 0.4560252446031348\n",
            "[Val] batch: 31 | loss: 0.7173602726913239\n",
            "[Val] batch: 32 | loss: 0.9595343240215751\n",
            "[Val] batch: 33 | loss: 0.4512461463147996\n",
            "[Val] batch: 34 | loss: 0.44402366694333034\n",
            "[Val] batch: 35 | loss: 0.31996003126144856\n",
            "[Val] batch: 36 | loss: 0.2825510383432921\n",
            "[Val] batch: 37 | loss: 0.41266060396561605\n",
            "[Val] batch: 38 | loss: 0.394824449285907\n",
            "[Val] batch: 39 | loss: 0.28456957541783434\n",
            "[Val] batch: 40 | loss: 0.38393187390125744\n",
            "[Val total] loss: 0.031785926326369424 | acc: 87.38738738738739\n",
            "[Val] batch: 1 | loss: 0.2159042124931885\n",
            "[Val] batch: 2 | loss: 0.5968494897422804\n",
            "[Val] batch: 3 | loss: 0.5850717823184325\n",
            "[Val] batch: 4 | loss: 0.6058353637504207\n",
            "[Val] batch: 5 | loss: 0.7159502463997132\n",
            "[Val] batch: 6 | loss: 0.45899679628721873\n",
            "[Val] batch: 7 | loss: 0.2279873978531395\n",
            "[Val] batch: 8 | loss: 0.300485084749115\n",
            "[Val] batch: 9 | loss: 0.2550089531479435\n",
            "[Val] batch: 10 | loss: 0.6023980071017284\n",
            "[Val] batch: 11 | loss: 0.5472002296601063\n",
            "[Val] batch: 12 | loss: 0.473699433000097\n",
            "[Val] batch: 13 | loss: 0.4241326252821194\n",
            "[Val] batch: 14 | loss: 0.552919972971133\n",
            "[Val] batch: 15 | loss: 0.37695714781631323\n",
            "[Val] batch: 16 | loss: 0.3638111299138176\n",
            "[Val] batch: 17 | loss: 0.3296785917612751\n",
            "[Val] batch: 18 | loss: 0.4945526194347941\n",
            "[Val] batch: 19 | loss: 0.3204487919945049\n",
            "[Val] batch: 20 | loss: 0.49643946699467495\n",
            "[Val] batch: 21 | loss: 0.36265904869100235\n",
            "[Val] batch: 22 | loss: 0.7719820635185614\n",
            "[Val] batch: 23 | loss: 0.404050222558967\n",
            "[Val] batch: 24 | loss: 0.5892581919371244\n",
            "[Val] batch: 25 | loss: 0.09111764211756414\n",
            "[Val] batch: 26 | loss: 0.6190190823695499\n",
            "[Val] batch: 27 | loss: 0.5586189060104235\n",
            "[Val] batch: 28 | loss: 0.3358634326563492\n",
            "[Val] batch: 29 | loss: 0.33553435871587073\n",
            "[Val] batch: 30 | loss: 0.27602764277403474\n",
            "[Val] batch: 31 | loss: 0.4601866684022826\n",
            "[Val] batch: 32 | loss: 0.7002394759313272\n",
            "[Val] batch: 33 | loss: 0.42972377164273906\n",
            "[Val] batch: 34 | loss: 0.5310661765044445\n",
            "[Val] batch: 35 | loss: 0.4461401310236501\n",
            "[Val] batch: 36 | loss: 0.6962201618457785\n",
            "[Val] batch: 37 | loss: 0.37320846965673404\n",
            "[Val] batch: 38 | loss: 0.48513841414204734\n",
            "[Val] batch: 39 | loss: 0.45660583189849696\n",
            "[Val] batch: 40 | loss: 0.12925902511358975\n",
            "[Val total] loss: 0.03242566857690549 | acc: 86.12612612612612\n",
            ">>>>>> epoch 5 starts\n",
            "[Train] batch: 1 | loss: 0.2815302945261766\n",
            "[Train] batch: 2 | loss: 0.30903696725367585\n",
            "[Train] batch: 3 | loss: 0.24746132552679143\n",
            "[Train] batch: 4 | loss: 0.3442236842226199\n",
            "[Train] batch: 5 | loss: 0.5988799911168298\n",
            "[Train] batch: 6 | loss: 0.3397025490113296\n",
            "[Train] batch: 7 | loss: 0.3827836215538651\n",
            "[Train] batch: 8 | loss: 0.1959347335230555\n",
            "[Train] batch: 9 | loss: 0.3058605693756113\n",
            "[Train] batch: 10 | loss: 0.4285188181643604\n",
            "[Train] batch: 11 | loss: 0.22108099354623587\n",
            "[Train] batch: 12 | loss: 0.308656289929622\n",
            "[Train] batch: 13 | loss: 0.2706592685115625\n",
            "[Train] batch: 14 | loss: 0.4580812624133806\n",
            "[Train] batch: 15 | loss: 0.4052675118639466\n",
            "[Train] batch: 16 | loss: 0.3421122983343864\n",
            "[Train] batch: 17 | loss: 0.31097899497031084\n",
            "[Train] batch: 18 | loss: 0.18592976980789014\n",
            "[Train] batch: 19 | loss: 0.7356099458020889\n",
            "[Train] batch: 20 | loss: 0.3690941047881307\n",
            "[Train] batch: 21 | loss: 0.5043441735998009\n",
            "[Train] batch: 22 | loss: 0.46265938203494505\n",
            "[Train] batch: 23 | loss: 0.39171763367639\n",
            "[Train] batch: 24 | loss: 0.6925644757907136\n",
            "[Train] batch: 25 | loss: 0.37613060095660755\n",
            "[Train] batch: 26 | loss: 0.0832606900329993\n",
            "[Train] batch: 27 | loss: 0.508522935872748\n",
            "[Train] batch: 28 | loss: 0.5239692892908079\n",
            "[Train] batch: 29 | loss: 0.3587470835831938\n",
            "[Train] batch: 30 | loss: 0.42032656926302314\n",
            "[Train] batch: 31 | loss: 0.2602833187478043\n",
            "[Train] batch: 32 | loss: 0.32758261252359105\n",
            "[Train] batch: 33 | loss: 0.36334257082481775\n",
            "[Train] batch: 34 | loss: 0.2265614341885371\n",
            "[Train] batch: 35 | loss: 0.28706864675956484\n",
            "[Train] batch: 36 | loss: 0.26928383210461593\n",
            "[Train] batch: 37 | loss: 0.38666190712496007\n",
            "[Train] batch: 38 | loss: 0.47841267549702593\n",
            "[Train] batch: 39 | loss: 0.5495892448556486\n",
            "[Train] batch: 40 | loss: 0.3658778973698175\n",
            "[Train] batch: 41 | loss: 0.4101354284634818\n",
            "[Train] batch: 42 | loss: 0.702904055320153\n",
            "[Train] batch: 43 | loss: 0.45723789704466417\n",
            "[Train] batch: 44 | loss: 0.3061379946381866\n",
            "[Train] batch: 45 | loss: 0.41782760210892184\n",
            "[Train] batch: 46 | loss: 0.7431971234308605\n",
            "[Train] batch: 47 | loss: 0.29046339799875\n",
            "[Train] batch: 48 | loss: 0.21983191443899838\n",
            "[Train] batch: 49 | loss: 0.30310936777119046\n",
            "[Train] batch: 50 | loss: 0.4182656151699643\n",
            "[Train] batch: 51 | loss: 0.3688670334857824\n",
            "[Train] batch: 52 | loss: 0.3532406040992172\n",
            "[Train] batch: 53 | loss: 0.2912497251922049\n",
            "[Train] batch: 54 | loss: 0.2653449563246572\n",
            "[Train] batch: 55 | loss: 0.31716890873539016\n",
            "[Train] batch: 56 | loss: 0.4053529337935866\n",
            "[Train] batch: 57 | loss: 0.4534805634432774\n",
            "[Train] batch: 58 | loss: 0.20179403174641472\n",
            "[Train] batch: 59 | loss: 0.2133110874856113\n",
            "[Train] batch: 60 | loss: 0.5168555219033589\n",
            "[Train] batch: 61 | loss: 0.4068946656055558\n",
            "[Train] batch: 62 | loss: 0.3989904452267977\n",
            "[Train] batch: 63 | loss: 0.2733768365835144\n",
            "[Train] batch: 64 | loss: 0.18238413622750746\n",
            "[Train] batch: 65 | loss: 0.37358828885921197\n",
            "[Train] batch: 66 | loss: 0.6490230731456045\n",
            "[Train] batch: 67 | loss: 0.2514863132192605\n",
            "[Train] batch: 68 | loss: 0.6185225903815174\n",
            "[Train] batch: 69 | loss: 0.5435910641578289\n",
            "[Train] batch: 70 | loss: 0.23294496250454277\n",
            "[Train] batch: 71 | loss: 0.26315336169647263\n",
            "[Train] batch: 72 | loss: 0.47181394477540367\n",
            "[Train] batch: 73 | loss: 0.3398823983545614\n",
            "[Train] batch: 74 | loss: 0.49630873234269607\n",
            "[Train] batch: 75 | loss: 0.18911795179366261\n",
            "[Train] batch: 76 | loss: 0.5739534011766919\n",
            "[Train] batch: 77 | loss: 0.3312446029815469\n",
            "[Train] batch: 78 | loss: 0.39082903514862766\n",
            "[Train] batch: 79 | loss: 0.36668274407999624\n",
            "[Train] batch: 80 | loss: 0.2841149393237013\n",
            "[Train] batch: 81 | loss: 0.412622344556442\n",
            "[Train] batch: 82 | loss: 0.4924783848689578\n",
            "[Train] batch: 83 | loss: 0.21782713182574426\n",
            "[Train] batch: 84 | loss: 0.40764386485494475\n",
            "[Train] batch: 85 | loss: 0.13015041699257893\n",
            "[Train] batch: 86 | loss: 0.4195008615995902\n",
            "[Train] batch: 87 | loss: 0.34561495084170896\n",
            "[Train] batch: 88 | loss: 0.543347885603218\n",
            "[Train] batch: 89 | loss: 0.46755051640695183\n",
            "[Train] batch: 90 | loss: 0.32716853344761093\n",
            "[Train] batch: 91 | loss: 0.2965713170042599\n",
            "[Train] batch: 92 | loss: 0.21716662805328898\n",
            "[Train] batch: 93 | loss: 0.29113214091641676\n",
            "[Train] batch: 94 | loss: 0.3601472967186229\n",
            "[Train] batch: 95 | loss: 0.3601630020994491\n",
            "[Train] batch: 96 | loss: 0.1387580549743432\n",
            "[Train] batch: 97 | loss: 0.4339360312853681\n",
            "[Train] batch: 98 | loss: 0.3809333701198257\n",
            "[Train] batch: 99 | loss: 0.38593469410504067\n",
            "[Train] batch: 100 | loss: 0.17264238287831177\n",
            "[Train] batch: 101 | loss: 0.2685636400252402\n",
            "[Train] batch: 102 | loss: 0.4637198119025774\n",
            "[Train] batch: 103 | loss: 0.3347365719922453\n",
            "[Train] batch: 104 | loss: 0.35909636960164865\n",
            "[Train] batch: 105 | loss: 0.3013401569329284\n",
            "[Train] batch: 106 | loss: 0.07517814442012338\n",
            "[Train] batch: 107 | loss: 0.21426814566228503\n",
            "[Train] batch: 108 | loss: 0.16580213754404127\n",
            "[Train] batch: 109 | loss: 0.28853767471780284\n",
            "[Train] batch: 110 | loss: 0.33186720190791136\n",
            "[Train] batch: 111 | loss: 0.28389382494101073\n",
            "[Train] batch: 112 | loss: 0.24188390944076982\n",
            "[Train] batch: 113 | loss: 0.6127653276664373\n",
            "[Train] batch: 114 | loss: 0.32516379852510846\n",
            "[Train] batch: 115 | loss: 0.5980682428312256\n",
            "[Train] batch: 116 | loss: 0.32065775485730824\n",
            "[Train] batch: 117 | loss: 0.28475250965965027\n",
            "[Train] batch: 118 | loss: 0.27346955178233107\n",
            "[Train] batch: 119 | loss: 0.6172008935831389\n",
            "[Train] batch: 120 | loss: 0.42325142619972755\n",
            "[Train] batch: 121 | loss: 0.22845661028263445\n",
            "[Train] batch: 122 | loss: 0.4460987103697328\n",
            "[Train] batch: 123 | loss: 0.3094122931073973\n",
            "[Train] batch: 124 | loss: 0.3056521574678535\n",
            "[Train] batch: 125 | loss: 0.6473366640760002\n",
            "[Train] batch: 126 | loss: 0.2463729066486143\n",
            "[Train] batch: 127 | loss: 0.5550388340823441\n",
            "[Train] batch: 128 | loss: 0.4527778716506518\n",
            "[Train] batch: 129 | loss: 0.3906213760011209\n",
            "[Train] batch: 130 | loss: 0.1519613250891214\n",
            "[Train] batch: 131 | loss: 0.22483797509233933\n",
            "[Train] batch: 132 | loss: 0.3573091238155209\n",
            "[Train] batch: 133 | loss: 0.2522597982200325\n",
            "[Train] batch: 134 | loss: 0.2897945878522722\n",
            "[Train] batch: 135 | loss: 0.24078572779597643\n",
            "[Train] batch: 136 | loss: 0.46270366584815126\n",
            "[Train] batch: 137 | loss: 0.3404379855743665\n",
            "[Train] batch: 138 | loss: 0.4320837816422487\n",
            "[Train] batch: 139 | loss: 0.583749260432283\n",
            "[Train] batch: 140 | loss: 0.39492125680853724\n",
            "[Train] batch: 141 | loss: 0.32856204406556977\n",
            "[Train] batch: 142 | loss: 0.2634390300716379\n",
            "[Train] batch: 143 | loss: 0.30929900762096024\n",
            "[Train] batch: 144 | loss: 0.5674565690481096\n",
            "[Train] batch: 145 | loss: 0.47984234004642606\n",
            "[Train] batch: 146 | loss: 0.4212296005101978\n",
            "[Train] batch: 147 | loss: 0.5339244052901813\n",
            "[Train] batch: 148 | loss: 0.29820148775870897\n",
            "[Train] batch: 149 | loss: 0.3005052922273393\n",
            "[Train] batch: 150 | loss: 0.4266764667679196\n",
            "[Train] batch: 151 | loss: 0.3379577512219924\n",
            "[Train] batch: 152 | loss: 0.1248004712258257\n",
            "[Train] batch: 153 | loss: 0.3273106452508176\n",
            "[Train] batch: 154 | loss: 0.37340783011314566\n",
            "[Train] batch: 155 | loss: 0.4891703168859944\n",
            "[Train] batch: 156 | loss: 0.21861923032806874\n",
            "[Train] batch: 157 | loss: 0.49774332277194056\n",
            "[Train] batch: 158 | loss: 0.29098691986350794\n",
            "[Train] batch: 159 | loss: 0.31859174671171697\n",
            "[Train] batch: 160 | loss: 0.2493680143837311\n",
            "[Train] batch: 161 | loss: 0.5727081049455341\n",
            "[Train] batch: 162 | loss: 0.32219316599742437\n",
            "[Train] batch: 163 | loss: 0.24934053106830567\n",
            "[Train] batch: 164 | loss: 0.3120102317257123\n",
            "[Train] batch: 165 | loss: 0.2117403046081856\n",
            "[Train] batch: 166 | loss: 0.36216425710007544\n",
            "[Train] batch: 167 | loss: 0.42126552741079537\n",
            "[Train] batch: 168 | loss: 0.2509443033840134\n",
            "[Train] batch: 169 | loss: 0.34106284659172237\n",
            "[Train] batch: 170 | loss: 0.17209393298584136\n",
            "[Train] batch: 171 | loss: 0.43463423678758106\n",
            "[Train] batch: 172 | loss: 0.33871141594199317\n",
            "[Train] batch: 173 | loss: 0.48595710487469096\n",
            "[Train] batch: 174 | loss: 0.3184094567867975\n",
            "[Train] batch: 175 | loss: 0.20410669123132444\n",
            "[Train] batch: 176 | loss: 0.48109618974898904\n",
            "[Train] batch: 177 | loss: 0.35031915183142\n",
            "[Train] batch: 178 | loss: 0.16215489357659824\n",
            "[Train] batch: 179 | loss: 0.6446715237775512\n",
            "[Train] batch: 180 | loss: 0.3839148641857947\n",
            "[Train] batch: 181 | loss: 0.40465210005291313\n",
            "[Train] batch: 182 | loss: 0.31717046320447617\n",
            "[Train] batch: 183 | loss: 0.5665041729110056\n",
            "[Train] batch: 184 | loss: 0.2762059139599284\n",
            "[Train] batch: 185 | loss: 0.2522013554621644\n",
            "[Train total] loss: 0.02589321747723431 | acc: 91.89189189189189\n",
            "[Val] batch: 1 | loss: 0.5799158496178766\n",
            "[Val] batch: 2 | loss: 0.3492029112499937\n",
            "[Val] batch: 3 | loss: 0.47373805425276433\n",
            "[Val] batch: 4 | loss: 0.2833735366383614\n",
            "[Val] batch: 5 | loss: 0.25382010208938877\n",
            "[Val] batch: 6 | loss: 0.642856820320181\n",
            "[Val] batch: 7 | loss: 0.543252584688272\n",
            "[Val] batch: 8 | loss: 0.45877641196674723\n",
            "[Val] batch: 9 | loss: 0.44990150497912973\n",
            "[Val] batch: 10 | loss: 0.5810257958354456\n",
            "[Val] batch: 11 | loss: 0.4295737438954622\n",
            "[Val] batch: 12 | loss: 0.541043438554947\n",
            "[Val] batch: 13 | loss: 0.6612710319025645\n",
            "[Val] batch: 14 | loss: 0.14159299007401907\n",
            "[Val] batch: 15 | loss: 0.1588740070074112\n",
            "[Val] batch: 16 | loss: 0.23173082803344522\n",
            "[Val] batch: 17 | loss: 0.15536137320613017\n",
            "[Val] batch: 18 | loss: 0.1738726631484794\n",
            "[Val] batch: 19 | loss: 0.2057109559785215\n",
            "[Val] batch: 20 | loss: 0.5495716974284198\n",
            "[Val] batch: 21 | loss: 0.7328305641655443\n",
            "[Val] batch: 22 | loss: 0.13467180337753462\n",
            "[Val] batch: 23 | loss: 0.45843008081298253\n",
            "[Val] batch: 24 | loss: 0.5279511924112024\n",
            "[Val] batch: 25 | loss: 0.2766157593846601\n",
            "[Val] batch: 26 | loss: 0.2663452928492517\n",
            "[Val] batch: 27 | loss: 0.40503097951618156\n",
            "[Val] batch: 28 | loss: 0.28666772113287087\n",
            "[Val] batch: 29 | loss: 0.32860769414135776\n",
            "[Val] batch: 30 | loss: 0.3706871937331586\n",
            "[Val] batch: 31 | loss: 0.689034993308305\n",
            "[Val] batch: 32 | loss: 0.8954165682559221\n",
            "[Val] batch: 33 | loss: 0.41653953261731663\n",
            "[Val] batch: 34 | loss: 0.4172900951549755\n",
            "[Val] batch: 35 | loss: 0.247562891558815\n",
            "[Val] batch: 36 | loss: 0.2797395931782886\n",
            "[Val] batch: 37 | loss: 0.34660318646726224\n",
            "[Val] batch: 38 | loss: 0.2293135850626748\n",
            "[Val] batch: 39 | loss: 0.2478955315660505\n",
            "[Val] batch: 40 | loss: 0.24391503150240132\n",
            "[Val total] loss: 0.028226334398314082 | acc: 88.10810810810811\n",
            "[Val] batch: 1 | loss: 0.176279173482123\n",
            "[Val] batch: 2 | loss: 0.5816003440792056\n",
            "[Val] batch: 3 | loss: 0.5843385949379977\n",
            "[Val] batch: 4 | loss: 0.5735587590362087\n",
            "[Val] batch: 5 | loss: 0.756609613059976\n",
            "[Val] batch: 6 | loss: 0.418747952520374\n",
            "[Val] batch: 7 | loss: 0.21171164586736882\n",
            "[Val] batch: 8 | loss: 0.2534643824739698\n",
            "[Val] batch: 9 | loss: 0.21588734474353868\n",
            "[Val] batch: 10 | loss: 0.5645305698782263\n",
            "[Val] batch: 11 | loss: 0.48765099218383384\n",
            "[Val] batch: 12 | loss: 0.4274612898899618\n",
            "[Val] batch: 13 | loss: 0.38520325389193205\n",
            "[Val] batch: 14 | loss: 0.4945424229996539\n",
            "[Val] batch: 15 | loss: 0.3056475702170971\n",
            "[Val] batch: 16 | loss: 0.3887511163425483\n",
            "[Val] batch: 17 | loss: 0.25810460703433363\n",
            "[Val] batch: 18 | loss: 0.4119479218999816\n",
            "[Val] batch: 19 | loss: 0.274768003112066\n",
            "[Val] batch: 20 | loss: 0.4058330817458268\n",
            "[Val] batch: 21 | loss: 0.34186569678212786\n",
            "[Val] batch: 22 | loss: 0.7305635216403709\n",
            "[Val] batch: 23 | loss: 0.324446565464579\n",
            "[Val] batch: 24 | loss: 0.4984995024667093\n",
            "[Val] batch: 25 | loss: 0.07381333832938622\n",
            "[Val] batch: 26 | loss: 0.5254044176385387\n",
            "[Val] batch: 27 | loss: 0.5063517586727528\n",
            "[Val] batch: 28 | loss: 0.32699074088080876\n",
            "[Val] batch: 29 | loss: 0.3324214364646561\n",
            "[Val] batch: 30 | loss: 0.2867651355806115\n",
            "[Val] batch: 31 | loss: 0.3967908793977473\n",
            "[Val] batch: 32 | loss: 0.6746850601456069\n",
            "[Val] batch: 33 | loss: 0.4461688381778841\n",
            "[Val] batch: 34 | loss: 0.46014359861008725\n",
            "[Val] batch: 35 | loss: 0.3912656563143682\n",
            "[Val] batch: 36 | loss: 0.6253671160748182\n",
            "[Val] batch: 37 | loss: 0.24424066674855638\n",
            "[Val] batch: 38 | loss: 0.43998267783466843\n",
            "[Val] batch: 39 | loss: 0.3590494753162087\n",
            "[Val] batch: 40 | loss: 0.10384504038700802\n",
            "[Val total] loss: 0.029306846418601293 | acc: 87.2072072072072\n",
            ">>>>>> epoch 6 starts\n",
            "[Train] batch: 1 | loss: 0.2979451098192662\n",
            "[Train] batch: 2 | loss: 0.29257391921339465\n",
            "[Train] batch: 3 | loss: 0.4005584376021631\n",
            "[Train] batch: 4 | loss: 0.43473976133358877\n",
            "[Train] batch: 5 | loss: 0.3616784636482107\n",
            "[Train] batch: 6 | loss: 0.2298596698547013\n",
            "[Train] batch: 7 | loss: 0.16592220059383683\n",
            "[Train] batch: 8 | loss: 0.36867868166099893\n",
            "[Train] batch: 9 | loss: 0.3011804758629712\n",
            "[Train] batch: 10 | loss: 0.21890584849170516\n",
            "[Train] batch: 11 | loss: 0.37865907719305403\n",
            "[Train] batch: 12 | loss: 0.29836813012110097\n",
            "[Train] batch: 13 | loss: 0.3195501284386853\n",
            "[Train] batch: 14 | loss: 0.271444843427859\n",
            "[Train] batch: 15 | loss: 0.22464585880586965\n",
            "[Train] batch: 16 | loss: 0.4961684003981824\n",
            "[Train] batch: 17 | loss: 0.40808808447518846\n",
            "[Train] batch: 18 | loss: 0.24212454628466912\n",
            "[Train] batch: 19 | loss: 0.35313729042763375\n",
            "[Train] batch: 20 | loss: 0.26644995817490064\n",
            "[Train] batch: 21 | loss: 0.6357967772590586\n",
            "[Train] batch: 22 | loss: 0.16635402421583817\n",
            "[Train] batch: 23 | loss: 0.5616539851187179\n",
            "[Train] batch: 24 | loss: 0.27235127588454267\n",
            "[Train] batch: 25 | loss: 0.44383586263256536\n",
            "[Train] batch: 26 | loss: 0.2773135978359199\n",
            "[Train] batch: 27 | loss: 0.5312657137038014\n",
            "[Train] batch: 28 | loss: 0.5707056843461841\n",
            "[Train] batch: 29 | loss: 0.33024454464207315\n",
            "[Train] batch: 30 | loss: 0.3806349175957621\n",
            "[Train] batch: 31 | loss: 0.36293682706922514\n",
            "[Train] batch: 32 | loss: 0.8111783858081429\n",
            "[Train] batch: 33 | loss: 0.24656576895128118\n",
            "[Train] batch: 34 | loss: 0.15787582756234886\n",
            "[Train] batch: 35 | loss: 0.2020293385293055\n",
            "[Train] batch: 36 | loss: 0.3856458063403228\n",
            "[Train] batch: 37 | loss: 0.2158446536567633\n",
            "[Train] batch: 38 | loss: 0.3607434897905014\n",
            "[Train] batch: 39 | loss: 0.29199944177616294\n",
            "[Train] batch: 40 | loss: 0.2067140699236145\n",
            "[Train] batch: 41 | loss: 0.5506471562600943\n",
            "[Train] batch: 42 | loss: 0.33350938556614584\n",
            "[Train] batch: 43 | loss: 0.5389507159244763\n",
            "[Train] batch: 44 | loss: 0.2925009947555851\n",
            "[Train] batch: 45 | loss: 0.4484788586011463\n",
            "[Train] batch: 46 | loss: 0.25730465837627287\n",
            "[Train] batch: 47 | loss: 0.3320082325592404\n",
            "[Train] batch: 48 | loss: 0.26188303646028344\n",
            "[Train] batch: 49 | loss: 0.16688950993808185\n",
            "[Train] batch: 50 | loss: 0.17906025912902443\n",
            "[Train] batch: 51 | loss: 0.18346940610186607\n",
            "[Train] batch: 52 | loss: 0.4262606562454771\n",
            "[Train] batch: 53 | loss: 0.22994700815810992\n",
            "[Train] batch: 54 | loss: 0.282878295440192\n",
            "[Train] batch: 55 | loss: 0.3621620759559658\n",
            "[Train] batch: 56 | loss: 0.5080984965010626\n",
            "[Train] batch: 57 | loss: 0.20244880518620958\n",
            "[Train] batch: 58 | loss: 0.20437593417072938\n",
            "[Train] batch: 59 | loss: 0.263610064457403\n",
            "[Train] batch: 60 | loss: 0.48215376727225673\n",
            "[Train] batch: 61 | loss: 0.2358910493705912\n",
            "[Train] batch: 62 | loss: 0.3875472363761006\n",
            "[Train] batch: 63 | loss: 0.35216744605760003\n",
            "[Train] batch: 64 | loss: 0.23603469372438632\n",
            "[Train] batch: 65 | loss: 0.3115481060921567\n",
            "[Train] batch: 66 | loss: 0.4405363278288644\n",
            "[Train] batch: 67 | loss: 0.38160289988653506\n",
            "[Train] batch: 68 | loss: 0.22997524285530252\n",
            "[Train] batch: 69 | loss: 0.19074420706009385\n",
            "[Train] batch: 70 | loss: 0.28485529666275716\n",
            "[Train] batch: 71 | loss: 0.6236724050430461\n",
            "[Train] batch: 72 | loss: 0.19036993482886228\n",
            "[Train] batch: 73 | loss: 0.2741693478375206\n",
            "[Train] batch: 74 | loss: 0.5044609995194128\n",
            "[Train] batch: 75 | loss: 0.20707958722749656\n",
            "[Train] batch: 76 | loss: 0.1781286716571917\n",
            "[Train] batch: 77 | loss: 0.4871184312615499\n",
            "[Train] batch: 78 | loss: 0.19645002908847187\n",
            "[Train] batch: 79 | loss: 0.16266645438761526\n",
            "[Train] batch: 80 | loss: 0.202097183514865\n",
            "[Train] batch: 81 | loss: 0.42544045964191246\n",
            "[Train] batch: 82 | loss: 0.4910070169656076\n",
            "[Train] batch: 83 | loss: 0.17921151006271605\n",
            "[Train] batch: 84 | loss: 0.4407699122100822\n",
            "[Train] batch: 85 | loss: 0.15344203169983886\n",
            "[Train] batch: 86 | loss: 0.38729837234988995\n",
            "[Train] batch: 87 | loss: 0.39104252259908484\n",
            "[Train] batch: 88 | loss: 0.5011560789956131\n",
            "[Train] batch: 89 | loss: 0.25921245663922904\n",
            "[Train] batch: 90 | loss: 0.2649023635609525\n",
            "[Train] batch: 91 | loss: 0.311112392612609\n",
            "[Train] batch: 92 | loss: 0.17289129819657686\n",
            "[Train] batch: 93 | loss: 0.46469073480338985\n",
            "[Train] batch: 94 | loss: 0.2725908051820413\n",
            "[Train] batch: 95 | loss: 0.17110660091785543\n",
            "[Train] batch: 96 | loss: 0.3337859504203401\n",
            "[Train] batch: 97 | loss: 0.42884725750822383\n",
            "[Train] batch: 98 | loss: 0.4403658331625126\n",
            "[Train] batch: 99 | loss: 0.46856824212936815\n",
            "[Train] batch: 100 | loss: 0.1721284677781101\n",
            "[Train] batch: 101 | loss: 0.35280815516326924\n",
            "[Train] batch: 102 | loss: 0.38180004928531514\n",
            "[Train] batch: 103 | loss: 0.3256821283125188\n",
            "[Train] batch: 104 | loss: 0.20298325890463612\n",
            "[Train] batch: 105 | loss: 0.32411237876381815\n",
            "[Train] batch: 106 | loss: 0.16036073629401967\n",
            "[Train] batch: 107 | loss: 0.11171863568134197\n",
            "[Train] batch: 108 | loss: 0.1064133266052392\n",
            "[Train] batch: 109 | loss: 0.2315524770796324\n",
            "[Train] batch: 110 | loss: 0.2725148942051056\n",
            "[Train] batch: 111 | loss: 0.32434138758127223\n",
            "[Train] batch: 112 | loss: 0.21368506475964705\n",
            "[Train] batch: 113 | loss: 0.38940102187441716\n",
            "[Train] batch: 114 | loss: 0.25128022325730076\n",
            "[Train] batch: 115 | loss: 0.28068317087247\n",
            "[Train] batch: 116 | loss: 0.13145124191419447\n",
            "[Train] batch: 117 | loss: 0.32618865579092754\n",
            "[Train] batch: 118 | loss: 0.25315435315507334\n",
            "[Train] batch: 119 | loss: 0.28674599233427667\n",
            "[Train] batch: 120 | loss: 0.3762021583174528\n",
            "[Train] batch: 121 | loss: 0.21051922724991487\n",
            "[Train] batch: 122 | loss: 0.3729673611446094\n",
            "[Train] batch: 123 | loss: 0.1752062468670646\n",
            "[Train] batch: 124 | loss: 0.2848395809134486\n",
            "[Train] batch: 125 | loss: 0.348141045937532\n",
            "[Train] batch: 126 | loss: 0.31652557737119186\n",
            "[Train] batch: 127 | loss: 0.2750182145053458\n",
            "[Train] batch: 128 | loss: 0.17549669036530374\n",
            "[Train] batch: 129 | loss: 0.38635398572961765\n",
            "[Train] batch: 130 | loss: 0.32180093042979346\n",
            "[Train] batch: 131 | loss: 0.21323732293311684\n",
            "[Train] batch: 132 | loss: 0.3051776114419536\n",
            "[Train] batch: 133 | loss: 0.24609458307040208\n",
            "[Train] batch: 134 | loss: 0.3326674198470041\n",
            "[Train] batch: 135 | loss: 0.3363458132972161\n",
            "[Train] batch: 136 | loss: 0.1553557519140662\n",
            "[Train] batch: 137 | loss: 0.17413282120874488\n",
            "[Train] batch: 138 | loss: 0.36663654166088916\n",
            "[Train] batch: 139 | loss: 0.3096365448377611\n",
            "[Train] batch: 140 | loss: 0.3213188787185894\n",
            "[Train] batch: 141 | loss: 0.16651932191374594\n",
            "[Train] batch: 142 | loss: 0.3248905866590394\n",
            "[Train] batch: 143 | loss: 0.15953641756826095\n",
            "[Train] batch: 144 | loss: 0.41810251908110196\n",
            "[Train] batch: 145 | loss: 0.2966378077613852\n",
            "[Train] batch: 146 | loss: 0.5558145889869436\n",
            "[Train] batch: 147 | loss: 0.07440239270119998\n",
            "[Train] batch: 148 | loss: 0.32765592978666425\n",
            "[Train] batch: 149 | loss: 0.17037195262946656\n",
            "[Train] batch: 150 | loss: 0.24130892549248317\n",
            "[Train] batch: 151 | loss: 0.2775930278496615\n",
            "[Train] batch: 152 | loss: 0.19588873048519825\n",
            "[Train] batch: 153 | loss: 0.2646449340438078\n",
            "[Train] batch: 154 | loss: 0.17160665777757816\n",
            "[Train] batch: 155 | loss: 0.3425882431808343\n",
            "[Train] batch: 156 | loss: 0.4711967166830057\n",
            "[Train] batch: 157 | loss: 0.2821798627833229\n",
            "[Train] batch: 158 | loss: 0.17809093531824824\n",
            "[Train] batch: 159 | loss: 0.2501002253017919\n",
            "[Train] batch: 160 | loss: 0.2697993324004619\n",
            "[Train] batch: 161 | loss: 0.14651986426495753\n",
            "[Train] batch: 162 | loss: 0.44856378467258295\n",
            "[Train] batch: 163 | loss: 0.34519308491959794\n",
            "[Train] batch: 164 | loss: 0.34297793542958027\n",
            "[Train] batch: 165 | loss: 0.2745118411439962\n",
            "[Train] batch: 166 | loss: 0.16207813381955896\n",
            "[Train] batch: 167 | loss: 0.16222313856960066\n",
            "[Train] batch: 168 | loss: 0.5530767464350468\n",
            "[Train] batch: 169 | loss: 0.2118661953483727\n",
            "[Train] batch: 170 | loss: 0.1865226949417201\n",
            "[Train] batch: 171 | loss: 0.19785921838067014\n",
            "[Train] batch: 172 | loss: 0.2616465389605677\n",
            "[Train] batch: 173 | loss: 0.1734530525196764\n",
            "[Train] batch: 174 | loss: 0.2549150523444178\n",
            "[Train] batch: 175 | loss: 0.23655276864594543\n",
            "[Train] batch: 176 | loss: 0.13690993993848116\n",
            "[Train] batch: 177 | loss: 0.2915724010639794\n",
            "[Train] batch: 178 | loss: 0.05882472436133584\n",
            "[Train] batch: 179 | loss: 0.24443425614355382\n",
            "[Train] batch: 180 | loss: 0.22346006700136845\n",
            "[Train] batch: 181 | loss: 0.5019022467839733\n",
            "[Train] batch: 182 | loss: 0.13074856449577446\n",
            "[Train] batch: 183 | loss: 0.48976584492801495\n",
            "[Train] batch: 184 | loss: 0.42252508534368144\n",
            "[Train] batch: 185 | loss: 0.2399593449065807\n",
            "[Train total] loss: 0.02166920275930844 | acc: 93.20463320463321\n",
            "[Val] batch: 1 | loss: 0.46234706928927766\n",
            "[Val] batch: 2 | loss: 0.26452977031072356\n",
            "[Val] batch: 3 | loss: 0.33910316418134834\n",
            "[Val] batch: 4 | loss: 0.21798444997620642\n",
            "[Val] batch: 5 | loss: 0.20157996340014409\n",
            "[Val] batch: 6 | loss: 0.5081259257163683\n",
            "[Val] batch: 7 | loss: 0.4737006586404801\n",
            "[Val] batch: 8 | loss: 0.319473690229431\n",
            "[Val] batch: 9 | loss: 0.3025132836005356\n",
            "[Val] batch: 10 | loss: 0.3905666627120564\n",
            "[Val] batch: 11 | loss: 0.3634126736120592\n",
            "[Val] batch: 12 | loss: 0.4360630455225535\n",
            "[Val] batch: 13 | loss: 0.4341645949766074\n",
            "[Val] batch: 14 | loss: 0.1526976717849024\n",
            "[Val] batch: 15 | loss: 0.09771730943533238\n",
            "[Val] batch: 16 | loss: 0.15711298637344903\n",
            "[Val] batch: 17 | loss: 0.09578057408083898\n",
            "[Val] batch: 18 | loss: 0.1751871650760464\n",
            "[Val] batch: 19 | loss: 0.16399077569475579\n",
            "[Val] batch: 20 | loss: 0.4288603337349027\n",
            "[Val] batch: 21 | loss: 0.5137215639759807\n",
            "[Val] batch: 22 | loss: 0.09781039967915363\n",
            "[Val] batch: 23 | loss: 0.3711993727434418\n",
            "[Val] batch: 24 | loss: 0.3697413576146839\n",
            "[Val] batch: 25 | loss: 0.20611448712202485\n",
            "[Val] batch: 26 | loss: 0.17876632509488766\n",
            "[Val] batch: 27 | loss: 0.30722259529919166\n",
            "[Val] batch: 28 | loss: 0.2374588766919934\n",
            "[Val] batch: 29 | loss: 0.22078723748524823\n",
            "[Val] batch: 30 | loss: 0.327149675897156\n",
            "[Val] batch: 31 | loss: 0.506400591616026\n",
            "[Val] batch: 32 | loss: 0.666363210401076\n",
            "[Val] batch: 33 | loss: 0.33425577978534626\n",
            "[Val] batch: 34 | loss: 0.28943808970262197\n",
            "[Val] batch: 35 | loss: 0.15874717782136427\n",
            "[Val] batch: 36 | loss: 0.18797775180060214\n",
            "[Val] batch: 37 | loss: 0.2782523041179352\n",
            "[Val] batch: 38 | loss: 0.17437428491388265\n",
            "[Val] batch: 39 | loss: 0.18885849388138126\n",
            "[Val] batch: 40 | loss: 0.17805732901635485\n",
            "[Val total] loss: 0.021220916527943022 | acc: 94.77477477477477\n",
            "[Val] batch: 1 | loss: 0.18571151186394647\n",
            "[Val] batch: 2 | loss: 0.41988271366584323\n",
            "[Val] batch: 3 | loss: 0.43976908889733946\n",
            "[Val] batch: 4 | loss: 0.4620779302754423\n",
            "[Val] batch: 5 | loss: 0.5366334071564826\n",
            "[Val] batch: 6 | loss: 0.3239105420058358\n",
            "[Val] batch: 7 | loss: 0.17458271344095277\n",
            "[Val] batch: 8 | loss: 0.1769835986328431\n",
            "[Val] batch: 9 | loss: 0.14702350392618607\n",
            "[Val] batch: 10 | loss: 0.43821789860091426\n",
            "[Val] batch: 11 | loss: 0.3804181317837589\n",
            "[Val] batch: 12 | loss: 0.2846712643629289\n",
            "[Val] batch: 13 | loss: 0.27642918830901914\n",
            "[Val] batch: 14 | loss: 0.4003123791047721\n",
            "[Val] batch: 15 | loss: 0.25896512578417213\n",
            "[Val] batch: 16 | loss: 0.2702574049258318\n",
            "[Val] batch: 17 | loss: 0.22856852742338088\n",
            "[Val] batch: 18 | loss: 0.28194433020057047\n",
            "[Val] batch: 19 | loss: 0.20227017709735465\n",
            "[Val] batch: 20 | loss: 0.2617473307142302\n",
            "[Val] batch: 21 | loss: 0.22443535474807014\n",
            "[Val] batch: 22 | loss: 0.5179914104751505\n",
            "[Val] batch: 23 | loss: 0.22977780933343536\n",
            "[Val] batch: 24 | loss: 0.36167248601995405\n",
            "[Val] batch: 25 | loss: 0.06515931078105322\n",
            "[Val] batch: 26 | loss: 0.3905225019798725\n",
            "[Val] batch: 27 | loss: 0.35394148153963056\n",
            "[Val] batch: 28 | loss: 0.2050358111334726\n",
            "[Val] batch: 29 | loss: 0.2353342765723188\n",
            "[Val] batch: 30 | loss: 0.21291674447241693\n",
            "[Val] batch: 31 | loss: 0.26376679342671117\n",
            "[Val] batch: 32 | loss: 0.5298448774224845\n",
            "[Val] batch: 33 | loss: 0.3188040366162661\n",
            "[Val] batch: 34 | loss: 0.34226356098215704\n",
            "[Val] batch: 35 | loss: 0.3187840607331235\n",
            "[Val] batch: 36 | loss: 0.42498352322653193\n",
            "[Val] batch: 37 | loss: 0.18415998646767898\n",
            "[Val] batch: 38 | loss: 0.3324926721981796\n",
            "[Val] batch: 39 | loss: 0.25633280036653344\n",
            "[Val] batch: 40 | loss: 0.09072045105680714\n",
            "[Val total] loss: 0.021638408500402974 | acc: 93.15315315315316\n",
            ">>>>>> epoch 7 starts\n",
            "[Train] batch: 1 | loss: 0.2145911355744507\n",
            "[Train] batch: 2 | loss: 0.3339261693727039\n",
            "[Train] batch: 3 | loss: 0.22949137739647815\n",
            "[Train] batch: 4 | loss: 0.403561624096676\n",
            "[Train] batch: 5 | loss: 0.20887099652854904\n",
            "[Train] batch: 6 | loss: 0.41691494767869786\n",
            "[Train] batch: 7 | loss: 0.24283362718281395\n",
            "[Train] batch: 8 | loss: 0.18982353272686336\n",
            "[Train] batch: 9 | loss: 0.37433676837015456\n",
            "[Train] batch: 10 | loss: 0.16421024002846116\n",
            "[Train] batch: 11 | loss: 0.21101649577459583\n",
            "[Train] batch: 12 | loss: 0.25764334743481926\n",
            "[Train] batch: 13 | loss: 0.23832760614972867\n",
            "[Train] batch: 14 | loss: 0.49469413182146404\n",
            "[Train] batch: 15 | loss: 0.5762757680262984\n",
            "[Train] batch: 16 | loss: 0.15196475492770636\n",
            "[Train] batch: 17 | loss: 0.1582839438827905\n",
            "[Train] batch: 18 | loss: 0.16058027160820812\n",
            "[Train] batch: 19 | loss: 0.47781542409643135\n",
            "[Train] batch: 20 | loss: 0.36872391684631395\n",
            "[Train] batch: 21 | loss: 0.33496222279846294\n",
            "[Train] batch: 22 | loss: 0.6125977316706029\n",
            "[Train] batch: 23 | loss: 0.3140825753707977\n",
            "[Train] batch: 24 | loss: 0.3403210449957174\n",
            "[Train] batch: 25 | loss: 0.20794986561374737\n",
            "[Train] batch: 26 | loss: 0.22056181591041166\n",
            "[Train] batch: 27 | loss: 0.41605416511119137\n",
            "[Train] batch: 28 | loss: 0.033014318839431275\n",
            "[Train] batch: 29 | loss: 0.1260541836787402\n",
            "[Train] batch: 30 | loss: 0.4733495767477415\n",
            "[Train] batch: 31 | loss: 0.1603110543627684\n",
            "[Train] batch: 32 | loss: 0.27194658321251075\n",
            "[Train] batch: 33 | loss: 0.24308509077178844\n",
            "[Train] batch: 34 | loss: 0.19798949140156538\n",
            "[Train] batch: 35 | loss: 0.3335357574347439\n",
            "[Train] batch: 36 | loss: 0.1195578376302927\n",
            "[Train] batch: 37 | loss: 0.2206020400355931\n",
            "[Train] batch: 38 | loss: 0.39671326318819694\n",
            "[Train] batch: 39 | loss: 0.4639497857229392\n",
            "[Train] batch: 40 | loss: 0.2518831419054148\n",
            "[Train] batch: 41 | loss: 0.41035396873239544\n",
            "[Train] batch: 42 | loss: 0.1672435877914622\n",
            "[Train] batch: 43 | loss: 0.17924697254437993\n",
            "[Train] batch: 44 | loss: 0.3659083061978224\n",
            "[Train] batch: 45 | loss: 0.3764379834864702\n",
            "[Train] batch: 46 | loss: 0.29212728630097223\n",
            "[Train] batch: 47 | loss: 0.24082541091110363\n",
            "[Train] batch: 48 | loss: 0.08248011166426669\n",
            "[Train] batch: 49 | loss: 0.2542324242283871\n",
            "[Train] batch: 50 | loss: 0.16449581985211226\n",
            "[Train] batch: 51 | loss: 0.23210267275566396\n",
            "[Train] batch: 52 | loss: 0.05390661037392714\n",
            "[Train] batch: 53 | loss: 0.21238659264140874\n",
            "[Train] batch: 54 | loss: 0.3082327248009332\n",
            "[Train] batch: 55 | loss: 0.192313944138347\n",
            "[Train] batch: 56 | loss: 0.21169659412706135\n",
            "[Train] batch: 57 | loss: 0.2544580559606739\n",
            "[Train] batch: 58 | loss: 0.3774148307007262\n",
            "[Train] batch: 59 | loss: 0.35009947121116397\n",
            "[Train] batch: 60 | loss: 0.19635728845625516\n",
            "[Train] batch: 61 | loss: 0.24194699151130453\n",
            "[Train] batch: 62 | loss: 0.18767461073898153\n",
            "[Train] batch: 63 | loss: 0.43918099662029003\n",
            "[Train] batch: 64 | loss: 0.11542958518436046\n",
            "[Train] batch: 65 | loss: 0.22003095363622274\n",
            "[Train] batch: 66 | loss: 0.43612942277726174\n",
            "[Train] batch: 67 | loss: 0.4255772396775093\n",
            "[Train] batch: 68 | loss: 0.6921489841184357\n",
            "[Train] batch: 69 | loss: 0.17393950943139833\n",
            "[Train] batch: 70 | loss: 0.05440458191720237\n",
            "[Train] batch: 71 | loss: 0.2913398873829016\n",
            "[Train] batch: 72 | loss: 0.27664015151677385\n",
            "[Train] batch: 73 | loss: 0.3103176774378289\n",
            "[Train] batch: 74 | loss: 0.19112752836672922\n",
            "[Train] batch: 75 | loss: 0.14351097976385394\n",
            "[Train] batch: 76 | loss: 0.30608113823562544\n",
            "[Train] batch: 77 | loss: 0.07968269847254462\n",
            "[Train] batch: 78 | loss: 0.14268176726211554\n",
            "[Train] batch: 79 | loss: 0.1720976987532486\n",
            "[Train] batch: 80 | loss: 0.2640274976140694\n",
            "[Train] batch: 81 | loss: 0.31502078091070385\n",
            "[Train] batch: 82 | loss: 0.261419171924831\n",
            "[Train] batch: 83 | loss: 0.2559461160594708\n",
            "[Train] batch: 84 | loss: 0.10390274665237233\n",
            "[Train] batch: 85 | loss: 0.2764974416550531\n",
            "[Train] batch: 86 | loss: 0.15284771213090442\n",
            "[Train] batch: 87 | loss: 0.4282206987132904\n",
            "[Train] batch: 88 | loss: 0.2135622974360953\n",
            "[Train] batch: 89 | loss: 0.1533696602808979\n",
            "[Train] batch: 90 | loss: 0.1350844732084619\n",
            "[Train] batch: 91 | loss: 0.30982823218166283\n",
            "[Train] batch: 92 | loss: 0.10800337197382161\n",
            "[Train] batch: 93 | loss: 0.2571718384394984\n",
            "[Train] batch: 94 | loss: 0.1597970746249047\n",
            "[Train] batch: 95 | loss: 0.1237837242608439\n",
            "[Train] batch: 96 | loss: 0.25394062701360215\n",
            "[Train] batch: 97 | loss: 0.3016012063825339\n",
            "[Train] batch: 98 | loss: 0.14298616644503365\n",
            "[Train] batch: 99 | loss: 0.2759513495941179\n",
            "[Train] batch: 100 | loss: 0.19364624867827737\n",
            "[Train] batch: 101 | loss: 0.4132474779880706\n",
            "[Train] batch: 102 | loss: 0.2602546414457361\n",
            "[Train] batch: 103 | loss: 0.3427388348334289\n",
            "[Train] batch: 104 | loss: 0.34935202813743643\n",
            "[Train] batch: 105 | loss: 0.3023009386511168\n",
            "[Train] batch: 106 | loss: 0.11520598844980638\n",
            "[Train] batch: 107 | loss: 0.21367923312950174\n",
            "[Train] batch: 108 | loss: 0.6193805418817335\n",
            "[Train] batch: 109 | loss: 0.09785032536610005\n",
            "[Train] batch: 110 | loss: 0.2398929186879697\n",
            "[Train] batch: 111 | loss: 0.25435689682933693\n",
            "[Train] batch: 112 | loss: 0.20206525962025965\n",
            "[Train] batch: 113 | loss: 0.11160427883710018\n",
            "[Train] batch: 114 | loss: 0.13684834115736375\n",
            "[Train] batch: 115 | loss: 0.11876666324512045\n",
            "[Train] batch: 116 | loss: 0.3748114613851354\n",
            "[Train] batch: 117 | loss: 0.24646095070635127\n",
            "[Train] batch: 118 | loss: 0.5382204948899375\n",
            "[Train] batch: 119 | loss: 0.11302516598507603\n",
            "[Train] batch: 120 | loss: 0.29708523374327905\n",
            "[Train] batch: 121 | loss: 0.12128734121782095\n",
            "[Train] batch: 122 | loss: 0.4951615044470854\n",
            "[Train] batch: 123 | loss: 0.18086000592190504\n",
            "[Train] batch: 124 | loss: 0.22248954512055677\n",
            "[Train] batch: 125 | loss: 0.27415237092923894\n",
            "[Train] batch: 126 | loss: 0.3800822794279634\n",
            "[Train] batch: 127 | loss: 0.1483313096266213\n",
            "[Train] batch: 128 | loss: 0.3112596917380201\n",
            "[Train] batch: 129 | loss: 0.1316138394600235\n",
            "[Train] batch: 130 | loss: 0.5153086734663597\n",
            "[Train] batch: 131 | loss: 0.5010110122099977\n",
            "[Train] batch: 132 | loss: 0.29237741808247214\n",
            "[Train] batch: 133 | loss: 0.1893596752721484\n",
            "[Train] batch: 134 | loss: 0.5935420223762655\n",
            "[Train] batch: 135 | loss: 0.14821002396403277\n",
            "[Train] batch: 136 | loss: 0.3266954705361682\n",
            "[Train] batch: 137 | loss: 0.12072243843792016\n",
            "[Train] batch: 138 | loss: 0.3003146698803064\n",
            "[Train] batch: 139 | loss: 0.27348053405198475\n",
            "[Train] batch: 140 | loss: 0.16025408871691063\n",
            "[Train] batch: 141 | loss: 0.09697014859498584\n",
            "[Train] batch: 142 | loss: 0.107471300774296\n",
            "[Train] batch: 143 | loss: 0.18618218074170678\n",
            "[Train] batch: 144 | loss: 0.23271727648309568\n",
            "[Train] batch: 145 | loss: 0.18917384835472775\n",
            "[Train] batch: 146 | loss: 0.0673428787982581\n",
            "[Train] batch: 147 | loss: 0.1995105703000919\n",
            "[Train] batch: 148 | loss: 0.1814309074481022\n",
            "[Train] batch: 149 | loss: 0.19839780042270969\n",
            "[Train] batch: 150 | loss: 0.4500516709930798\n",
            "[Train] batch: 151 | loss: 0.09019490769566842\n",
            "[Train] batch: 152 | loss: 0.17720394195101943\n",
            "[Train] batch: 153 | loss: 0.29003051521975715\n",
            "[Train] batch: 154 | loss: 0.11387976807945578\n",
            "[Train] batch: 155 | loss: 0.2610049259975939\n",
            "[Train] batch: 156 | loss: 0.1387202177246839\n",
            "[Train] batch: 157 | loss: 0.15191657495423563\n",
            "[Train] batch: 158 | loss: 0.39982864794231726\n",
            "[Train] batch: 159 | loss: 0.4341072498263939\n",
            "[Train] batch: 160 | loss: 0.2748988595545109\n",
            "[Train] batch: 161 | loss: 0.07756478032420179\n",
            "[Train] batch: 162 | loss: 0.13281759453018943\n",
            "[Train] batch: 163 | loss: 0.23976325216268216\n",
            "[Train] batch: 164 | loss: 0.28635744286790954\n",
            "[Train] batch: 165 | loss: 0.06894668646097915\n",
            "[Train] batch: 166 | loss: 0.12596795617439557\n",
            "[Train] batch: 167 | loss: 0.21900803035709263\n",
            "[Train] batch: 168 | loss: 0.2327217477746906\n",
            "[Train] batch: 169 | loss: 0.24404484310165794\n",
            "[Train] batch: 170 | loss: 0.14887791812055903\n",
            "[Train] batch: 171 | loss: 0.20924941112010087\n",
            "[Train] batch: 172 | loss: 0.5108371755005497\n",
            "[Train] batch: 173 | loss: 0.22125020728557512\n",
            "[Train] batch: 174 | loss: 0.13982664650575274\n",
            "[Train] batch: 175 | loss: 0.6495646839786716\n",
            "[Train] batch: 176 | loss: 0.245162295217037\n",
            "[Train] batch: 177 | loss: 0.31799468797021235\n",
            "[Train] batch: 178 | loss: 0.3677284792324765\n",
            "[Train] batch: 179 | loss: 0.5191963536053608\n",
            "[Train] batch: 180 | loss: 0.2683906754756845\n",
            "[Train] batch: 181 | loss: 0.15569980290628807\n",
            "[Train] batch: 182 | loss: 0.20245013224941336\n",
            "[Train] batch: 183 | loss: 0.1399274279755479\n",
            "[Train] batch: 184 | loss: 0.11523082644960668\n",
            "[Train] batch: 185 | loss: 0.42889985304343453\n",
            "[Train total] loss: 0.01840287792042002 | acc: 95.32818532818533\n",
            "[Val] batch: 1 | loss: 0.4313011831539954\n",
            "[Val] batch: 2 | loss: 0.21655132890427123\n",
            "[Val] batch: 3 | loss: 0.2971759833586139\n",
            "[Val] batch: 4 | loss: 0.15751606776758859\n",
            "[Val] batch: 5 | loss: 0.13319441547021266\n",
            "[Val] batch: 6 | loss: 0.42378565447919087\n",
            "[Val] batch: 7 | loss: 0.3674642324746132\n",
            "[Val] batch: 8 | loss: 0.2833782826626704\n",
            "[Val] batch: 9 | loss: 0.27029940747415876\n",
            "[Val] batch: 10 | loss: 0.3934080380734218\n",
            "[Val] batch: 11 | loss: 0.30806127498277724\n",
            "[Val] batch: 12 | loss: 0.37927898972409163\n",
            "[Val] batch: 13 | loss: 0.38053051650856695\n",
            "[Val] batch: 14 | loss: 0.09179857933205429\n",
            "[Val] batch: 15 | loss: 0.06929106934880506\n",
            "[Val] batch: 16 | loss: 0.1261073861632875\n",
            "[Val] batch: 17 | loss: 0.0919851133494849\n",
            "[Val] batch: 18 | loss: 0.09810834078305726\n",
            "[Val] batch: 19 | loss: 0.14882155917128173\n",
            "[Val] batch: 20 | loss: 0.38484420617876175\n",
            "[Val] batch: 21 | loss: 0.5144156660970091\n",
            "[Val] batch: 22 | loss: 0.07865176400339068\n",
            "[Val] batch: 23 | loss: 0.30931402850850537\n",
            "[Val] batch: 24 | loss: 0.34038514176397955\n",
            "[Val] batch: 25 | loss: 0.18331786878857312\n",
            "[Val] batch: 26 | loss: 0.15179226941094362\n",
            "[Val] batch: 27 | loss: 0.25353694704693264\n",
            "[Val] batch: 28 | loss: 0.22634167985515224\n",
            "[Val] batch: 29 | loss: 0.17501213958789502\n",
            "[Val] batch: 30 | loss: 0.2944855715162503\n",
            "[Val] batch: 31 | loss: 0.4931168933852676\n",
            "[Val] batch: 32 | loss: 0.6124939618111054\n",
            "[Val] batch: 33 | loss: 0.29606986515583406\n",
            "[Val] batch: 34 | loss: 0.26882508227970187\n",
            "[Val] batch: 35 | loss: 0.13715556454436972\n",
            "[Val] batch: 36 | loss: 0.15228531870759868\n",
            "[Val] batch: 37 | loss: 0.24002899817452591\n",
            "[Val] batch: 38 | loss: 0.13556693185152618\n",
            "[Val] batch: 39 | loss: 0.133305263131299\n",
            "[Val] batch: 40 | loss: 0.14101789861208325\n",
            "[Val total] loss: 0.01836039726773487 | acc: 93.87387387387388\n",
            ">>>>>> epoch 8 starts\n",
            "[Train] batch: 1 | loss: 0.5567436071920869\n",
            "[Train] batch: 2 | loss: 0.1577156654813778\n",
            "[Train] batch: 3 | loss: 0.21612504022586404\n",
            "[Train] batch: 4 | loss: 0.2918347979973631\n",
            "[Train] batch: 5 | loss: 0.29609108647918614\n",
            "[Train] batch: 6 | loss: 0.23056780258261605\n",
            "[Train] batch: 7 | loss: 0.2387665179438603\n",
            "[Train] batch: 8 | loss: 0.2706602012171147\n",
            "[Train] batch: 9 | loss: 0.2660068188506783\n",
            "[Train] batch: 10 | loss: 0.13590131931349345\n",
            "[Train] batch: 11 | loss: 0.2511457000931035\n",
            "[Train] batch: 12 | loss: 0.09663476906033819\n",
            "[Train] batch: 13 | loss: 0.3779416238336183\n",
            "[Train] batch: 14 | loss: 0.09389971951219823\n",
            "[Train] batch: 15 | loss: 0.045533341925910034\n",
            "[Train] batch: 16 | loss: 0.347778652583759\n",
            "[Train] batch: 17 | loss: 0.29509766056204895\n",
            "[Train] batch: 18 | loss: 0.18170536668923806\n",
            "[Train] batch: 19 | loss: 0.14661383023072738\n",
            "[Train] batch: 20 | loss: 0.23978431413577125\n",
            "[Train] batch: 21 | loss: 0.3325995512243245\n",
            "[Train] batch: 22 | loss: 0.11579131964255586\n",
            "[Train] batch: 23 | loss: 0.3170387732634805\n",
            "[Train] batch: 24 | loss: 0.19692200011328678\n",
            "[Train] batch: 25 | loss: 0.11113969184866061\n",
            "[Train] batch: 26 | loss: 0.19955173027192455\n",
            "[Train] batch: 27 | loss: 0.20505428785431984\n",
            "[Train] batch: 28 | loss: 0.11167692571355495\n",
            "[Train] batch: 29 | loss: 0.21880396564900914\n",
            "[Train] batch: 30 | loss: 0.22277537156309757\n",
            "[Train] batch: 31 | loss: 0.22215912262722864\n",
            "[Train] batch: 32 | loss: 0.13689449804277107\n",
            "[Train] batch: 33 | loss: 0.18119967732019188\n",
            "[Train] batch: 34 | loss: 0.37028160375417773\n",
            "[Train] batch: 35 | loss: 0.17563066879618555\n",
            "[Train] batch: 36 | loss: 0.1634261336060032\n",
            "[Train] batch: 37 | loss: 0.15507275095121606\n",
            "[Train] batch: 38 | loss: 0.09872438333764645\n",
            "[Train] batch: 39 | loss: 0.2553544792382823\n",
            "[Train] batch: 40 | loss: 0.13611848309113353\n",
            "[Train] batch: 41 | loss: 0.16717415476622954\n",
            "[Train] batch: 42 | loss: 0.2370860429306784\n",
            "[Train] batch: 43 | loss: 0.2816386169880741\n",
            "[Train] batch: 44 | loss: 0.21747519910151672\n",
            "[Train] batch: 45 | loss: 0.09414676847852552\n",
            "[Train] batch: 46 | loss: 0.21165517488419922\n",
            "[Train] batch: 47 | loss: 0.41804958711816376\n",
            "[Train] batch: 48 | loss: 0.3305454592871109\n",
            "[Train] batch: 49 | loss: 0.13834061464005326\n",
            "[Train] batch: 50 | loss: 0.0899438615505542\n",
            "[Train] batch: 51 | loss: 0.40104188132607316\n",
            "[Train] batch: 52 | loss: 0.23554610221228042\n",
            "[Train] batch: 53 | loss: 0.1631402241609375\n",
            "[Train] batch: 54 | loss: 0.12556142177552618\n",
            "[Train] batch: 55 | loss: 0.19410607178134426\n",
            "[Train] batch: 56 | loss: 0.2891372187592162\n",
            "[Train] batch: 57 | loss: 0.24481441567405868\n",
            "[Train] batch: 58 | loss: 0.14987033793467444\n",
            "[Train] batch: 59 | loss: 0.12771740584396915\n",
            "[Train] batch: 60 | loss: 0.18829088949667327\n",
            "[Train] batch: 61 | loss: 0.5700138536928269\n",
            "[Train] batch: 62 | loss: 0.39527712873850945\n",
            "[Train] batch: 63 | loss: 0.34021303809293707\n",
            "[Train] batch: 64 | loss: 0.4840467145647781\n",
            "[Train] batch: 65 | loss: 0.22499447809373102\n",
            "[Train] batch: 66 | loss: 0.1894854375729882\n",
            "[Train] batch: 67 | loss: 0.4475915261976567\n",
            "[Train] batch: 68 | loss: 0.15344270230512597\n",
            "[Train] batch: 69 | loss: 0.15217935556960954\n",
            "[Train] batch: 70 | loss: 0.21231701111164147\n",
            "[Train] batch: 71 | loss: 0.19733733331311365\n",
            "[Train] batch: 72 | loss: 0.09954856708772757\n",
            "[Train] batch: 73 | loss: 0.34909976274848553\n",
            "[Train] batch: 74 | loss: 0.14141725920195364\n",
            "[Train] batch: 75 | loss: 0.07541780962553281\n",
            "[Train] batch: 76 | loss: 0.3300302909478318\n",
            "[Train] batch: 77 | loss: 0.27560884222418663\n",
            "[Train] batch: 78 | loss: 0.26313823076488924\n",
            "[Train] batch: 79 | loss: 0.13326179559431361\n",
            "[Train] batch: 80 | loss: 0.09703773744549958\n",
            "[Train] batch: 81 | loss: 0.1439617119559317\n",
            "[Train] batch: 82 | loss: 0.15902033608432403\n",
            "[Train] batch: 83 | loss: 0.12473785804184499\n",
            "[Train] batch: 84 | loss: 0.15970000555001845\n",
            "[Train] batch: 85 | loss: 0.08295951064765093\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn [1], line 411\u001b[0m\n\u001b[0;32m    409\u001b[0m model\u001b[38;5;241m=\u001b[39mmodel\u001b[38;5;241m.\u001b[39mdouble()\n\u001b[0;32m    410\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m--> 411\u001b[0m loss_train, acc_train \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msettings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    412\u001b[0m loss_val, acc_val \u001b[38;5;241m=\u001b[39m val(dataloaders[\u001b[38;5;241m1\u001b[39m], settings, model, criterion)\n\u001b[0;32m    413\u001b[0m record_learning_curve(lc_name,epoch,results,loss_train,acc_train,loss_val,acc_val)\n",
            "Cell \u001b[1;32mIn [1], line 353\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(dataloaders, settings, model, criterion, optimizer)\u001b[0m\n\u001b[0;32m    350\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m    352\u001b[0m \u001b[38;5;66;03m# evaluate\u001b[39;00m\n\u001b[1;32m--> 353\u001b[0m running_corrects \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mtop_k\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mlen\u001b[39m(label)\n\u001b[0;32m    354\u001b[0m running_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m    355\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m[Train] batch: \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;28mstr\u001b[39m(batch_idx\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m | loss: \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;28mstr\u001b[39m(loss\u001b[38;5;241m.\u001b[39mitem()))\n",
            "Cell \u001b[1;32mIn [1], line 321\u001b[0m, in \u001b[0;36mtop_k\u001b[1;34m(pred, label, k)\u001b[0m\n\u001b[0;32m    319\u001b[0m a \u001b[38;5;241m=\u001b[39m a\u001b[38;5;241m.\u001b[39mto(torch\u001b[38;5;241m.\u001b[39mint8)\n\u001b[0;32m    320\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m a \u001b[38;5;241m*\u001b[39m label \u001b[38;5;241m+\u001b[39m (\u001b[38;5;241m1\u001b[39m\u001b[38;5;241m-\u001b[39ma) \u001b[38;5;241m*\u001b[39m k_labels[:,\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m--> 321\u001b[0m acc \u001b[38;5;241m=\u001b[39m accuracy_score(\u001b[43my_pred\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m, label\u001b[38;5;241m.\u001b[39mcpu())\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m100\u001b[39m\n\u001b[0;32m    322\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m acc\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.nn import Parameter\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import math\n",
        "import pickle\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics import accuracy_score\n",
        "from fcwt import *\n",
        "#import fcwt\n",
        "import pywt\n",
        "import time\n",
        "from scipy import signal\n",
        "\n",
        "import os\n",
        "#os.chdir('C:/Users/artaa/Documents/ITSC_sim')\n",
        "\n",
        "from Mchcnn2d import *\n",
        "\n",
        "\n",
        "\n",
        "def normalise(spectra):\n",
        "    if type(spectra) is np.ndarray:\n",
        "        max_I = np.max(spectra)\n",
        "        min_I = np.min(spectra)\n",
        "    elif type(spectra) is torch.Tensor:\n",
        "        max_I = max(spectra)\n",
        "        min_I = min(spectra)\n",
        "    spectra_normed = (spectra - min_I) / (max_I - min_I)\n",
        "    return spectra_normed\n",
        "\n",
        "def random_data_split(spectra, labels, settings):\n",
        "    thresh1 = round(settings[0]*settings[5])\n",
        "    thresh2 = round((settings[0] - thresh1)/2 + thresh1)\n",
        "    l = list(range(settings[0]))\n",
        "    if os.path.isfile('data_test_ind.csv'):\n",
        "      lr = np.loadtxt('data_test_ind.csv', delimiter=\",\").astype(int)\n",
        "    else:\n",
        "      lr = random.sample(l, settings[0])\n",
        "      np.savetxt('data_test_ind.csv',lr, delimiter=\",\")\n",
        "\n",
        "    data_train = np.array([spectra[idx] for idx in lr[:thresh1]])\n",
        "    data_val = np.array([spectra[idx] for idx in lr[thresh1:thresh2]])\n",
        "    data_test = np.array([spectra[idx] for idx in lr[thresh2:]])\n",
        "    labels_train = np.array([labels[idx] for idx in lr[:thresh1]])\n",
        "    labels_val = np.array([labels[idx] for idx in lr[thresh1:thresh2]])\n",
        "    labels_test = np.array([labels[idx] for idx in lr[thresh2:]])\n",
        "\n",
        "    return (data_train, data_val, data_test), (labels_train, labels_val, labels_test)\n",
        "\n",
        "\n",
        "class AugmentedDataset(Dataset):\n",
        "    def __init__(self, tensors, settings, settings_aug):\n",
        "        self.tensors = tensors\n",
        "        self.settings = settings\n",
        "        self.settings_aug = settings_aug\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        x = torch.from_numpy(np.asarray(self.tensors[0][index][:])).to(self.settings[4])\n",
        "        y = torch.tensor(\n",
        "            self.tensors[1][index].astype(np.float32)\n",
        "        ).to(self.settings[4])\n",
        "        return x, y\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.tensors[0])\n",
        "\n",
        "def AugmentedDataloader(spectra, labels, settings, settings_aug):\n",
        "    tensors = (spectra, labels)\n",
        "    #print('spectrashape',spectra.shape)\n",
        "    ds = AugmentedDataset(\n",
        "        tensors,\n",
        "        settings,\n",
        "        settings_aug,\n",
        "    )\n",
        "    loader = DataLoader(\n",
        "        ds,\n",
        "        batch_size=settings[6],\n",
        "        shuffle=True,\n",
        "        drop_last=True,\n",
        "    )\n",
        "    return loader\n",
        "\n",
        "def dataloader_preparation(split_ratio=0.7, batch_size=50):\n",
        "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    # load dataset\n",
        "    #spectra = normalise(xrd_datasets[0][:, np.newaxis, :])\n",
        "    #labels = xrd_datasets[1]\n",
        "    #loaded_arr = np.loadtxt(\"SFT_Data_sim_SVM_BLAC_Simple_STFT3.csv\",delimiter=\",\")\n",
        "    #loaded_arr = np.loadtxt(\"SFT_Data_simple.csv\",delimiter=\",\")\n",
        "    #Data = loaded_arr.reshape(loaded_arr.shape[0], loaded_arr.shape[1] //51, 51)\n",
        "\n",
        "    xrd_datasets1 = np.loadtxt('Vib_data2.csv',delimiter=\",\")\n",
        "    xrd_datasets = xrd_datasets1[:,:]\n",
        "    #morl1 = Morlet(0.5)\n",
        "    #morl2 = Morlet(4.0)\n",
        "    #morl3 = Morlet(16.0)\n",
        "    fs = 12000\n",
        "    freq_a = 60\n",
        "    freq_i=freq_a/4\n",
        "    N_s=2400\n",
        "    N_f=2400//4\n",
        "    NFFT=fs//freq_i\n",
        "    NPSEG=NFFT//2\n",
        "    NOVER=NPSEG-(N_s//1200)\n",
        "\n",
        "    Data1=np.zeros([len(xrd_datasets[:,2400]),50,1201])\n",
        "    #Data2=np.zeros([len(xrd_datasets[:,2400]),100,2400])\n",
        "    #Data3=np.zeros([len(xrd_datasets[:,2400]),100,2400])\n",
        "    spectra = []\n",
        "    Class_label=np.zeros(len(xrd_datasets[:,2400]))\n",
        "\n",
        "    t0 = time.time()\n",
        "    for i in range(len(xrd_datasets[:,2400])):#\n",
        "\n",
        "      x=xrd_datasets[i,0:N_s].astype('single')#+np.random.normal(0,.5,N_s).astype('single')\n",
        "      class_n=xrd_datasets[i,2400]\n",
        "\n",
        "      ###################################STFT###################################\n",
        "      f, t, Zxx = signal.stft(x , fs, nperseg=NPSEG, noverlap=NOVER, nfft=NFFT)\n",
        "      Data1[i,:,:] = np.abs(Zxx[0:50])\n",
        "\n",
        "      '''\n",
        "\n",
        "      #############################tuned settings fCWT##########################\n",
        "\n",
        "\n",
        "      for j in range(fn):\n",
        "        BW = 0.3*(j+1)\n",
        "        morl1 = Morlet(BW)\n",
        "        out = np.zeros((2,len(x)), dtype='csingle')\n",
        "        freqs = np.zeros((2), dtype='single')\n",
        "        scales = Scales(morl1,FCWT_LOGSCALES,fs,(j+1)*30,(j+2)*30,2)\n",
        "        scales.getFrequencies(freqs)\n",
        "        fcwt = FCWT(morl1, 8, False, False)\n",
        "\n",
        "        fcwt.cwt(x, scales, out)\n",
        "        Data1[i,j,:] = abs(out[0,:])\n",
        "\n",
        "      '''\n",
        "      '''\n",
        "      ####################CONVENTIONAL tuned settings fCWT######################\n",
        "\n",
        "\n",
        "      BW = 4\n",
        "      morl1 = Morlet(BW)\n",
        "      out = np.zeros((fn,len(x)), dtype='csingle')\n",
        "      freqs = np.zeros((fn), dtype='single')\n",
        "      scales = Scales(morl1,FCWT_LOGSCALES,fs,f0,f1,fn)\n",
        "      scales.getFrequencies(freqs)\n",
        "      fcwt = FCWT(morl1, 8, False, False)\n",
        "\n",
        "      fcwt.cwt(x, scales, out)\n",
        "      Data1[i,:,:] = abs(out)\n",
        "\n",
        "      '''\n",
        "      '''\n",
        "      out = np.zeros((fn,len(x)), dtype='csingle')\n",
        "      freqs = np.zeros((fn), dtype='single')\n",
        "      scales = Scales(morl2,FCWT_LOGSCALES,fs,f0,f1,fn)\n",
        "      scales.getFrequencies(freqs)\n",
        "      fcwt = FCWT(morl2, 8, False, False)\n",
        "\n",
        "      fcwt.cwt(x, scales, out)\n",
        "      Data2[i,:,:] = abs(out)\n",
        "\n",
        "      out = np.zeros((fn,len(x)), dtype='csingle')\n",
        "      freqs = np.zeros((fn), dtype='single')\n",
        "      scales = Scales(morl3,FCWT_LOGSCALES,fs,f0,f1,fn)\n",
        "      scales.getFrequencies(freqs)\n",
        "      fcwt = FCWT(morl3, 8, False, False)\n",
        "\n",
        "      fcwt.cwt(x, scales, out)\n",
        "      Data3[i,:,:] = abs(out)\n",
        "      '''\n",
        "      '''\n",
        "      for j in range(19):\n",
        "        idx = np.arange(j*30+28,j*30+32)\n",
        "        Data[i,j*5:j*5+4,:]=np.abs(out[idx,:])\n",
        "      '''\n",
        "      Class_label[i]=class_n\n",
        "      #'''\n",
        "      '''\n",
        "      ##################################fCWT####################################\n",
        "      signal_CWRU=xrd_datasets[i,0:N_s]\n",
        "      class_n=xrd_datasets[i,2400]\n",
        "      freqs, coef=fcwt.cwt(signal_CWRU,12000,1,500,20)\n",
        "      Data[i,:,:]=coef\n",
        "      Class_label[i]=class_n\n",
        "      '''\n",
        "      '''\n",
        "      #############################Conventional CWT#############################\n",
        "      signal_CWRU=xrd_datasets[i,0:N_s]\n",
        "      class_n=xrd_datasets[i,2400]\n",
        "\n",
        "      coef, freqs=pywt.cwt(signal_CWRU,np.arange(1,51),'gaus1')\n",
        "      Data1[i,:,:]=coef\n",
        "      '''\n",
        "    spectra = Data1[:,np.newaxis,:,:]\n",
        "    #spectra = np.append(spectra,Data2[:,np.newaxis,:,:],axis=1)\n",
        "    #spectra = np.append(spectra,Data3[:,np.newaxis,:,:],axis=1)\n",
        "    '''\n",
        "    ############################################################################\n",
        "    f0 = 1\n",
        "    f1 = 1000\n",
        "\n",
        "    Data=np.zeros([len(xrd_datasets[:,2400]),fn,2400])\n",
        "    Class_label=np.zeros(len(xrd_datasets[:,2400]))\n",
        "    for i in range(len(xrd_datasets[:,2400])):#\n",
        "      fs=12000\n",
        "      N_s=2400\n",
        "      N_f=2400\n",
        "\n",
        "      #############################tuned settings fCWT##########################\n",
        "      x=xrd_datasets[i,0:N_s].astype('single')#+np.random.normal(0,.5,N_s).astype('single')\n",
        "      class_n=xrd_datasets[i,2400]\n",
        "\n",
        "      out = np.zeros((fn,len(x)), dtype='csingle')\n",
        "      freqs = np.zeros((fn), dtype='single')\n",
        "      scales = Scales(morl,FCWT_LOGSCALES,fs,f0,f1,fn)\n",
        "      scales.getFrequencies(freqs)\n",
        "      fcwt = FCWT(morl, 8, True, False)\n",
        "\n",
        "      fcwt.cwt(x, scales, out)\n",
        "\n",
        "      Data[i,:,:]=abs(out)\n",
        "      Class_label[i]=class_n\n",
        "    spectra = np.append(spectra,Data[:,np.newaxis,:,:],axis=1)\n",
        "\n",
        "    ############################################################################\n",
        "    f0 = 1\n",
        "    f1 = 5000\n",
        "\n",
        "    Data=np.zeros([len(xrd_datasets[:,2400]),fn,2400])\n",
        "    Class_label=np.zeros(len(xrd_datasets[:,2400]))\n",
        "    for i in range(len(xrd_datasets[:,2400])):#\n",
        "      fs=12000\n",
        "      N_s=2400\n",
        "      N_f=2400\n",
        "\n",
        "      #############################tuned settings fCWT##########################\n",
        "      x=xrd_datasets[i,0:N_s].astype('single')#+np.random.normal(0,.5,N_s).astype('single')\n",
        "      class_n=xrd_datasets[i,2400]\n",
        "\n",
        "      out = np.zeros((fn,len(x)), dtype='csingle')\n",
        "      freqs = np.zeros((fn), dtype='single')\n",
        "      scales = Scales(morl,FCWT_LOGSCALES,fs,f0,f1,fn)\n",
        "      scales.getFrequencies(freqs)\n",
        "      fcwt = FCWT(morl, 8, True, False)\n",
        "\n",
        "      fcwt.cwt(x, scales, out)\n",
        "\n",
        "      Data[i,:,:]=abs(out)\n",
        "      Class_label[i]=class_n\n",
        "    spectra = np.append(spectra,Data[:,np.newaxis,:,:],axis=1)\n",
        "    ############################################################################\n",
        "    '''\n",
        "    #labels = np.loadtxt(\"Class_label_SVM_BLAC_test.csv\",delimiter=',')\n",
        "    #labels = np.loadtxt(\"Class_label_specto_simple.csv\",delimiter=',')\n",
        "    print('STFT time is:'+str(time.time()-t0))\n",
        "    labels = Class_label\n",
        "    # measure the numbers of dataset shape\n",
        "    n_samples, n_channel, n_length,_ = spectra.shape\n",
        "    n_class = len(np.unique(labels))\n",
        "    settings = (n_samples, n_channel, n_length, n_class, device, split_ratio, batch_size)\n",
        "    settings_aug = (100, 120, 0.2, 0.2, 0.5)\n",
        "    # (window size, max peak shift size, probability of peak elimination,\n",
        "    #  probability of peak scailing, probability of peak shift)\n",
        "\n",
        "    # dataloaders\n",
        "    spectra_split, labels_split = random_data_split(spectra, labels, settings)\n",
        "    dataloader_train = AugmentedDataloader(\n",
        "        spectra_split[0],\n",
        "        labels_split[0],\n",
        "        settings,\n",
        "        settings_aug,\n",
        "    )\n",
        "\n",
        "    dataloader_val = DataLoader(\n",
        "        MyDataset(spectra_split[1],labels_split[1]),\n",
        "        batch_size=settings[6],\n",
        "        #drop_last=True,\n",
        "    )\n",
        "\n",
        "    dataloader_test = DataLoader(\n",
        "        MyDataset(spectra_split[2],labels_split[2]),\n",
        "        batch_size=settings[6],\n",
        "        #drop_last=True,\n",
        "    )\n",
        "\n",
        "    # compile dataloaders and settings\n",
        "    dataloaders = (dataloader_train, dataloader_val, dataloader_test)\n",
        "    return dataloaders, settings\n",
        "\n",
        "def load_model(settings):\n",
        "\n",
        "    #model = MSResNet(input_channel=1,layers=[4,2,5], num_classes=4)\n",
        "\n",
        "    model = Net(in_channels=1,\n",
        "                   n_class=12,##################################################################\n",
        "                  )\n",
        "\n",
        "    model.to(settings[4])\n",
        "    model=model.double()\n",
        "    return model\n",
        "\n",
        "def top_k(pred, label, k:int = 1):\n",
        "    labels_dim = 1\n",
        "    k_labels = torch.topk(input=pred, k=k, dim=1, largest=True, sorted=True)[1]\n",
        "    a = ~torch.prod(\n",
        "        input = torch.abs(label.unsqueeze(labels_dim) - k_labels),\n",
        "        dim=labels_dim,\n",
        "    ).to(torch.bool)\n",
        "    a = a.to(torch.int8)\n",
        "    y_pred = a * label + (1-a) * k_labels[:,0]\n",
        "    acc = accuracy_score(y_pred.cpu(), label.cpu())*100\n",
        "    return acc\n",
        "\n",
        "def record_learning_curve(lc_name, epoch, results, loss_train, acc_train, loss_val, acc_val):\n",
        "    results[epoch, :] = np.array([loss_train, acc_train, loss_val, acc_val])\n",
        "    df = pd.DataFrame(results, columns=['loss_train', 'acc_train', 'loss_val', 'acc_val'])\n",
        "    df.to_csv(lc_name)\n",
        "\n",
        "def save_model(best_acc, epoch, model):\n",
        "    print('--------> The best model has been replaced.')\n",
        "    print('epoch: '+str(epoch)+' | best_acc: '+str(best_acc))\n",
        "    model_path = './wdcnn2d_epoch_CWRU'+str(epoch)+'.pt'\n",
        "    torch.save(model.state_dict(), model_path)\n",
        "    print('The best model has been saved in '+model_path)\n",
        "\n",
        "def train(dataloaders, settings, model, criterion, optimizer):\n",
        "    running_loss = 0.0\n",
        "    running_corrects = 0.0\n",
        "    model.train()\n",
        "    model.zero_grad()\n",
        "\n",
        "    for batch_idx, batch in enumerate(dataloaders[0]):\n",
        "        # train\n",
        "        input, label = tuple(t.to(settings[4]) for t in batch)\n",
        "        label = label.long()\n",
        "        pred = model(input)\n",
        "        loss = criterion(pred, label)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # evaluate\n",
        "        running_corrects += top_k(pred, label, k=1) * len(label)\n",
        "        running_loss += loss.item()\n",
        "        print('[Train] batch: '+str(batch_idx+1)+' | loss: '+str(loss.item()))\n",
        "\n",
        "    # summarise\n",
        "    n_train = round(settings[0] * settings[5])\n",
        "    epoch_loss = running_loss / n_train\n",
        "    epoch_acc = running_corrects / n_train\n",
        "    print('[Train total] loss: '+str(epoch_loss)+' | acc: '+str(epoch_acc))\n",
        "    return epoch_loss, epoch_acc\n",
        "\n",
        "def val(dataloader, settings, model, criterion):\n",
        "    running_loss = 0.0\n",
        "    running_corrects = 0.0\n",
        "    model.eval()\n",
        "    model.zero_grad()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, batch in enumerate(dataloader):\n",
        "            # test\n",
        "            input, label = tuple(t.to(settings[4]) for t in batch)\n",
        "            label = label.long()\n",
        "            model=model.to(settings[4])\n",
        "            pred = model(input.double())\n",
        "            loss = criterion(pred, label)\n",
        "\n",
        "            # evaluate\n",
        "            running_corrects += top_k(pred, label, k=1) * len(label)\n",
        "            running_loss += loss.item()\n",
        "            print('[Val] batch: '+str(batch_idx+1)+' | loss: '+str(loss.item()))\n",
        "\n",
        "    # summarise\n",
        "    n_test = round(settings[0] * (1 - settings[5])/2)\n",
        "    epoch_loss = running_loss / n_test\n",
        "    epoch_acc = running_corrects / n_test\n",
        "    print('[Val total] loss: '+str(epoch_loss)+' | acc: '+str(epoch_acc))\n",
        "    return epoch_loss, epoch_acc\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    lc_name = 'learning_curve_Mch_lstm_CNN2D_CWRU_stft.csv'\n",
        "    n_epoch = 50\n",
        "    batch = 14\n",
        "\n",
        "    dataloaders, settings = dataloader_preparation(batch_size=batch)\n",
        "    model = load_model(settings)\n",
        "    criterion = nn.CrossEntropyLoss().to(settings[4])\n",
        "    optimizer = optim.SGD(model.parameters(), lr=0.001)\n",
        "    '''\n",
        "    from torchsummary import summary\n",
        "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "    summary(model.to(device=device, dtype=torch.float),(1,10,51))\n",
        "    '''\n",
        "    best_acc = 0.0\n",
        "    results = np.zeros([n_epoch, 4])\n",
        "    for epoch in range(n_epoch):\n",
        "        print('>>>>>> epoch '+str(epoch)+' starts')\n",
        "        model=model.double()\n",
        "        model.eval()\n",
        "        loss_train, acc_train = train(dataloaders, settings, model, criterion, optimizer)\n",
        "        loss_val, acc_val = val(dataloaders[1], settings, model, criterion)\n",
        "        record_learning_curve(lc_name,epoch,results,loss_train,acc_train,loss_val,acc_val)\n",
        "\n",
        "        # save better model\n",
        "        if best_acc <= acc_val:\n",
        "            best_acc = acc_val\n",
        "            loss_test, acc_test = val(dataloaders[2], settings, model, criterion)\n",
        "\n",
        "            #if epoch>10:\n",
        "            #save_model(best_acc, epoch, model)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def save_model(best_acc, epoch, model):\n",
        "    print('--------> The best model has been replaced.')\n",
        "    print('epoch: '+str(epoch)+' | best_acc: '+str(best_acc))\n",
        "    model_path = './wdcnn2d_epoch_CWRU_stft'+str(epoch)+'.pt'\n",
        "    torch.save(model.state_dict(), model_path)\n",
        "    print('The best model has been saved in '+model_path)\n",
        "save_model(best_acc, epoch, model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yq9sA19_83NB",
        "outputId": "9ee8c189-aa8f-4bd4-e694-261fdceaffc9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------> The best model has been replaced.\n",
            "epoch: 49 | best_acc: 98.73873873873873\n",
            "The best model has been saved in ./wdcnn2d_epoch_CWRU_stft49.pt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model=model.double()\n",
        "model.eval()\n",
        "loss_train, acc_train = train(dataloaders, settings, model, criterion, optimizer)\n",
        "loss_val, acc_val = val(dataloaders[1], settings, model, criterion)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MvqgfhP24jD5",
        "outputId": "b7f1cc06-bfed-45bf-c118-299a266cb8d4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Train] batch: 1 | loss: 0.004106310981769187\n",
            "[Train] batch: 2 | loss: 0.00484953258299111\n",
            "[Train] batch: 3 | loss: 0.00207602781886246\n",
            "[Train] batch: 4 | loss: 0.0017417583620496278\n",
            "[Train] batch: 5 | loss: 0.002026513327701472\n",
            "[Train] batch: 6 | loss: 0.003400024750747749\n",
            "[Train] batch: 7 | loss: 0.004154615213936001\n",
            "[Train] batch: 8 | loss: 0.024609286899057083\n",
            "[Train] batch: 9 | loss: 0.029842058460603036\n",
            "[Train] batch: 10 | loss: 0.0031891444066827848\n",
            "[Train] batch: 11 | loss: 0.0029260814379938787\n",
            "[Train] batch: 12 | loss: 0.008664795564117141\n",
            "[Train] batch: 13 | loss: 0.005246996042016952\n",
            "[Train] batch: 14 | loss: 0.009376435785394642\n",
            "[Train] batch: 15 | loss: 0.009372676685855192\n",
            "[Train] batch: 16 | loss: 0.007666466961427551\n",
            "[Train] batch: 17 | loss: 0.004735193468067624\n",
            "[Train] batch: 18 | loss: 0.004152478588660332\n",
            "[Train] batch: 19 | loss: 0.0013618823836973226\n",
            "[Train] batch: 20 | loss: 0.0010770054128629058\n",
            "[Train] batch: 21 | loss: 0.027601059596686715\n",
            "[Train] batch: 22 | loss: 0.008106172579249954\n",
            "[Train] batch: 23 | loss: 0.009274472186106681\n",
            "[Train] batch: 24 | loss: 0.0014714130067194715\n",
            "[Train] batch: 25 | loss: 0.0011801793104059862\n",
            "[Train] batch: 26 | loss: 0.000956418136082368\n",
            "[Train] batch: 27 | loss: 0.022351075903593306\n",
            "[Train] batch: 28 | loss: 0.0011899431735285203\n",
            "[Train] batch: 29 | loss: 0.01235299747984909\n",
            "[Train] batch: 30 | loss: 0.011453583618121165\n",
            "[Train] batch: 31 | loss: 0.0036374594838448917\n",
            "[Train] batch: 32 | loss: 0.0021518874260204697\n",
            "[Train] batch: 33 | loss: 0.013528294288402496\n",
            "[Train] batch: 34 | loss: 0.01599970772846475\n",
            "[Train] batch: 35 | loss: 0.003475592289144386\n",
            "[Train] batch: 36 | loss: 0.011054854914748561\n",
            "[Train] batch: 37 | loss: 0.0032614787937837674\n",
            "[Train] batch: 38 | loss: 0.004473894069417629\n",
            "[Train] batch: 39 | loss: 0.006676619025491658\n",
            "[Train] batch: 40 | loss: 0.0076992961830675555\n",
            "[Train] batch: 41 | loss: 0.003759461279012314\n",
            "[Train] batch: 42 | loss: 0.012869534860091918\n",
            "[Train] batch: 43 | loss: 0.008722312569838712\n",
            "[Train] batch: 44 | loss: 0.006443950483218908\n",
            "[Train] batch: 45 | loss: 0.0011382103755089943\n",
            "[Train] batch: 46 | loss: 0.011017878395309239\n",
            "[Train] batch: 47 | loss: 0.008153836055899888\n",
            "[Train] batch: 48 | loss: 0.006755371125530686\n",
            "[Train] batch: 49 | loss: 0.003874716481250385\n",
            "[Train] batch: 50 | loss: 0.025797796108629045\n",
            "[Train] batch: 51 | loss: 0.006317669450058934\n",
            "[Train] batch: 52 | loss: 0.006509296361542475\n",
            "[Train] batch: 53 | loss: 0.0038811971365508144\n",
            "[Train] batch: 54 | loss: 0.0010820423743064932\n",
            "[Train] batch: 55 | loss: 0.00449889482481694\n",
            "[Train] batch: 56 | loss: 0.005874327762441692\n",
            "[Train] batch: 57 | loss: 0.00649576960355551\n",
            "[Train] batch: 58 | loss: 0.010588846320455623\n",
            "[Train] batch: 59 | loss: 0.02741527526196186\n",
            "[Train] batch: 60 | loss: 0.01239401713847858\n",
            "[Train] batch: 61 | loss: 0.006895266435670807\n",
            "[Train] batch: 62 | loss: 0.0017029873923971788\n",
            "[Train] batch: 63 | loss: 0.007070537202683991\n",
            "[Train] batch: 64 | loss: 0.0003798562301121939\n",
            "[Train] batch: 65 | loss: 0.0016503896543144051\n",
            "[Train] batch: 66 | loss: 0.004688977304506168\n",
            "[Train] batch: 67 | loss: 0.005705088801919791\n",
            "[Train] batch: 68 | loss: 0.007538252766835931\n",
            "[Train] batch: 69 | loss: 0.012406442892793912\n",
            "[Train] batch: 70 | loss: 0.003517660828095022\n",
            "[Train] batch: 71 | loss: 0.009658729303900515\n",
            "[Train] batch: 72 | loss: 0.006747629587106507\n",
            "[Train] batch: 73 | loss: 0.01748238242951402\n",
            "[Train] batch: 74 | loss: 0.005025744567859619\n",
            "[Train] batch: 75 | loss: 0.026254542851053914\n",
            "[Train] batch: 76 | loss: 0.001956318701154615\n",
            "[Train] batch: 77 | loss: 0.014290179574730544\n",
            "[Train] batch: 78 | loss: 0.0029223803614983365\n",
            "[Train] batch: 79 | loss: 0.0017478214590171722\n",
            "[Train] batch: 80 | loss: 0.009396049133495054\n",
            "[Train] batch: 81 | loss: 0.0031620850626755373\n",
            "[Train] batch: 82 | loss: 0.0301810843405159\n",
            "[Train] batch: 83 | loss: 0.010779015755969202\n",
            "[Train] batch: 84 | loss: 0.0043096250915967945\n",
            "[Train] batch: 85 | loss: 0.0012033336505182083\n",
            "[Train] batch: 86 | loss: 0.008176029295153767\n",
            "[Train] batch: 87 | loss: 0.006841476300541173\n",
            "[Train] batch: 88 | loss: 0.011078651653482376\n",
            "[Train] batch: 89 | loss: 0.001125097772389739\n",
            "[Train] batch: 90 | loss: 0.0008905139687575382\n",
            "[Train] batch: 91 | loss: 0.011198323543164563\n",
            "[Train] batch: 92 | loss: 0.011069313497888476\n",
            "[Train] batch: 93 | loss: 0.014782678607974466\n",
            "[Train] batch: 94 | loss: 0.0017038218450711445\n",
            "[Train] batch: 95 | loss: 0.004373774055943619\n",
            "[Train] batch: 96 | loss: 0.014571808612926545\n",
            "[Train] batch: 97 | loss: 0.002309739555321288\n",
            "[Train] batch: 98 | loss: 0.01267738118103134\n",
            "[Train] batch: 99 | loss: 0.008755049452510436\n",
            "[Train] batch: 100 | loss: 0.0012545645308665407\n",
            "[Train] batch: 101 | loss: 0.008759344232031443\n",
            "[Train] batch: 102 | loss: 0.0053262345540995406\n",
            "[Train] batch: 103 | loss: 0.006628716358212884\n",
            "[Train] batch: 104 | loss: 0.0023889605966117104\n",
            "[Train] batch: 105 | loss: 0.004247003149627078\n",
            "[Train] batch: 106 | loss: 0.0070968590035244685\n",
            "[Train] batch: 107 | loss: 0.02613727644020877\n",
            "[Train] batch: 108 | loss: 0.022245396110111816\n",
            "[Train] batch: 109 | loss: 0.009332583252645603\n",
            "[Train] batch: 110 | loss: 0.004290757283696224\n",
            "[Train] batch: 111 | loss: 0.0016309406394359857\n",
            "[Train] batch: 112 | loss: 0.006395583169438624\n",
            "[Train] batch: 113 | loss: 0.008312107915821165\n",
            "[Train] batch: 114 | loss: 0.004009357158922008\n",
            "[Train] batch: 115 | loss: 0.0009546633895208206\n",
            "[Train] batch: 116 | loss: 0.005326367710729412\n",
            "[Train] batch: 117 | loss: 0.004430093926476571\n",
            "[Train] batch: 118 | loss: 0.007664328868730594\n",
            "[Train] batch: 119 | loss: 0.0034550299639219298\n",
            "[Train] batch: 120 | loss: 0.016983031685720185\n",
            "[Train] batch: 121 | loss: 0.015699350512301195\n",
            "[Train] batch: 122 | loss: 0.011825914284526578\n",
            "[Train] batch: 123 | loss: 0.004216690631943211\n",
            "[Train] batch: 124 | loss: 0.0030372466026916617\n",
            "[Train] batch: 125 | loss: 0.008888254549612583\n",
            "[Train] batch: 126 | loss: 0.009059323241062905\n",
            "[Train] batch: 127 | loss: 0.002698387822357693\n",
            "[Train] batch: 128 | loss: 0.005467406690808622\n",
            "[Train] batch: 129 | loss: 0.009275543903247004\n",
            "[Train] batch: 130 | loss: 0.0017708745819781525\n",
            "[Train] batch: 131 | loss: 0.003267218265072649\n",
            "[Train] batch: 132 | loss: 0.002894146090713283\n",
            "[Train] batch: 133 | loss: 0.00533887590522753\n",
            "[Train] batch: 134 | loss: 0.0065398201050802145\n",
            "[Train] batch: 135 | loss: 0.006031389848538489\n",
            "[Train] batch: 136 | loss: 0.005077879893264288\n",
            "[Train] batch: 137 | loss: 0.008372202468279287\n",
            "[Train] batch: 138 | loss: 0.0011586145495739335\n",
            "[Train] batch: 139 | loss: 0.005659467390086899\n",
            "[Train] batch: 140 | loss: 0.006359474619718226\n",
            "[Train] batch: 141 | loss: 0.010144728151366272\n",
            "[Train] batch: 142 | loss: 0.004645568771640322\n",
            "[Train] batch: 143 | loss: 0.005656685618251929\n",
            "[Train] batch: 144 | loss: 0.011973497148607806\n",
            "[Train] batch: 145 | loss: 0.010140760343744651\n",
            "[Train] batch: 146 | loss: 0.00850430373404274\n",
            "[Train] batch: 147 | loss: 0.011697616085052737\n",
            "[Train] batch: 148 | loss: 0.015625746475923904\n",
            "[Train] batch: 149 | loss: 0.012184226557157935\n",
            "[Train] batch: 150 | loss: 0.004291629167710347\n",
            "[Train] batch: 151 | loss: 0.0020783222331948225\n",
            "[Train] batch: 152 | loss: 0.0035983002143193145\n",
            "[Train] batch: 153 | loss: 0.0016191002970238095\n",
            "[Train] batch: 154 | loss: 0.0071405483292831\n",
            "[Train] batch: 155 | loss: 0.004181319807642197\n",
            "[Train] batch: 156 | loss: 0.0036022954452740864\n",
            "[Train] batch: 157 | loss: 0.006957738670777752\n",
            "[Train] batch: 158 | loss: 0.01777019306948039\n",
            "[Train] batch: 159 | loss: 0.017935881931460746\n",
            "[Train] batch: 160 | loss: 0.0025336139169667886\n",
            "[Train] batch: 161 | loss: 0.02055568688083209\n",
            "[Train] batch: 162 | loss: 0.003317867364922309\n",
            "[Train] batch: 163 | loss: 0.005504464973665655\n",
            "[Train] batch: 164 | loss: 0.004631209972881591\n",
            "[Train] batch: 165 | loss: 0.009592392305721325\n",
            "[Train] batch: 166 | loss: 0.013567995322586224\n",
            "[Train] batch: 167 | loss: 0.007437071289131676\n",
            "[Train] batch: 168 | loss: 0.002679804254775739\n",
            "[Train] batch: 169 | loss: 0.007817191614847148\n",
            "[Train] batch: 170 | loss: 0.003568159964292211\n",
            "[Train] batch: 171 | loss: 0.004882091948681523\n",
            "[Train] batch: 172 | loss: 0.0044888620205871185\n",
            "[Train] batch: 173 | loss: 0.0018766801019665213\n",
            "[Train] batch: 174 | loss: 0.0024173982273608\n",
            "[Train] batch: 175 | loss: 0.0027153204306560423\n",
            "[Train] batch: 176 | loss: 0.002002328145365353\n",
            "[Train] batch: 177 | loss: 0.00755132792321043\n",
            "[Train] batch: 178 | loss: 0.007935188627409127\n",
            "[Train] batch: 179 | loss: 0.003755391831882848\n",
            "[Train] batch: 180 | loss: 0.007979965731749654\n",
            "[Train] batch: 181 | loss: 0.00452174787531881\n",
            "[Train] batch: 182 | loss: 0.001756717843734271\n",
            "[Train] batch: 183 | loss: 0.001202290568443977\n",
            "[Train] batch: 184 | loss: 0.006683797813327991\n",
            "[Train] batch: 185 | loss: 0.002073140562985237\n",
            "[Train total] loss: 0.0005268693360924593 | acc: 100.0\n",
            "[Val] batch: 1 | loss: 0.1188228499437974\n",
            "[Val] batch: 2 | loss: 0.01833282259810291\n",
            "[Val] batch: 3 | loss: 0.14443776740940759\n",
            "[Val] batch: 4 | loss: 0.04210468619293616\n",
            "[Val] batch: 5 | loss: 0.05723798638995858\n",
            "[Val] batch: 6 | loss: 0.12906398465376326\n",
            "[Val] batch: 7 | loss: 0.08619774933224925\n",
            "[Val] batch: 8 | loss: 0.24634756160100743\n",
            "[Val] batch: 9 | loss: 0.11344240794753982\n",
            "[Val] batch: 10 | loss: 0.2979901420232745\n",
            "[Val] batch: 11 | loss: 0.07179536126913219\n",
            "[Val] batch: 12 | loss: 0.28098268639063256\n",
            "[Val] batch: 13 | loss: 0.11810940504009212\n",
            "[Val] batch: 14 | loss: 0.033252340852283395\n",
            "[Val] batch: 15 | loss: 0.005300935235685004\n",
            "[Val] batch: 16 | loss: 0.10464818242929921\n",
            "[Val] batch: 17 | loss: 0.008261056334069749\n",
            "[Val] batch: 18 | loss: 0.02597242546106545\n",
            "[Val] batch: 19 | loss: 0.044151267207863165\n",
            "[Val] batch: 20 | loss: 0.18900587872145472\n",
            "[Val] batch: 21 | loss: 0.2949957515636041\n",
            "[Val] batch: 22 | loss: 0.012523002717688386\n",
            "[Val] batch: 23 | loss: 0.017583298186460913\n",
            "[Val] batch: 24 | loss: 0.2548579912599167\n",
            "[Val] batch: 25 | loss: 0.21260557734622074\n",
            "[Val] batch: 26 | loss: 0.04022344143496935\n",
            "[Val] batch: 27 | loss: 0.14790127369132522\n",
            "[Val] batch: 28 | loss: 0.17059213963492031\n",
            "[Val] batch: 29 | loss: 0.005868319171986479\n",
            "[Val] batch: 30 | loss: 0.20191533137133452\n",
            "[Val] batch: 31 | loss: 0.17154914804026664\n",
            "[Val] batch: 32 | loss: 0.3395957547925012\n",
            "[Val] batch: 33 | loss: 0.35639118305563056\n",
            "[Val] batch: 34 | loss: 0.12353711832854554\n",
            "[Val] batch: 35 | loss: 0.03288160983223173\n",
            "[Val] batch: 36 | loss: 0.06758742263841849\n",
            "[Val] batch: 37 | loss: 0.10047604881249025\n",
            "[Val] batch: 38 | loss: 0.07108855804871005\n",
            "[Val] batch: 39 | loss: 0.02884077994318198\n",
            "[Val] batch: 40 | loss: 0.09519945587441642\n",
            "[Val total] loss: 0.00879580306806925 | acc: 96.21621621621621\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "Hx9WKunds26a",
        "U2oHikw3LhG9",
        "K2mFM-ABLf-G"
      ],
      "mount_file_id": "1RYzgvHETVHjzPPixGRkqHOCp7ygM8-Y3",
      "authorship_tag": "ABX9TyN2OkGs/KJ8iM/tJ+Hm/JcD",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}